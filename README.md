# ğŸ“ Part 1 â€” Introduction and Problem Setup (Slides 1 â€“ 12)

---

## **Slide 1 â€“ Title Slide**

**Best Subset Selection via Mixed Integer Optimization**

â€œGood [morning/afternoon]. Today Iâ€™ll present *Best Subset Selection via Mixed Integer Optimization*, a work by Bertsimas, King and Mazumder.  
This talk revisits a classical statistical problem â€” best subset selection â€” through a modern optimization lens.  
The central message is that what was once computationally impossible can, with advances in optimization algorithms and hardware, be solved exactly and efficiently for moderate-sized problems.â€

â€œWeâ€™ll move from the classical formulation to its modern MIO version, analyze theoretical equivalence, and end with computational and statistical insights.â€  
*(pause â†’ transition)* â€œLet me begin with the precise problem statement.â€

---

## **Slide 2 â€“ Outline**

â€œHere is the roadmap.  
We start with the problem formulation, then develop the mixed integer optimization approach.  
Weâ€™ll follow with theoretical and statistical analysis, the algorithmic implementation, and finally discuss empirical results and implications.â€  
*(transition)* â€œSo letâ€™s begin by formalizing what best subset selection actually means.â€

---

## **Slide 3 â€“ Best Subset Selection Problem**

Given data matrix $X \in \mathbb{R}^{n\times p}$ and response vector $y \in \mathbb{R}^n$:

$$
\min_{\beta \in \mathbb{R}^p}\;\tfrac{1}{2}\|y - X\beta\|_2^2
\quad\text{s.t.}\quad
\|\beta\|_0 \le k
$$

where $\|\beta\|_0 = |\{j : \beta_j \ne 0\}|$ counts non-zero coefficients.

**Intuition** â€” â€œThis seeks the linear model that minimizes residual sum of squares using â‰¤ k predictors.  
It yields exactly sparse, interpretable models and is theoretically optimal under oracle assumptions.â€

*(side note)* If asked â€˜Why Â½?â€™ â†’ for derivative convenience; doesnâ€™t change minimizer.  
*(transition)* â€œHowever, this elegance hides a severe computational difficulty.â€

---

## **Slide 4 â€“ Understanding the Objective Function**

$$
\tfrac{1}{2}\|y âˆ’ X\beta\|_2^2 = \tfrac{1}{2}\sum_{i=1}^n (y_i âˆ’ \hat y_i)^2
$$

â€œThe factor Â½ simplifies derivatives.  
The constraint $\|\beta\|_0 \le k$ forces most coefficients to zero â€” selecting â‰¤ k variables.â€

**Geometric intuition** â€” â€œSearching for the least-squares hyperplane restricted to coordinate subspaces spanned by k variables.  
This discrete structure gives interpretability but makes the search combinatorial.â€  
*(transition)* â€œThat combinatorial explosion is what historically made this problem â€˜impossible.â€™â€

---

## **Slide 5 â€“ The Computational Nightmare**

â€œTo find the best subset, one would test every combination of k predictors among p: $\binom{p}{k}$ possibilities.  
For $p = 1000$ and $k = 10$, $\binom{1000}{10} \approx 2.6\times10^{23}$ â€” utterly intractable.â€

**Historical workarounds**
- Convex relaxations (LASSO: replace $\|\beta\|_0$ with $\|\beta\|_1$)  
- Greedy algorithms (forward/backward stepwise)  
- Heuristics (genetic algorithms, simulated annealing)

â€œAll sacrifice exact optimality for speed.â€  
*(transition)* â€œSo can we recover exact optimality yet stay computationally reasonable?  
The next slide introduces the key idea â€” a mixed-integer reformulation.â€

---

## **Slide 6 â€“ MIO Reformulation â€” The Key Insight**

Introduce binary variables $z_i \in \{0,1\}$ indicating whether feature $i$ is active:

$$
\min_{\beta,z}\tfrac{1}{2}\|y âˆ’ X\beta\|_2^2
$$

subject to

$$
âˆ’M z_i \le \beta_i \le M z_i,\quad i=1,\ldots,p,
\qquad
\sum_{i=1}^p z_i \le k,\quad z_i \in \{0,1\}.
$$

Here $M$ is a large constant bounding coefficients.

**Intuition** â€” â€œIf $z_i=0$ â‡’ $\beta_i=0$; if $z_i=1$ â‡’ $\beta_i\in[âˆ’M,M]$.  
$\sum z_i \le k$ imposes sparsity logically rather than by counting.â€  
*(transition)* â€œLetâ€™s see precisely how these Big-M constraints work.â€

---

## **Slide 7 â€“ How Big-M Constraints Work**

For each $i$:

$$
âˆ’M z_i \le \beta_i \le M z_i
$$

- If $z_i=0$ â‡’ $\beta_i=0$  
- If $z_i=1$ â‡’ $\beta_i\in[âˆ’M,M]$

â€œM must be large enough so all optimal Î²â€™s lie inside [âˆ’M,M]; too large â†’ numerical issues.  
Later weâ€™ll see theoretical bounds for M.â€

*(transition)* â€œThis reformulation was known decades ago, but only recently became computable thanks to hardware and algorithmic advances.â€

---

## **Slide 8 â€“ Why MIO Works Now â€” Computational Revolution (I)**

â€œFrom 1990s â†’ 2010s:  
CPU â‰ˆ 10Â³Ã— faster, memory â‰ˆ 10â´Ã— larger, multi-core parallelism standard.  
Algorithmic breakthroughs (cutting planes, branching heuristics, presolve) made MIP vastly faster.â€

*(Gesture to plot)* â€œOverall solver capability improved â‰ˆ 10Â¹Â¹â€“10Â¹Â²Ã— since early 1990s.â€  
*(transition)* â€œEqually important were software advances.â€

---

## **Slide 9 â€“ Why MIO Works Now â€” Computational Revolution (II)**

â€œState-of-the-art solvers (Gurobi, CPLEX, SCIP) implement branch-and-bound + cutting planes + presolve + heuristics that find good solutions early and tighten bounds.â€

**Key point** â€” â€œNet effect: â‰ˆ 200-billion-fold improvement in practical solvability.  
Problems once hopeless are now feasible on modern hardware.â€  
*(transition)* â€œBut we must prove that the MIO formulation really solves the original statistical problem.â€

---

## **Slide 10 â€“ Why Theoretical Analysis Is Crucial**

â€œThe authors ask:  
1ï¸âƒ£ Equivalence â€” does MIO yield same solution as original?  
2ï¸âƒ£ Parameter bounds â€” how large must M be?  
3ï¸âƒ£ Statistical properties â€” does it retain optimal behavior?  
4ï¸âƒ£ Certificates â€” can we know when optimum is reached?â€

â€œWithout this analysis we lack guarantees that computation matches statistical truth.â€  
*(transition)* â€œLetâ€™s set up some mathematical preliminaries before proving equivalence.â€

---

## **Slide 11 â€“ Mathematical Preliminaries**

Definitions:  
- Support of Î²: $\text{supp}(\beta)=\{i:\beta_i\ne0\}$  
- â„“â‚€ norm: $\|\beta\|_0=|\text{supp}(\beta)|$  
- â„“â‚ norm: $\|\beta\|_1=\sum_i|\beta_i|$  
- â„“âˆ norm: $\|\beta\|_\infty=\max_i|\beta_i|$

Matrix quantities:  
- Restricted eigenvalue  

$$
\eta_k = \min_{|S|\le k}\min_{\substack{\text{supp}(\beta)\subseteq S\\\|\beta\|_2=1}} \frac{\|X\beta\|_2^2}{n}
$$

- Coherence  

$$
\mu = \max_{i\ne j}\frac{|X_i^T X_j|}{\|X_i\|_2 \|X_j\|_2}
$$

*(side note)* $\eta_k$ â†’ conditioning of submatrices; $\mu$ â†’ pairwise correlation.  
*(transition)* â€œWith these we can compare three formulations next.â€

---

## **Slide 12 â€“ Problem Hierarchy â€” Mathematical Structure**

Three problems:

**Pâ‚€ (Original Best Subset):**

$$
\min_{\beta}\tfrac{1}{2}\|y âˆ’ X\beta\|_2^2
\quad\text{s.t.}\quad
\|\beta\|_0\le k
$$

**Pâ‚ (MIO Reformulation):**

$$
\min_{\beta,z}\tfrac{1}{2}\|y âˆ’ X\beta\|_2^2
\quad\text{s.t.}\quad
âˆ’M z_i\le \beta_i\le M z_i,\;
\sum_i z_i\le k,\;
z_i\in\{0,1\}
$$

**Pâ‚‚ (Relaxation):** Convex approximation adding â„“â‚ and â„“âˆ bounds.

**Interpretation**
- Pâ‚€: pure combinatorial  
- Pâ‚: explicit sparsity with binary variables  
- Pâ‚‚: convex relaxation for analysis  

*(transition)* â€œNext we see how these relate through the Hierarchy Theorem â€” showing MIO â‰¡ original best subset.â€

| Concept | What it measures | Good / Bad | Consequence |
|:--|:--|:--|:--|
| $\mu$ (coherence) | Pairwise correlation | $\mu\approx0$ good | High $\mu$ â†’ hard to distinguish variables |
| $\eta_k$ (restricted eigenvalue) | Conditioning of k-subsets | Large $\eta_k$ good | Small $\eta_k$ â†’ instability, large coeffs |

# ğŸ“ Part 2 â€” Theoretical Equivalence and Statistical Analysis (Slides 13 â€“ 24)

---

## **Slide 13 â€“ Problem Hierarchy: Three Related Formulations**

â€œNow that weâ€™ve seen how the MIO formulation is constructed, letâ€™s look at how it fits into a hierarchy of related problems.  
The authors define three formulations â€” $(P_0)$, $(P_1)$, and $(P_2)$.  
Understanding these helps us see where the exact version and the relaxations live.â€

---

### **1ï¸âƒ£ $P_0$ â€” Original Best Subset Problem**

$$
\min_{\beta}\frac{1}{2}\|y - X\beta\|_2^2 \quad \text{s.t.} \quad \|\beta\|_0 \le k
$$

â€œThis is the classical version we started with: find the regression coefficients $\beta$ that minimize total squared prediction error using at most $k$ non-zero coefficients.  
Itâ€™s purely combinatorial because we must decide which variables are non-zero.â€

---

### **2ï¸âƒ£ $P_1$ â€” MIO Formulation**

$$
\min_{\beta,z}\frac{1}{2}\|y - X\beta\|_2^2
$$

subject to

$$
-Mz_i \le \beta_i \le Mz_i,\quad \sum_{i=1}^p z_i \le k,\quad z_i\in\{0,1\}
$$

â€œHere we introduce binary variables $z_i$ to indicate if a feature is active (1) or inactive (0).  
This turns the implicit sparsity constraint into explicit logical rules.â€

---

### **3ï¸âƒ£ $P_2$ â€” Relaxed Version**

$$
\min_{\beta}\frac{1}{2}\|y - X\beta\|_2^2
\quad\text{s.t.}\quad
\|\beta\|_1\le R_1,\;
\|\beta\|_\infty\le R_\infty,\;
\|\beta\|_0\le k
$$

â€œ$P_2$ adds softer bounds â€” one on total magnitude ($\ell_1$) and one on the largest coefficient ($\ell_\infty$).  
This makes it closer to convex optimization and easier to analyze mathematically.â€

---

**Intuitive picture:**  
â€œThink of $(P_2)$ as a relaxed outer shell, $(P_1)$ as the exact computational engine, and $(P_0)$ as the ideal goal.  
Together, they form a sandwich of tractability and exactness.â€  
*(transition)* â€œNext weâ€™ll see how these problems relate through an important equivalence theorem.â€

---

## **Slide 14 â€“ Hierarchy Theorem (Equivalence and Bounds)**

â€œThe theorem links their optimal values $(Z_0, Z_1, Z_2)$:â€

$$
Z_2 \le Z_1 = Z_0
$$

- $(Z_1 = Z_0)$: MIO gives the same optimum as classical best subset  
- $(Z_2 \le Z_1)$: Relaxation provides a lower bound  
- If $M \ge \|\hat\beta_{\text{OLS}}\|_\infty$, the equivalence holds exactly

â€œIn plain words: as long as $M$ isnâ€™t too small, solving the MIO problem is equivalent to solving best subset selection.â€  
*(transition)* â€œLetâ€™s go a step deeper and see why this equivalence is mathematically true.â€

---

## **Slides 15â€“16 â€“ Proof of Equivalence (The MIOâ€“BSS Theorem)**

â€œThe proof proceeds in two directions.â€

---

### **Step 1 â€“ From $P_0$ to $P_1$**

Suppose $\beta^*$ solves $(P_0)$.  
Define

$$
z_i^* =
\begin{cases}
1, & \text{if } \beta_i^* \ne 0 \\
0, & \text{otherwise}
\end{cases}
$$

- If $\beta_i^* \ne 0$, constraint $âˆ’Mz_i \le \beta_i \le Mz_i$ holds with $z_i=1$  
- If $\beta_i^* = 0$, it holds with $z_i=0$

Thus $(\beta^*, z^*)$ is feasible for $(P_1)$ and gives the same objective.

---

### **Step 2 â€“ From $P_1$ to $P_0$**

If $(\beta^*, z^*)$ solves $(P_1)$:

- $z_i=0 \Rightarrow \beta_i=0$  
- So the number of non-zero $\beta$ equals $\sum z_i \le k$  

Therefore $\beta^*$ is feasible for $(P_0)$ and has the same objective.

**Conclusion:** $(P_0)$ and $(P_1)$ are equivalent â€” proving that the MIO reformulation exactly reproduces best subset selection.

*(transition)* â€œBut to analyze performance and stability, we need to understand the geometry of $X$ â€” thatâ€™s where coherence and restricted eigenvalues come in.â€

---

## **Slide 17 â€“ Matrix Coherence ($\mu$)**

$$
\mu[k-1] = \max_{|T|\le k-1}\max_{i\notin T,j\in T}
\frac{|X_i^T X_j|}{\|X_i\|_2 \|X_j\|_2}
$$

â€œYou can think of this as the worst correlation between any variable outside a chosen subset and those inside it.  
Each $X_i^T X_j$ measures the cosine of the angle between two columns of $X$.â€

| Î¼ value | Meaning |
|:--|:--|
| 0 | Orthogonal (independent) |
| 0.5 | Moderate correlation |
| 0.9 | Strong correlation |
| 1 | Identical columns (redundant) |

â€œHigh coherence means predictors overlap in information â€” making variable selection harder.â€  
*(transition)* â€œNext, the restricted eigenvalue provides a broader version of this idea.â€

---

# ğŸ“ **Slides 18â€“19 (with Proof: Coefficient Bounds Integrated)**

---

## **Slide 18 â€“ Restricted Eigenvalue ($\eta_k$)**

$$
\eta_k = \min_{|S|\le k}\lambda_{\min}\!\left(\frac{1}{n}X_S^T X_S\right)
$$

> â€œHere we introduce the *restricted eigenvalue*, denoted $\eta_k$.  
> For every subset $S$ of up to $k$ features, we look at the smallest eigenvalue of the corresponding matrix $(X_S^T X_S / n)$.  
> This matrix describes how strongly those features interact.â€

---

**Intuitive explanation**

> â€œImagine each feature as a direction in space.  
> When some directions almost overlap, it becomes hard to tell them apart â€” this is what happens when $\eta_k$ is small.  
> A large $\eta_k$ means that even small subsets of features remain independent enough for stable estimation.â€

| $\eta_k$ value | Interpretation |
| -------- | ------------------------------------------------------------------ |
| Large | Subsets well-conditioned â€” coefficients stable |
| Small | Some subset nearly redundant â€” coefficients can blow up |
| Zero | Perfect collinearity â€” model cannot distinguish features |

---

**Analogy**

> â€œThink of a tripod:  
> If its legs are far apart, it stands firm (large $\eta_k$).  
> If the legs are nearly parallel, it topples easily (small $\eta_k$).  
> Thatâ€™s the geometric intuition here.â€

---

**Transition to Proof**

> â€œNow, using both $\mu$ â€” the coherence â€” and $\eta_k$ â€” the restricted eigenvalue â€” we can rigorously *prove* the coefficient bounds that we saw earlier.  
> The next slide outlines that proof step by step.â€

---

## **(Slide 18.1) Proof: Coefficient Bounds**

> â€œThis slide formally shows how the $\ell_1$ and $\ell_\infty$ bounds are derived using basic linear algebra tools â€” matrix inversion, perturbation, and inequalities.â€

---

### **Part I â€“ $\ell_1$ Bound (Total coefficient magnitude)**

**Step 1: Start from the optimality condition**

$$
X_S^T(y - X_S \hat{\beta}_S) = 0
$$

> â€œThis is the standard least-squares condition: at the optimum, residuals are orthogonal to the selected features.â€

---

**Step 2: Rearrange**

$$
X_S^T X_S \hat{\beta}_S = X_S^T y
$$

> â€œWe can express $\hat\beta_S$ as a linear transformation of $y$.â€

---

**Step 3: Write $(X_S^T X_S)$ as $(I + G)$**

> â€œIf predictors were perfectly uncorrelated, $(X_S^T X_S)$ would equal the identity $I$.  
> But correlations cause deviations, captured by $G$, where $\|G\|_{1,1} \le \mu[kâˆ’1]$.â€

*(Say aloud)* â€œ$G$ measures how correlated the chosen features are â€” when $\mu$ is small, $G$ is tiny.â€

---

**Step 4: Bound the matrix inverse**

$$
(X_S^T X_S)^{-1} = (I + G)^{-1}, \quad
\|(I + G)^{-1}\|_{1,1} \le \frac{1}{1 - \|G\|_{1,1}}
$$

> â€œThis comes from matrix perturbation theory.  
> As long as $G$ isnâ€™t too large (predictors not too correlated), the inverse is stable.â€

---

**Step 5: Substitute back**

$$
\|\hat{\beta}_S\|_1 \le \frac{1}{1 - \mu[k-1]}\|X_S^T y\|_1
$$

> â€œIn words: the total coefficient magnitude grows as predictors become more correlated.â€

---

### **Part II â€“ $\ell_\infty$ Bound (Maximum coefficient)**

**Step 1: Start from normal equations**

$$
\hat{\beta}_S = (X_S^T X_S)^{-1} X_S^T y
$$

**Step 2: Apply Cauchyâ€“Schwarz**

$$
\|\hat{\beta}_S\|_\infty \le \|(X_S^T X_S)^{-1}\|_2 \, \|X_S^T y\|_2
$$

> â€œThis bounds how large any single coefficient can be, depending on matrix conditioning.â€

---

**Step 3: Use restricted eigenvalue**

$$
\|(X_S^T X_S)^{-1}\|_2 = 1 / \lambda_{\min}(X_S^T X_S) \le 1 / \eta_k
$$

---

**Step 4: Bound the right-hand term**

$$
\|X_S^T y\|_2 \le \sqrt{k}\|y\|_2
$$

---

**Step 5: Combine**

$$
\|\hat{\beta}_S\|_\infty \le \frac{1}{\sqrt{\eta_k}}\|y\|_2
$$

> â€œThe largest coefficient is bounded by $\|y\|_2$ scaled by $1/\sqrt{\eta_k}$ â€” poor conditioning ($\eta_k$ small) makes coefficients explode.â€

---

**Slide 18.1 Summary (to say aloud)**

> â€œSo the $\ell_1$ bound depends on $\mu$ (pairwise correlation),  
> and the $\ell_\infty$ bound depends on $\eta_k$ (global stability).  
> Together they tell us when coefficients stay small â€” and guide how to set the Big-M constant in the MIO problem.â€

---

**Transition to Slide 19**

> â€œNow that weâ€™ve proved these bounds, letâ€™s interpret them statistically â€” how to pick $M$ and ensure stability.â€

---

## **Slide 19 â€“ Parameter Bounds and Big-M Choice**

> â€œThe coefficient bounds we just proved provide explicit guidance for choosing the Big-M constant and understanding numerical stability.â€

$$
\|\hat\beta\|_1
\le
\frac{1}{1-\mu[k-1]}
\sum_{j=1}^{k}|\langle X_{(j)},y\rangle|,
\quad
\|\hat\beta\|_\infty
\le
\frac{1}{\sqrt{\eta_k}}\|y\|_2
$$

---

**Interpretation**

> â€œThe $\ell_1$ bound controls total coefficient size (depends on $\mu[kâˆ’1]$),  
> while the $\ell_\infty$ bound limits the largest coefficient (depends on $\eta_k$).  
> High $\mu$ or small $\eta_k$ means instability and larger coefficients.â€

---

**Practical takeaway**

> â€œPick $M$ large enough to exceed these bounds, ensuring feasibility but not too large to avoid solver instability:  
> typically, $M \approx \frac{1}{\sqrt{\eta_k}}\|y\|_2$.â€

*(transition)* â€œWith bounded coefficients and proper $M$, the MIO is mathematically correct and numerically stable.  
Next, we link these properties to statistical prediction risk.â€

---

## **Slide 21 â€“ Statistical Risk â€” Measuring Prediction Error**

Prediction risk:

$$
E[\|X(\hat\beta - \beta^*)\|_2^2]
$$

- $(\hat\beta - \beta^*)$: estimation error  
- $X(\hat\beta - \beta^*)$: prediction error  
- Expectation averages over data noise

â€œThis measures the average squared error on new data.â€  
*(transition)* â€œNext, we introduce the notation used in the theorem.â€

---

## **Slide 22 â€“ Statistical Optimality Notation Guide**

| Symbol | Description | Meaning |
|:--|:--|:--|
| $E[\cdot]$ | Expectation | Average over data randomness |
| $\hat\beta_{\text{BSS}}$ | Best-subset estimator | From MIO |
| $\hat\beta_{\text{LASSO}}$ | LASSO estimator | From $\ell_1$-penalized regression |
| $\beta^*$ | True coefficients | Ground truth |
| $\sigma^2$ | Noise variance | Unexplained variation |
| $s$ | True sparsity | Non-zeros in $\beta^*$ |
| $(n,p,k)$ | Sample size, features, subset size | Dimensions |
| $\phi_{\min}^2(k)$ | Restricted eigenvalue (for LASSO) | Correlation penalty |

*(transition)* â€œNow we can state the main statistical theorem.â€

---

## **Slide 23 â€“ Theorem on Prediction Risk Bounds**

$$
E[\|\hat{\beta}_{\text{BSS}} - \beta^{*}\|_2^2]
\le
C_1\,\frac{\sigma^2 s \log p}{n}
$$

$$
E[\|\hat{\beta}_{\text{LASSO}} - \beta^{*}\|_2^2]
\le
C_2\,\frac{\sigma^2 s \log p}{\phi_{\min}^{2}(k)\, n}
$$



**Interpretation:**
- Both share the optimal rate $\frac{\sigma^2 s \log p}{n}$.  
- LASSO worsens when $\phi_{\min}(k)$ decreases (high correlation).  
- Best-subset keeps the oracle rate regardless of correlations.

*(transition)* â€œLetâ€™s see why this happens mathematically.â€

---

## **Slide 24 â€“ Proof Sketch for Statistical Optimality**

1ï¸âƒ£ **Oracle case:**  
If we knew the true support $S^*$, OLS on those columns gives risk â‰ˆ $\sigma^2 s/n$ (oracle rate).  

2ï¸âƒ£ **Search penalty:**  
Since we must identify $S^*$ among $p$ candidates, a $\log p$ factor appears â†’ $\sigma^2 s \log p / n$.  

3ï¸âƒ£ **Correlation impact:**  
LASSOâ€™s $\ell_1$ shrinkage introduces dependence on $\phi_{\min}^2(k)$; small $\phi_{\min}$ inflates risk.  
Best-subset avoids this issue.

**Summary:**  
- Best-subset achieves oracle rate without extra assumptions.  
- LASSO degrades under strong correlation but remains cheaper computationally.

*(transition)* â€œNext, weâ€™ll move to the algorithmic implementation and empirical evidence.â€  


# ğŸ“ Part 3 â€” Algorithm, Experiments, and Implications (Slides 25 â€“ 37)

---

## **Slide 25 â€“ When Best Subset Wins**

â€œSo far, weâ€™ve seen that best-subset selection is statistically optimal in theory.  
But when does this actually matter in practice â€” when does it beat alternatives like LASSO?â€

### **When LASSO tends to fail**
â€œLASSO struggles when predictors are highly correlated.  
Because it shrinks coefficients via an $\ell_1$ penalty, correlated variables tend to share or split coefficients unpredictably.  
It also fails when the design matrix is ill-conditioned â€” some columns nearly linearly dependent.â€

### **When Best Subset Selection stays strong**
â€œBest subset selection performs well when:
- The signal-to-noise ratio is decent (true features stand out),  
- The true sparsity level $s$ is known or approximated,  
- The correct support can be recovered.â€

| Method | Statistical Quality | Computational Cost |
| :-- | :-- | :-- |
| Best Subset (MIO) | Oracle-level accuracy | Heavy optimization (~minutesâ€“hours) |
| LASSO | Slightly biased under correlation | Very fast (msâ€“s) |

â€œSo we gain exactness at the price of computation.â€  
*(transition)* â€œLetâ€™s explore how MIO keeps that computation feasible â€” through branch-and-bound.â€

---

## **Slide 26 â€“ Branch-and-Bound Example**

â€œMixed-integer optimization solvers use **branch-and-bound** â€” a smart search over possible subsets.â€

**Example:**  
â€œSuppose we have four features and want to pick two.â€

1ï¸âƒ£ Solve a relaxed version with fractional $z_i \in [0,1]$ â†’ maybe $z^* = (0.7, 0.6, 0.4, 0.3)$ with objective 100.  
2ï¸âƒ£ Branch on $z_1=0$ or $z_1=1$, creating two subproblems.  
3ï¸âƒ£ Compare relaxed objectives (say 105 and 110) with the current best integer solution.  
4ï¸âƒ£ If a branch canâ€™t improve the best solution, itâ€™s pruned.

â€œIn essence, it searches the combinatorial space but skips huge chunks proven suboptimal.â€  
*(transition)* â€œTo decide when to stop, solvers rely on optimality certificates.â€

---

## **Slide 27 â€“ Optimality Certificates**

â€œAn optimality certificate quantifies how close our current solution is to the global optimum.â€

$$
\frac{Z_{\text{true opt}} - Z_{\text{current best}}}{Z_{\text{current best}}} \le \varepsilon
$$

â€œIf this relative gap â‰¤ Îµ, the solution is within Îµ of the true optimum.â€

**Example:**  
â€œA 2% gap means the modelâ€™s objective is at most 2% worse than the absolute best possible â€” a mathematical guarantee.â€

â€œLASSO canâ€™t provide this; MIO can.â€  
*(transition)* â€œHowever, starting MIO from scratch can be slow.  
The authors fix this with a two-stage algorithm.â€

---

## **Slide 28 â€“ Two-Stage Algorithm: Motivation**

â€œThey identify a **cold-start problem** â€” solvers waste time before finding any good integer solution.â€

### **Stage 1 â€” Warm-start with a fast heuristic**
â€œRun a discrete first-order method (â‰ˆ30â€“60s) that quickly finds a good subset and feasible $(\beta,z)$.â€

### **Stage 2 â€” MIO refinement**
â€œStart MIO from that warm-start, not from scratch.  
Because the solver already knows a good upper bound, it prunes aggressively â€” converging in 1â€“10 minutes instead of hours.â€

â€œThis hybrid design merges speed from heuristics and optimality from MIO.â€  
*(transition)* â€œLetâ€™s see the exact steps.â€

---

## **Slide 29 â€“ Complete Algorithm I**

### **Stage 1 â€” Discrete first-order method**

1. Initialize $\beta^{(0)} = 0$  
2. For $t = 1,2,\dots,T_{\max}$:
   - Compute gradient $g^{(t)} = X^T(X\beta^{(t-1)} - y)$  
   - Select support $S^{(t)}$ of $k$ indices with smallest $|g_i|$  
   - Solve restricted least squares on $S^{(t)}$ to update $\beta$

*(Comment)* â€œThis is a greedy yet efficient search that focuses on the most informative features.â€

---

### **Stage 2 â€” MIO with warm start**

- Set $M = 2\|\beta^{(T_{\max})}\|_\infty$  
- Initialize $(\beta, z) = (\beta^{(T_{\max})}, z^{(T_{\max})})$  
- Run solver until optimality gap $< \varepsilon$ or time limit

**Output:** $\beta^*$ â€” optimal coefficients + certificate of global optimality.  
*(transition)* â€œNow letâ€™s see how this performs in experiments.â€

---

## **Slide 30 â€“ Key Experimental Findings**

**Effect of correlation:**

| Correlation $\rho$ | Observation |
| :-- | :-- |
| 0.5 (low) | MIO â‰ˆ LASSO â€” similar performance |
| 0.8 (moderate) | MIO starts outperforming |
| 0.9 (high) | MIO â‰ˆ 30% lower prediction error |

â€œAs $\rho$ increases, MIOâ€™s advantage grows â€” it chooses exact subsets rather than shared coefficients.â€

---

**Computation times**

| Method | Typical runtime |
| :-- | :-- |
| LASSO | msâ€“s |
| Warm-start heuristic | 30â€“60 s |
| MIO (warm start) | 1â€“10 min |
| MIO (cold start) | Often fails to finish |

â€œMIO yields exact sparsity and unbiased coefficients; LASSO shrinks coefficients toward zero.â€  
*(transition)* â€œThe next slide visualizes this difference.â€

---

## **Slide 31 â€“ Key Experimental Figures**

â€œThe figure plots prediction error vs. correlation level.â€

- For $\rho=0.5$, both curves overlap.  
- As $\rho \to 0.9$, LASSOâ€™s error rises steeply, MIOâ€™s stays nearly flat.

*(Gesture)* â€œNotice LASSOâ€™s curve climbing â€” MIO remains robust.â€  
*(transition)* â€œStill, the experiments have some limitations.â€

---

## **Slide 32 â€“ Experimental Design Flaws and Missing Answers**

â€œThe experiments are convincing but limited in scope.â€

- Mostly synthetic data: $n=500$, $p=100$ â†’ relatively easy  
- Real-world high-dimensional cases are much larger

| Domain | Typical $n$ | Typical $p$ |
| :-- | :-- | :-- |
| Genomics | 50 | 20 000 |
| NLP (Text) | $10^6$ | $10^6$ |

â€œIn such regimes, MIO becomes impractical â€” results donâ€™t scale directly.â€  
*(transition)* â€œThe next slide continues this critique.â€

---

## **Slide 33 â€“ Critical Experimental Limitations**

â€œKey missing tests:â€

- Outliers, missing data, or heteroscedastic noise  
- Non-sparse true models (model misspecification)  
- Comparison to modern penalized methods (Elastic Net, SCAD, MCP)

**Parameter-selection gap:**  
â€œIn real use, $k$ is unknown â€” choosing it via cross-validation multiplies runtime by â‰ˆ10Ã—.â€

â€œSo, MIO works beautifully under ideal conditions but not yet for large, messy data.â€  
*(transition)* â€œLetâ€™s consider what this means for high-dimensional statistics.â€

---

## **Slide 34 â€“ Implications for High-Dimensional Statistics I**

â€œScalability is the main issue.â€

â€œWhen $p \gg n$ (e.g., $p=10^6$, $n=10^2$):  
$\binom{p}{k}$ explodes, making even smart branch-and-bound infeasible.  
Statistically, reliable recovery also needs $n \gg s\log p$, which rarely holds.â€

*(transition)* â€œThe next slide summarizes this paradox.â€

---

## **Slide 35 â€“ The High-Dimensional Paradox**

| We want BSS becauseâ€¦ | But it fails becauseâ€¦ |
| :-- | :-- |
| Handles correlation & gives interpretability | Search complexity explodes as $p$ grows |
| Yields unbiased coefficients | Computationally infeasible beyond a few thousand features |

â€œThus, MIO-based subset selection is powerful for moderate $p$, but not a replacement for convex relaxations in large-scale ML.â€  
*(transition)* â€œThe authors close with a balanced summary.â€

---

## **Slide 36 â€“ Summary and Assessment**

### **Core Contributions**

1. **Computational feasibility:**  
   Proved that exact best-subset selection is solvable for moderate $p$ with modern solvers.

2. **Rigorous theory:**  
   Proved mathematical equivalence and explicit parameter bounds.

3. **Statistical optimality:**  
   Showed best-subset achieves the oracle risk rate.

4. **Algorithmic innovation:**  
   Proposed a two-stage solver with optimality certificates in minutes.

---

**Critical assessment:**  
â€œA landmark methodological step, but not a silver bullet.  
It re-establishes best-subset selection as viable, yet convex methods like LASSO and Elastic Net remain essential for high-dimensional problems.â€  
*(transition)* â€œThe final slide captures its broader impact.â€

---

## **Slide 37 â€“ Conclusion and Broader Impact**

â€œTo conclude, *Best Subset Selection via Mixed Integer Optimization* bridges statistics and optimization.  
It shows that with modern solvers, we can achieve exact sparsity and theoretical optimality once thought impossible.â€

### **Broader significance**

â€œBeyond regression, this approach redefines how we view estimation â€” as discrete optimization solvable with rigorous tools.  
Itâ€™s a paradigm shift: not replacing convex methods, but unifying computational and statistical perspectives.â€

---

**Closing line:**  
â€œIn summary:

- Conceptually, it unites classical statistics with modern optimization.  
- Practically, it offers a powerful tool for medium-scale problems.  
- Philosophically, it reminds us that tractability evolves â€” what was impossible twenty years ago may be routine tomorrow.  

**Thank you.**â€
