\documentclass[12pt,a4paper]{article}
% ----------------- Packages -----------------
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{sfmath}
\usepackage{upgreek}
\usepackage{enumitem}
\usepackage{float}
\usepackage{tabularx}
\usepackage[square, numbers, sort&compress]{natbib} % For numbered, compressed citations
\usepackage{array}
\newcolumntype{Y}{>{\centering\arraybackslash}X} % centered, wrapping column
\usepackage{minted}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{newunicodechar}
\newunicodechar{└}{\textSFxx}

\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{textcomp}
\usetikzlibrary{arrows.meta, positioning}
\usepackage{standalone} % allows inclusion of external TeX figures
\usepackage[utf8]{inputenc}
\usepackage{newunicodechar}
\usepackage{textgreek}
\newunicodechar{│}{|}

\tikzset{
  startstop/.style = {ellipse, draw=black, fill=green!20, text width=3cm, align=center, minimum height=1cm},
  process/.style   = {rectangle, draw=black, fill=blue!10, text width=4cm, align=center, rounded corners, minimum height=1cm},
  decision/.style  = {diamond, draw=black, fill=orange!15, text width=3cm, align=center, minimum height=1cm},
  arrow/.style     = {thick, ->, >=Latex}
}

\geometry{margin=1in}
\setstretch{1.2}
\setcounter{secnumdepth}{3}
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}
% ----------------- Document -----------------
\DeclareUnicodeCharacter{251C}{|}
\DeclareUnicodeCharacter{2500}{\textemdash}
\begin{document}
% ---------- Title Page ----------
\begin{titlepage}
\centering
\vspace*{2cm}
{\LARGE\bfseries AI-Based Data Assimilation: Learning the Functional of Analysis Estimation \par}
\vspace{0.5cm}
{\Large Replication and Robustness Study under Chaotic and Stochastic Dynamics \par}
\vspace{2cm}
{\large \textbf{Author:} Sri Vidya Yeluripati \par}
\vspace{0.3cm}
{\large \textbf{Supervisor:} Claudia Strauch \par}
\vspace{0.3cm}
{\large \textbf{Course:} Master Practical(WS 2024/25) \par}
\vspace{0.3cm}
{\large \textbf{Date:} 11 November 2025 \par}
\vspace{0.3cm}
{\large \textbf{Code Repository: \href{https://github.com/SriVidyaYeluripati/Data-Assimilation}{Github}} \par}
\vfill
\end{titlepage}
\tableofcontents
\newpage
\section*{Abstract}
Data assimilation (DA) is central to state estimation in nonlinear dynamical systems, such as those in atmospheric and ocean sciences, where chaotic sensitivity to initial conditions limits forecasting accuracy. Classical methods, such as three-dimensional variational assimilation (3D-Var) and Ensemble Kalman Filters (EnKF), depend on linear-Gaussian assumptions and costly iterative optimization. The \textbf{AI-Based Data Assimilation (AI-DA)} framework learns $\Phi$ by \emph{minimizing a differentiable 3D-Var objective} directly (self‑supervised), using only observations, a background state, and their covariances at training time (no analysis labels).
\medskip
\noindent
This study replicates and evaluates AI-DA on the canonical \textbf{Lorenz-63 system}, testing stability and generalization under increasing observation noise ($\sigma_{\mathrm{obs}} = 0.05$--$1.0$) and partial observability (\textit{X}, \textit{XY}, \textit{X}$^2$). Three neural architectures—a Multilayer Perceptron (MLP), Gated Recurrent Unit (GRU), and Long Short-Term Memory (LSTM)—are compared for their ability to approximate the analysis functional.
\medskip
\noindent
Two key findings emerge: (1) recurrent models (GRU/LSTM) outperform MLPs, and (2) a \textbf{stochastic resampling regime} for background priors improves robustness. Test RMSE is computed against the (synthetic) true state used only for offline evaluation, preventing failures such as \textbf{Attractor Escape} observed under fixed priors. These results confirm that functional learning can successfully emulate data assimilation for low-dimensional chaotic systems, provided temporal modeling and stochastic regularization are incorporated.
\newpage
\section{Introduction}
\subsection{Motivation}
\noindent
State estimation in chaotic dynamical systems is a fundamental challenge in science and engineering, particularly in high-stakes fields such as numerical weather prediction (NWP) and oceanography. Chaotic systems, epitomized by the \textbf{Lorenz-63} atmospheric convection model, exhibit sensitive dependence on initial conditions, where even infinitesimal errors in the initial state or forecast grow exponentially over time, making long-term prediction practically impossible \cite{lorenz_1963}. To mitigate this divergence, \textbf{Data Assimilation (DA)} techniques optimally combine noisy observations with the model’s short-term forecast (the \textit{background} state) to produce a refined \textit{analysis} state, thereby constraining the trajectory to the true dynamical manifold and limiting forecast uncertainty.
\begin{figure}[H]
\centering
\includegraphics[width=0.4\linewidth]{lorenz.png}
\caption{Lorenz Attractor (sample trajectories)}
\label{fig:lorenz}
\end{figure}
\noindent
Traditional DA methods rely on linearity and Gaussian error assumptions. Variational approaches, such as 3D-Var, minimize a cost function that balances background and observation information but require iterative solvers or matrix inversions and depend on accurate covariance specifications (B and R). The \textbf{AI-Based Data Assimilation (AI-DA)} framework replaces explicit variational optimization by learning a mapping $\Phi$ that minimizes a differentiable 3D‑Var objective directly (self‑supervised), using only observations, a background state, and their covariances (no analysis labels). 
\noindent
A critical challenge in this setting is that the true state ($x_{\text{true}}$) is unobservable, requiring the network to learn from surrogate analysis targets rather than from the ground truth. This work systematically investigates how different neural architectures—a memory-less Multilayer Perceptron (MLP) versus recurrent models (GRU, LSTM)—approximate the analysis functional, testing the hypothesis that temporal sequence modeling enhances stability and coherence in chaotic, partially observed systems.

\newpage
\subsection{ Project Goals}
The central aim of this project is to develop and rigorously evaluate a machine learning-based data assimilation framework, utilizing the Lorenz-63 system as a demanding testbed. We specifically seek to \textbf{train neural network models to learn the analysis functional ($\Phi$)}—the state correction given a background and observations—and evaluate their performance across systematic variations in the experimental setup.

\begin{table}[H]
\centering
\small
\begin{tabularx}{0.85\textwidth}{l X}
\toprule
\textbf{Setting} & \textbf{Value} \\
\midrule
\# Trajectories (train/test) & 1000 / 500 \\
Time steps per trajectory & 200 \\
$\Delta t$ & 0.01 \\
Observation modes & $x$, $xy$, $x^2$ \\
Noise $\sigma$ & 0.05, 0.10, 0.50, 1.00 \\
Sequence window $L$ & 5 \\
Architectures & MLP, GRU, LSTM \\
Hidden size & 64 \\
Batch size & 256 \\
Epochs & 30 \\
Loss & 3D-Var objective using background covariance $B$, observation covariance $R(\sigma)$, and mode-specific observation operator $H$. \\
\bottomrule
\end{tabularx}
\caption{Experimental setup and hyperparameters.}
\label{tab:hyperparams}
\end{table}


\noindent
Key project goals, designed to provide a comprehensive understanding of the method's boundaries, include:
\begin{enumerate}
\item \textbf{Systematic Architectural Benchmarking:} To investigate the impact of temporal context on assimilation performance, we compare three architectures—MLP, GRU, and LSTM—on identical tasks, identifying whether recurrent models offer an advantage over a memory-less MLP in this sequential filtering problem.

\item \textbf{Evaluation Across Multiple Observability Conditions:} The learned analysis is tested under three distinct observation scenarios: linear single-variable (\textit{x} mode), multivariate (\textit{xy} mode), and the challenging nonlinear observation ($x^2$ mode) to assess how the networks handle partial and nonlinear data.
\item \textbf{Rigorously Testing Noise Robustness:} We evaluate performance across a wide range of observation noise levels ($\sigma=0.05, 0.1, 0.5, 1.0$). The explicit goal is to take the strongest models (GRU/LSTM) and \textbf{"try to break them"} by systematically increasing noise to see "when a method breaks" \cite{lorenz_1963}.
\item \textbf{Quantifying Generalized Performance:} Due to the inherent stochasticity (noise addition) and chaotic dynamics, performance must be derived from \textbf{averages} across multiple runs or test sets (e.g., 500 held-out test trajectories). This ensures that the reported results reflect the *average* performance for this *type* of problem in general, and not one specific stochastic instance \cite{lorenz_1963}.
\item \textbf{Comprehensive Error Analysis:} We measure error using the Root Mean Square Error (RMSE) and quantify the error in \textbf{all three state components} (X, Y, Z), as the model must accurately recover unobserved dimensions (like Z).
\end{enumerate}
\subsection{Scope of the Project}
This work conducts a highly \textbf{controlled study} centered exclusively on the synthetic Lorenz-63 model. The synthetic environment ensures that we can systematically vary factors (noise, observation mode, architecture) and repeat many trials to gather robust statistics, ensuring a fair cross-model comparison.
\medskip
\noindent
The project implements and evaluates the learned analysis functionality within two fundamentally different sequential assimilation schemes:
\begin{enumerate}
\item \textbf{Diagnostic Control - Baseline (No-Mean) Regime}
This is the minimal learning configuration. The network receives \textbf{only the observation sequence} ($y_{t}$ or $y_{1:\tau}$) and explicitly excludes all prior information from the background—neither the ensemble mean ($\bar{x}$) nor the covariance ($B$) is supplied. The Baseline MLP acts as this control, isolating the pure functional approximation capacity of the network, which is dependent only on observed data. The inclusion of this configuration is essential to quantify the necessary contribution of background information in the more complex regimes and to prove that assimilation behavior does not emerge purely from observed data.

\item \textbf{Secondary Benchmark - FixedMean Regime}
This scheme serves as a critical diagnostic benchmark designed to rigorously test the system's resilience to prior bias. In the FixedMean mode, the background state ($\bar{x}$) and covariance ($B$) remain \textbf{constant} throughout training, based on a single, static prior initialization. This non-sequential approach stresses the network by forcing it to correct from an increasingly biased prior, leading to expected instability. If the loss explodes on the fixed mean, this is acceptable, as it "just means the problem is too difficult" for this architecture under those specific noise conditions \cite{lorenz_1963, kalman_1960}. Its predictable instability, resulting in catastrophic failure modes like \textbf{Attractor Escape} on unseen data, provides critical insight into the limitations of the AI-DA functional when sequential feedback is compromised.
\item \textbf{Resample Regime (main experimental scheme)}
This is the main experimental scheme and the focus of our conclusions. This regime emulates a stable, realistic sequential filtering scenario. In the Resampled Mean regime, the background ($\bar{x}$) and covariance ($B$) are \textbf{dynamically resampled} at every epoch or batch from the ensemble distribution \cite{ai_da_fablet}. This stochastic variability acts as data-level regularization, preventing overfitting to a fixed background and forcing the network to learn robust, generalized state corrections. By concentrating on this stable, closed-loop assimilation setting, we ensure that the models are assessed under conditions optimized for long-term temporal coherence and stability, mitigating the risk of divergence observed in the FixedMean regime.
\end{enumerate}
The project evaluates performance using the Root Mean Square Error (RMSE) of the state estimate, averaged over held-out test trajectories, and the \textbf{percentage improvement} metric, which contextualizes gains relative to the Baseline or FixedMean performance. Furthermore, trajectory visualizations confirm that the learned analysis maintains the chaotic attractor structure and generalizes to unseen data.
\subsection{ Challenges and Limitations}
Implementing the AI-DA framework presented several challenges and documented limitations:
\begin{itemize}
\item \textbf{FixedMean Instability and Attractor Escape:} The FixedMean assimilation scheme proved to be \textbf{fundamentally unstable beyond moderate noise} ($\sigma > 0.5$) for the Lorenz-63 system. Without sequentially feeding back the analysis, errors accumulated, confirming our expectation that if the loss explodes, "the problem is too difficult" \cite{lorenz_1963}. This resulted in failure modes such as Attractor Escape on unseen trajectories, justifying the focus on the stable ResampledMean regime for primary conclusions.
\item \textbf{Overfitting in Recurrent Networks:} Recurrent models (GRU and LSTM) have a high parameter count relative to the 3D state, creating a risk of memorizing training dynamics. The LSTM, in particular, showed initial tendencies toward \textbf{overfitting to the background}, leading to reduced generalization compared to the GRU in some tests \cite{neural_da_bocquet}.

 This challenge was successfully mitigated by the stochastic resampling strategy.
\item \textbf{Noise and Observability Limits:} FixedMean becomes fragile beyond $\sigma \approx 0.5$. Under Resample, x and xy remain stable up to $\sigma=1.0$, while $x^2$ shows higher variability and sensitivity.
\item \textbf{Lack of Transparency:} The neural network outputting the analysis is a \textbf{"black box"} function. This lack of interpretability prevents the easy extraction of analytical insights (such as Kalman gains or explicit covariance adjustments), forcing reliance primarily on empirical RMSE and stability metrics for validation.
\end{itemize}
\subsection{Contributions}
This project makes several detailed contributions to the study of data assimilation with machine learning, focusing on rigorous testing and systematic documentation:
\begin{enumerate}
\item \textbf{Viable Learned Analysis Update:} We demonstrate a robust, learning-based approach using MLP, GRU, and LSTM models to approximate the 3D-Var analysis functional on the Lorenz-63 system. The network successfully learns to estimate the optimal \textbf{analysis increment ($\Delta x$)} that minimizes the variational cost \cite{ai_da_fablet}.
\item \textbf{Systematic Benchmarking of Architectures and Regimes:} A comprehensive benchmarking is presented, comparing all three architectures across three distinct observation modes (x, xy, x²) and a wide spectrum of observation noise levels ($\sigma=0.05$ to $1.0$).
\item \textbf{Validation of Key Insights:} The project successfully validates the two central insights mandated by the project requirements:
\begin{itemize}
\item \textbf{Temporal context helps, but there is no clear winner:} Under resampling, all three architectures are close; GRU is often slightly ahead in xy, while MLP matches or exceeds RNNs in several x and x$^2$ settings.
\item \textbf{Stochastic Resampling is Essential:} The Resample regime substantially reduces the normalized Hausdorff distance compared to the baseline/fixed mean and prevents the catastrophic failures seen with a fixed prior on unseen trajectories (see Sec. 4.5).
\end{itemize}
\item \textbf{Failure Analysis and Stability Quantification:} By documenting the instability of the FixedMean approach and systematically increasing noise up to $\sigma=1.0$, the project quantitatively determines the critical thresholds (i.e., $\sigma \approx 0.5$) at which the learned methods begin to degrade gracefully or fail outright.
\item \textbf{Foundation for Future Generative Work:} The robust setup and performance baselines established here provide the necessary foundation for future exploration of advanced models, such as Generative Neural Networks (e.g., Generative LSTM), as noted in the project outlook.
\end{enumerate}

\section{Background}
\subsection{Data Assimilation (DA) and the Variational Principle}
Data assimilation (DA) is a specialized methodology essential for state estimation in complex physical and geophysical systems, particularly those characterized by high dimensionality and sensitive dependence on initial conditions, such as atmospheric and ocean models. The fundamental purpose of DA is to estimate the true, latent state of a system by optimally combining prior knowledge—known as the \textit{background} or forecast state—with sparse, noisy observations gathered from sensors. This optimal combination results in the \textit{analysis} state, which is then used as the initial condition for the subsequent model forecast.

\medskip
\noindent
Classical DA methods originate from statistical estimation theory. The \textbf{Kalman Filter} provides a recursive solution for state estimation in linear Gaussian systems, operating through a sequential prediction–update cycle \cite{kalman_1960}. Extensions like the Ensemble Kalman Filter (EnKF) \cite{ensemble_methods} and variational methods, such as \textbf{Three-Dimensional Variational Assimilation (3D-Var)}, extend this principle to nonlinear and high-dimensional systems. These conventional approaches generally rely on Gaussian assumptions regarding error distribution and require accurate specification of error covariance matrices (background covariance $B$ and observation covariance $R$).

\medskip
\noindent
The \textbf{3D-Var} method frames the analysis problem as an optimization task, seeking the analysis state ($a^*$) that minimizes a cost function ($J(a)$) which measures the distance of the analysis state from both the background forecast and the observations. Mathematically, the analysis minimizer $a^*$ is defined by the following expression:
$$
a^* = \arg\min_a \left[ (a - b)^T B^{-1} (a - b) + (y - H a)^T R^{-1} (y - H a) \right]
$$
where $b$ is the prior background state, $y$ is the observation vector, $H$ is the observation operator that maps the state to the observed space, and $B$ and $R$ represent the prior and observation error covariances, respectively \cite{variational_da}. The solution requires computationally expensive iterative solvers or matrix inversions during inference, which poses a limitation for real-time or embedded applications.
\subsection{The AI-Based Data Assimilation (AI-DA) Paradigm}
The \textbf{AI-Based Data Assimilation (AI-DA)} framework, pioneered by Rucka et al., reframes the complex and computationally intensive DA analysis update as a supervised learning problem in functional approximation \cite{ai_da_fablet}. Instead of solving the variational optimization problem iteratively during inference, AI-DA proposes learning a parametric surrogate for the analysis functional ($\Phi$) via a neural network.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{classical DA.png}
\caption{ 3D-Var Functional/AI-DA Concept}
\label{fig:lorenz}
\end{figure}
\medskip
\noindent
The core objective is to learn $\Phi$ so that its output \emph{minimizes} the 3D‑Var cost:
\[
\hat{x}^a = \Phi(y, \bar{x}, H, R, B),\quad
J(x)=\tfrac12(x-\bar{x})^\top B^{-1}(x-\bar{x})+\tfrac12(Hx-y)^\top R^{-1}(Hx-y).
\]
Training is \emph{self‑supervised}: we backpropagate through $J(\hat{x}^a)$; no analysis labels are required.
This machine learning approach offers several critical advantages:
\begin{enumerate}
\item \textbf{Inference Efficiency:} Once trained, the network can infer the analysis state through a single forward pass, thereby eliminating the need for iterative numerical solvers and speeding up the analysis cycle significantly.
\item \textbf{Nonlinear Approximation:} Neural networks have the capacity to learn complex, non-linear relationships directly from data, potentially capturing dynamics that might be overlooked or simplified by traditional linearized variational methods.
\end{enumerate}
\subsubsection*{The Critical Constraint: Absence of True Ground Truth}
A crucial element is that the true state $x_{\text{true}}$ is \textbf{unobservable} in realistic DA. Training is therefore self-supervised: we backpropagate through the 3D-Var cost $J(\hat{x}^a)$ and do not use analysis labels ($a^\star$). Truth and $a^\star$ are computed only for offline evaluation and diagnostics (e.g., RMSE and geometry metrics). This constraint means the network must learn to reconstruct trajectories based on observations and the statistical information provided by the background state and its error covariance.

\subsection{The Lorenz-63 Testbed and Problem Formulation}
To rigorously evaluate the viability and limitations of the AI-DA framework, this project employs the \textbf{Lorenz-63 system} as a canonical testbed \cite{lorenz_1963}. Introduced by Edward Lorenz in 1963, this three-dimensional system models atmospheric convection and is defined by the set of coupled ordinary differential equations:
$$
\frac{dx}{dt} = \sigma (y - x), \quad \frac{dy}{dt} = x(\rho - z) - y, \quad \frac{dz}{dt} = xy - \beta z.
$$
The standard parameters used are $\sigma = 10$, $\rho = 28$, and $\beta = 8/3$. The system exhibits chaotic dynamics characterized by the double-wing attractor, making it a demanding benchmark for testing state estimation methods due to its sensitive dependence on initial conditions.
\medskip
\noindent
The problem formulation involves estimating the full 3D state vector $\mathbf{x} = (x, y, z)$ from \textbf{partial and noisy observations}. The system is evaluated under three distinct observability conditions to probe information completeness and nonlinearity:
\begin{itemize}
\item \textbf{Linear Single-Variable Mode (\textit{x}):} Only the first component $x$ is observed ($H(\mathbf{x})=[x]$).
\item \textbf{Multivariate Mode (\textit{xy}):} Two linear components are observed ($H(\mathbf{x})=[x, y]$).
\item \textbf{Nonlinear Mode (\textit{x²}):} A nonlinear transform of the state is observed ($H(\mathbf{x})=[x^2]$), introducing complexity and ill-posedness.
\end{itemize}
In all cases, observations are corrupted by Gaussian noise ($\epsilon \sim \mathcal{N}(0, \sigma^2 I)$) across four noise levels ($\sigma \in \{0.05, 0.10, 0.50, 1.00\}$) \cite{lorenz_1963}. The models must recover the full 3D state, including the unobserved components (like $z$), based on these limited and uncertain inputs.
\subsubsection*{Architectural Comparison and Regimes}
To systematically investigate the efficacy of the learned analysis functional, the project benchmarks three primary model classes against a minimal control model:
\begin{enumerate}
\item \textbf{ Baseline MLP (No-Mean:)}
The \textbf{Baseline MLP} constitutes the minimal learning configuration and serves as the required baseline comparison. This architecture is a simple two-layer fully connected network (feedforward). Critically, it \textbf{excludes all prior background information} ($\bar{x}$ or $B$). It is implemented using \textbf{tanh activation}. Its purpose is to quantify the network's pure learning capacity, isolating assimilation behavior that emerges solely from observed data, independent of the variational structure or prior knowledge \cite{ai_da_fablet}.
\item \textbf{Multilayer Perceptron (MLP)}
The primary MLP is a standard feedforward architecture that \textbf{explicitly conditions on the background mean} ($\bar{x}$) alongside the observation ($y_t$). It operates as a memory-less functional approximator, predicting an additive correction ($\Delta x$) to the background state. The MLP provides a lightweight reference model, performing surprisingly well in low-noise regimes \cite{ai_da_fablet}.
\item \textbf{Gated Recurrent Unit (GRU)}
The GRU is a recurrent network designed to capture temporal dependencies by processing a sequence of observations \cite{neural_da_bocquet}. It aggregates temporal information into a hidden state ($h_t$) which, when combined with the background ($\bar{x}$), informs the final analysis update. This architecture tests the hypothesis that time dependency is necessary for system coherence. The GRU often achieved the best improvement and generalization, avoiding the overfitting observed in the LSTM in some fixed-background tests \cite{neural_da_bocquet}.
\item \textbf{Long Short-Term Memory (LSTM)}
The LSTM mirrors the GRU but includes explicit cell-state memory, allowing it to capture potentially longer-range dependencies. It is a more expressive recurrent network, used to confirm if deeper sequence modeling capacity provides better stability, particularly in nonlinear ($x^2$) observation modes and high-noise settings. While often performing strongly in terms of absolute RMSE, it showed signs of overfitting to the background in early tests \cite{neural_da_bocquet}.
\end{enumerate}
\medskip
\noindent
These architectures are analyzed across three key training paradigms defined by the handling of the background prior ($\bar{x}$):
\begin{itemize}
\item \textbf{Baseline Regime (No-Mean):} Uses the Baseline MLP (or similar architecture) with \textbf{no background prior}.
\item \textbf{FixedMean Regime:} Uses a \textbf{static, uncorrected prior} ($\bar{x}$) throughout training, serving as a critical diagnostic to reveal instability and failure modes (e.g., Attractor Escape) \cite{lorenz_1963}.
\item \textbf{Resample Regime:} Uses a \textbf{dynamically updated prior} by resampling the background at every training step, which acts as data-level regularization to improve generalization and robustness \cite{ai_da_fablet}.
\end{itemize}
This structured comparison is essential for documenting the key project insights: the effectiveness of temporal architectures and the necessity of the resampling strategy for robust AI-DA performance.

\newpage
\section{Methods}
The methodology deployed in this project is characterized by a high degree of control and systematic comparison, designed to isolate the performance contributions of architectural choice and background conditioning strategy within the AI-DA framework. All experiments were conducted within a unified pipeline to ensure complete traceability and comparability across model variants, observation modes, and noise regimes.
\subsection{Common Evaluation Protocols}
A standardized evaluation protocol was established for all models—MLP, GRU, LSTM, and the FixedMean baseline—adhering to the core philosophy of \textit{AI-Based Data Assimilation: Learning the Functional of Analysis Estimation} \cite{ai_da_fablet, neural_da_bocquet}. The central objective was to rigorously quantify each model’s ability to approximate the analysis functional ($\Phi$)—the mapping from noisy observations and prior information to the posterior analysis state that minimizes the 3D-Var cost function \cite{variational_da}. To counteract the effects of stochasticity inherent in chaotic systems and noise injection \cite{lorenz_1963}, a strict regime of performance averaging was implemented.

\subsubsection{Assimilation Evaluation Setup}
Each model learns
\[
(y_{t-L+1:t}, H, R, \bar{x}_t) \mapsto \hat{x}^a_t = f_\theta(\bar{x}_t, y_{t-L+1:t}),
\]
by minimizing the 3D‑Var cost $J(\hat{x}^a_t)$ during training. At test time, performance is computed against the held‑out true state $x_{\text{true}}$.

\subsection{Evaluation Metric: Root Mean Square Error and Trajectory Fidelity}
\label{sec:rmse_trajectory_fidelity}
The principal quantitative measure of assimilation accuracy deployed throughout this study is the \textbf{Root Mean Square Error (RMSE)}. However, given the context of state estimation in chaotic, partially observed systems, mere instantaneous RMSE is insufficient. We utilize RMSE as the foundation for a suite of integrated metrics designed to assess three critical aspects: instantaneous accuracy, generalized stability over \textbf{unseen trajectories}, and long-term \textbf{temporal coherence}.
\subsubsection{Core Quantitative Metrics}
\paragraph{A. Root Mean Square Error (RMSE)}
The RMSE is calculated between the predicted analysis state $\hat{x}^a_t$ and the true state $x^{\text{true}}_t$:
\[
\text{RMSE}=\sqrt{\frac{1}{N}\sum_{i=1}^N \lVert \hat{x}^a_i - x^{\text{true}}_i\rVert^2 }.
\]
This metric is computed across \textbf{all three components} ($X$, $Y$, and $Z$) of the latent state vector and then averaged across the entire validation set (typically 500 held-out test trajectories $\times$ 200 time steps) to provide a stable, generalized indicator of performance.
Due to the partial observability inherent in the problem setup, the network’s capacity to accurately recover the \textbf{unobserved $Z$ component} is a critical, high-priority indicator of successful assimilation.

\paragraph{Improvement metrics}
We report two related improvement measures for clarity and reproducibility.

\textbf{(i) Improvement relative to the background (primary).}
This is the primary metric used throughout the paper:
\[
\mathrm{Improvement}_{\mathrm{bg}}(\%) \;=\; 100 \cdot 
\frac{\mathrm{RMSE}_b - \mathrm{RMSE}_a}{\mathrm{RMSE}_b + \varepsilon},
\qquad \varepsilon=10^{-12},
\]

where $\mathrm{RMSE}_b$ is the pre-assimilation (background) RMSE and
$\mathrm{RMSE}_a$ is the post-assimilation RMSE.

\textbf{(ii) Improvement relative to FixedMean (secondary).}
When we explicitly compare methods to the FixedMean baseline we report:
\[
\mathrm{Improvement}_{\mathrm{FixedMean}}(\%)
\;=\;100\cdot\left(1-\frac{\mathrm{RMSE}_\text{model}}{\mathrm{RMSE}_\text{FixedMean}}\right).
\]

\noindent\textit{Note:} All figures and tables use the named metric in their captions; when a secondary metric is shown this is explicitly stated in the caption. We report mean~$\pm$~std in the main text and provide median and IQR for quantities affected by outliers (e.g., when attractor escape occurs).

\subsubsection{Temporal Coherence and Trajectory Fidelity Protocols}
Due to the sensitive dependence on initial conditions characteristic of the Lorenz-63 system, success demands that the learned analysis functional ($\Phi$) must continuously constrain the estimated trajectory to the true chaotic manifold, preventing the exponential growth of errors (Lyapunov instability) \cite{lorenz_1963}.
To rigorously quantify this behavior, the following protocols are enforced.
\paragraph{A. Temporal Error Evolution and Stability Analysis}
Evaluation explicitly includes tracking the \textbf{Euclidean error evolution over time} along the entire assimilation window (200 time steps) for held-out test trajectories.
This analysis serves multiple diagnostic purposes:
\begin{enumerate}
\item \textbf{Convergence Rate:} We measure the speed and stability of error correction, confirming that successful assimilation is characterized by \textbf{fast convergence} (e.g., stabilizing error within $\tau \approx 50$ steps).
\item \textbf{Stability Comparison:} We contrast the temporal error profiles between stable regimes (like \textbf{Resample}) and unstable regimes (like \textbf{FixedMean}). Unstable regimes are documented to exhibit catastrophic error oscillations or outright divergence, confirming the necessity of stochastic regularization for sequential stability.
\item \textbf{Generalization Across Steps:} Since the models are evaluated on 500 trajectories $\times$ 200 time steps, the analysis verifies that the performance reflects the \textbf{average behavior} for this class of problems rather than a favorable single instance.
\end{enumerate}
\paragraph{B. Attractor Geometry Preservation (Hausdorff Distance)}
To provide a stringent, quantitative check of the network’s ability to learn a \textbf{generalizable update rule} rather than merely memorizing training dynamics, we evaluate the preservation of the chaotic system’s geometry on entirely \textbf{unseen test trajectories}.

\begin{enumerate}
\item \textbf{Metric Definition:} The \textbf{Hausdorff distance} is utilized as the metric to quantify the maximum displacement between the true attractor (long-term sampled trajectory) and the attractor reconstructed by the learned analysis trajectory ($\hat{x}^a$).
\item \textbf{Failure Diagnosis:} This metric explicitly diagnoses failure modes, particularly \textit{Attractor Escape}, where the analysis trajectory drifts toward stable fixed points or breaks the expected chaotic structure entirely—a frequent failure observed in the unstable FixedMean regime \cite{lorenz_1963}.

\item \textbf{Quantitative Note:} If computed, we report the Hausdorff distance as a diagnostic; we do not enforce a fixed threshold in this report.
\end{enumerate}
\paragraph{C. Component-wise Analysis of the Correction Term ($\Delta x$)}
For deep architectural insight, the instantaneous \textbf{analysis increment} (or correction term, $\Delta x = \hat{x}^a - \bar{x}$) is decomposed and analyzed component-wise ($X$, $Y$, $Z$).
\begin{enumerate}
\item \textbf{Recurrent Smoothing:} We examine the residuals for $X$ and $Y$ components to compare the \textbf{variance ($\sigma_{\text{res}}$)} produced by the memory-less MLP versus the recurrent models (GRU/LSTM). Recurrent networks are hypothesized to exhibit \textbf{lower variance in corrections} (e.g., in the range of $0.1$–$0.2$) because their temporal awareness smooths the update, mitigating high-frequency noise artifacts introduced by instantaneous observations \cite{neural_da_bocquet}.

\item \textbf{Unobserved Component Behavior:} The corrections applied to the unobserved $Z$ component are analyzed, noting their expected \textbf{smooth, monotonic correction profile} due to the influence of coupled dynamics ($\frac{dz}{dt} = xy - \beta z$).
\end{enumerate}
\subsubsection{Standardization and Cross-Model Comparability}
To guarantee fairness, all experiments adhered to a fixed configuration regime: identical dataset partitions, learning rate schedules, optimization parameters (Adam, learning rate $1\times 10^{-3}$), and consistent random seeds across Python, NumPy, and PyTorch for full reproducibility of metric logs. Evaluation was performed strictly post-training, using the final-epoch checkpoint archived with the run metadata (see §3.4.4).For key pairwise comparisons (e.g., GRU vs MLP) we report 95\% bootstrap confidence intervals and perform paired tests (Wilcoxon signed-rank or paired bootstrap)
on per-run metrics when appropriate. Tables and figure captions indicate the sample size (N) and the confidence intervals used to assess statistical significance.

\subsubsection{Reproducibility and Visualization}
\subsubsection*{Reproducibility statement}
All experiments were executed from the code repository referenced on the title page; the Git commit hash used for each run is archived in the corresponding
\texttt{results/run\_*/} folder. Experiments used Python 3.x and PyTorch (version recorded in \texttt{environment.yml} in the repo). Each run logs the full \texttt{config.yaml}, the RNG seed, and the Git commit hash; these artifacts are archived in the run directory. Where relevant, scripts to reproduce the figures from saved metrics (without retraining) are provided in the repository's `scripts/` folder.
For numerical stability we
solved linear systems involving $B$ and $R$ via Cholesky factorizations rather than explicit inverses. Each run logs the full \texttt{config.yaml} (hyperparameters),
the RNG seed, and the checkpoint used for evaluation. A small utility \texttt{set\_seed(seed)} was used to initialize Python, NumPy and PyTorch RNGs
(deterministic CUDA flags were set where available). Minimal reproduction of the main figures is possible from the stored \texttt{results/*/metrics/} CSV/JSON
files without retraining; scripts that regenerate report figures from saved metrics are included in the repository.
\noindent
All experimental outputs were saved following a standardized, time-stamped directory schema. This included RMSE tables, convergence metrics, and publication-quality plots (e.g., RMSE-versus-noise curves and improvement-over-baseline bar plots). This structured approach ensures that all results are traceable across models, noise levels, and observation modes, supporting the project's emphasis on rigor and reproducible science.
\subsection{Data Generation and Problem Setup}
The experimental environment simulates the challenging state estimation problem using the \textbf{Lorenz-63 system} as the core chaotic testbed \cite{lorenz_1963}. This highly controlled, synthetic setup allows for systematic variation of key factors (noise, observation mode) essential for stress-testing the AI-DA method.
\subsubsection{The Lorenz-63 Dynamical System}
The Lorenz-63 system is defined by its three coupled ordinary differential equations \cite{lorenz_1963}:
$$
\frac{dx}{dt} = \sigma (y - x), \quad\frac{dy}{dt} = x(\rho - z) - y, \quad\frac{dz}{dt} = xy - \beta z
$$
Standard chaotic parameters ($\sigma = 10$, $\rho = 28$, and $\beta = 8/3$) were used, and the system was integrated via a fourth-order Runge--Kutta scheme with fixed time step $\Delta t = 0.01$. Long trajectories (20,000 time steps) were generated to ensure sufficient coverage of the system's characteristic double-wing attractor manifold \cite{lorenz_1963}.

\subsubsection{Observation Operators and Noise Regimes}
The problem involves estimating the full 3D state vector $\mathbf{x} = (x, y, z)$ from \textbf{partial and noisy observations}. Three distinct observation modes ($H$) were implemented to systematically investigate varying levels of information richness and non-linearity, a key component of the experimental design.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{obsoperators.png}
\caption{Observation Modes and Noise Regimes}
\label{fig:placeholder}
\end{figure}
\begin{itemize}
\item \textbf{Linear Single-Variable Mode ($x$):} Observing only the first component, $H(\mathbf{x})=[x]$.
\item \textbf{Multivariate Mode ($xy$):} Observing two linear components, $H(\mathbf{x})=[x, y]$, providing increased informational coupling.
\item \textbf{Nonlinear Mode ($x^2$):} Observing a nonlinear transform of the state, $H(\mathbf{x})=[x^2]$. This mode introduces significant ill-posedness, as the operator removes sign information and amplifies noise.
\end{itemize}
In all modes, observations were corrupted by additive Gaussian noise ($\epsilon \sim \mathcal{N}(0, \sigma^2 I)$). To thoroughly stress-test the method and determine "when a method breaks" \cite{lorenz_1963}, four distinct noise levels were evaluated: $\sigma \in \{0.05, 0.10, 0.50, 1.00\}$ \cite{lorenz_1963}.
\subsubsection{Generation of Background and Analysis States}

For each ground-truth trajectory, background priors were generated from an independent climatological ensemble. Concretely, we first generated a
large ensemble of states by sampling from long Lorenz-63 trajectories (ensemble size $E$). For the Resample regime, at each training minibatch we sampled
$m$ ensemble members (with replacement) and computed the batch background mean $\bar{x}$ and empirical covariance $B$ from those $m$ members.
To ensure numerical stability, $B$ was regularized as $B \leftarrow B + \varepsilon I$ with $\varepsilon = 10^{-6}$. For the FixedMean regime we used the climatological
mean $\bar{x}_{\text{clim}}$ (computed from the full ensemble) and the corresponding climatological covariance $B_{\text{clim}}$ as a static prior.
 \cite{ensemble_methods}.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{Backgroundstats.png}
\caption{Generation of Background and Analysis States}
\label{fig:placeholder}
\end{figure}
\noindent
In this study we used (unless otherwise stated) $E=10000$ and $m=32$, which provided a practical balance between ensemble diversity and computational cost.
The true analysis ($a^\star$) used for offline diagnostics was obtained by numerically minimizing the 3D-Var cost (via gradient-based L-BFGS/fixed-point
iterations) and was used only for evaluation — never during training.
\subsection{Model Architectures and Training Configuration}
This section describes the four parameterized estimators used to learn the analysis functional, detailing the architectural features designed to test the hypothesis of temporal dependency, and the adherence to the loss function.
\subsubsection{Input Encoding and Normalization}
The inputs to the networks include the observation vector ($y_t$) and, depending on the regime, the background state mean ($\bar{x}_t$). For recurrent architectures (GRU and LSTM), the input is a sequence of observations ($\{y_{t-\tau+1},\ldots,y_t\}$) to capture temporal context \cite{neural_da_bocquet}. All inputs are rigorously standardized feature-wise (zero mean, unit variance) using the training-split statistics to ensure numerical stability.
\subsubsection{ Architecture Definitions}
The project systematically benchmarks four model setups, ensuring that architectural comparability (e.g., matching hidden size, 64) is maintained where possible.
\begin{table}[H]
\centering
\small
\begin{tabularx}{0.85\textwidth}{l X c c}
\toprule
\textbf{Model} & \textbf{Description} & \textbf{Activation} & \textbf{Hidden width} \\
\midrule
Baseline MLP (No-Mean) & 2 linear layers, maps $y_t\mapsto\hat{x}^a_t$ (no $\bar{x}$ or $B$) & Tanh & 32 \\
MLP (with $\bar{x}$) & Concatenate $[\bar{x},y_t]$, 3 dense layers and additive correction $\Delta x$ & ReLU & 64 \\
GRU & Single-layer GRU(64) over $y_{t-L+1:t}$; concat $[\bar{x},h_t]$ + MLP head & ReLU & 64 \\
LSTM & Single-layer LSTM(64) over $y_{t-L+1:t}$; concat $[\bar{x},h_t]$ + MLP head & ReLU & 64 \\
\bottomrule
\end{tabularx}
\caption{Architecture definitions used throughout the paper (hidden widths and activations).}
\label{tab:arch_summary}
\end{table}


\begin{enumerate}
\item \textbf{Diagnostic Control: Baseline MLP (No-Mean)}
 The Baseline MLP is intentionally smaller (hidden width = 32) and is used as a lightweight diagnostic control; the main MLP/GRU/LSTM models use a 64-dimensional hidden representation for fair comparison of primary models.
This model serves as the \textbf{necessary baseline comparison} and diagnostic control, to introduce the problem and quantify the benefit of prior information \cite{ai_da_fablet}. The Baseline MLP is a minimal, memory-less feedforward network that \textbf{excludes all prior background information} ($\bar{x}$ or $B$). It learns a direct mapping from the most recent observation ($y_t$) to the analysis state: $\hat{x}^a = f_\theta(y_t)$ \cite{ai_da_fablet}. The architecture comprises two linear layers with a hidden dimension of 32 and uses the \textbf{Tanh activation function}. Performance results confirm that relying solely on observation information yields limited generalization, particularly under high noise, confirming that background priors are critical for robust AI-DA \cite{kalman_1960}.
\item \textbf{Multilayer Perceptron (MLP)}
The primary MLP extends the baseline by explicitly concatenating the background mean ($\bar{x}$) and the last observation ($y_t$). It is formulated to predict an additive correction ($\Delta x$) to the background state: $\hat{x}^a = \bar{x} + \Delta x = f_\theta([\bar{x}, y_t])$ \cite{ai_da_fablet}. This architecture, using ReLU hidden layers of width 64, serves as the simplest model capable of approximating the 3D-Var update in a single forward pass and provides a lightweight reference. It was observed to be "surprisingly good" in certain contexts, particularly in the resampling regime \cite{neural_da_bocquet}.
\item \textbf{Gated Recurrent Unit (GRU)}
The GRU introduces temporal context by processing an observation sequence ($L=5$ steps typically) \cite{neural_da_bocquet}. By aggregating temporal information into a hidden state ($h_t$), the GRU tests the core hypothesis that time dependency is necessary for stability in a sequential, chaotic system \cite{neural_da_bocquet, lorenz_1963}. The final hidden state is concatenated with $\bar{x}$ to predict the analysis increment ($\Delta x$). The GRU often achieved the best improvement and generalization, avoiding the overfitting issues sometimes seen in the LSTM \cite{neural_da_bocquet}.
\item \textbf{Long Short-Term Memory (LSTM)}
The LSTM replaces the GRU encoder with an LSTM unit, maintaining both hidden and cell states to capture potentially longer-range dependencies \cite{neural_da_bocquet}. This model is included to test if a more expressive recurrent architecture provides better stability, particularly in nonlinear ($x^2$) observation modes and high-noise settings. While strong, it exhibited signs of "overfitting to the background a bit" compared to the GRU in some tests \cite{neural_da_bocquet}.
\end{enumerate}

\subsubsection{Learning Objective (Self‑supervised 3D‑Var)}
We train without analysis labels by minimizing
\[
\mathcal{L}_{\mathrm{DA}} = \frac{1}{N}\sum_t \Big[
\tfrac12(\hat{x}^a_t-\bar{x}_t)^\top B^{-1}(\hat{x}^a_t-\bar{x}_t)
+\tfrac12(H\hat{x}^a_t-y_t)^\top R^{-1}(H\hat{x}^a_t-y_t)
\Big].
\]
No truth anchor or $a^\star$ labels are used.

\subsubsection{Optimization, Schedules, and Stopping}
All models were trained using the Adam optimizer (initial learning rate $=10^{-3}$) for up to 30 epochs. No learning-rate scheduler or
automatic early stopping was applied during these runs; we trained for the full 30 epochs and archived the final epoch weights for each
configuration. Each saved model bundle contains the model state (\texttt{state\_dict}), the full \texttt{config.yaml}, the RNG seed, and the Git
commit hash used for the run (see Reproducibility subsection). This policy ensures reproducible training snapshots that match the
experiments reported in this work.


\subsubsection{Training Regimes: FixedMean vs Resample}

Two complementary training regimes govern how background states are provided, forming a central component of the project's comparison and leading directly to the two main insights:
\begin{enumerate}
\item \textbf{FixedMean Regime:} This regime uses a \textit{fixed} background mean ($\bar{x}$) and covariance ($B$), computed once from the initial ensemble, remaining static throughout training. This configuration is designed to stress the network by forcing it to correct from a constant, potentially biased prior \cite{lorenz_1963}. It highlights potential limitations of the AI-DA method, as instability and "Attractor Escape" failures were observed, particularly at high noise ($\sigma > 0.5$) \cite{lorenz_1963}.
\item \textbf{Resample Regime:} This strategy dynamically updates the background, where the background ($\bar{x}$) and covariance ($B$) are \textbf{resampled from the ensemble distribution at every epoch or batch} \cite{ai_da_fablet}. This stochastic variability functions as a data-level regularization, preventing the model from overfitting to a single background realization and forcing it to learn a generalized analysis functional. This regime was proven critical for robustness and generalization capacity \cite{ai_da_fablet}.
\end{enumerate}
\subsubsection{Training and Evaluation Protocol}
\medskip
\noindent\textbf{Seeding and averaging policy.}
To obtain statistically robust performance estimates, every $(\text{architecture}\times\text{regime}\times\text{mode}\times\sigma)$
configuration is trained once with a fixed seed for reproducibility.
Evaluation is performed on $K=500$ held-out test trajectories; for each test trajectory
we draw $S=3$ independent observation-noise realizations. Additionally, $K_{\text{unseen}}=3$ fully unseen trajectories
(generated from new initial conditions with seed 4321) are used for generalization testing.
Reported aggregate metrics (e.g., RMSE, Improvement$_{\mathrm{bg}}$, Hausdorff) therefore summarize
$K\times S$ samples in total (i.e., $500\times 3=1{,}500$ samples).
Qualitative figures (representative trajectory reconstructions) use $n_{\text{test}}=5$ test trajectories
to keep plots legible; we mark these figures as ``qualitative'' in their captions.

\noindent
Each model–regime pair (for the learned DA models: MLP/GRU/LSTM) is trained independently for every configuration of observation mode ($x$, $xy$, $x^2$) and noise level $\sigma\in\{0.05,0.1,0.5,1.0\}$. Training used the Adam optimizer (learning rate $=10^{-3}$) for exactly 30 epochs; no LR scheduler or early stopping was applied. Model weights (final epoch \texttt{state\_dict}) were saved for each configuration alongside run metadata. Each run directory contains the configuration, the RNG seed, and the Git commit hash used for that run enabling reproducibility. (If desired, a ReduceLROnPlateau scheduler and early stopping were planned in the methods but not applied in these experiments; they are available as an optional training mode in the codebase.)

\noindent
Evaluation is performed on held-out test trajectories with a sequential window of length $L=5$. 
For each time index $t \ge L{-}1$, the model uses the last $L$ observations to produce an analysis estimate $\hat{x}^a_t$. 
Metrics are computed over the cropped window $t = L{-}1,\dots,T_{\text{eval}}{-}1$:

\begin{itemize}
    \item \textbf{Background error (before):} RMSE$_b$ between the static background mean ($B$ - mean) and the truth.  
    Unless noted, the background is static; in “fast” diagnostic runs, it may advance one Lorenz–63 step.
    
    \item \textbf{Analysis error (after):} RMSE$_a$ between $\hat{x}^a$ and the true state.
    
    \item \textbf{Improvement (\%):} 
    \[
    100 \cdot \frac{\text{RMSE}_b - \text{RMSE}_a}{\text{RMSE}_b + \varepsilon},
    \quad \varepsilon = 10^{-12}.
    \]
\end{itemize}

\noindent
A standardized RMSE is used throughout to ensure comparability across regimes and models: the squared error is averaged over time and state dimensions, then square-rooted. 
To reduce variance, we apply an averaging protocol: for each $(\text{mode},\ \sigma_{\mathrm{obs}})$,
evaluation uses $K = 500$ test trajectories and $S = 3$ independent noise realizations for the
quantitative results reported in the tables and figures. Qualitative figures (representative
trajectory reconstructions) use $K = 5$ to keep plots legible; these figures are labelled
explicitly as qualitative.
Each configuration’s first run saves truth, analysis, and background arrays for qualitative plots. 
Loss curves and metrics are logged in timestamped JSON/CSV directories for aggregation.

\newpage

\section{Experiments and Results}

\subsection{Experimental Setup}
\label{sec:4.1}

This section summarizes the experimental configuration used to evaluate the AI-based data assimilation framework on the Lorenz-63 system. The setup is designed to systematically test how neural network architectures (MLP, GRU, LSTM) learn the analysis functional $\Phi$ under varying observation modes, noise levels, and background sampling regimes. All experiments follow a modular pipeline implemented in \texttt{src/}, with results stored under \texttt{results/}.

\subsubsection{Data Generation and Observation Configurations}

All data are derived from the canonical Lorenz-63 chaotic system \cite{lorenz_1963}, integrated using a fourth-order Runge--Kutta scheme with time step $\Delta t = 0.01$ and standard parameters ($\sigma = 10$, $\rho = 28$, $\beta = 8/3$). A total of 1500 trajectories were generated, each containing 200 time steps. The dataset was split into training (1000 trajectories) and test (500 trajectories) sets to ensure robust evaluation.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{Dataset.png}
\caption{Dataset distribution across train and test splits for all observation modes and noise levels. Each configuration contributes approximately $2.94 \times 10^5$ effective samples, ensuring balanced evaluation.}
\label{fig:dataset_dist}
\end{figure}

Figure~\ref{fig:dataset_dist} confirms that all observation modes and noise levels contribute identical sample counts, preventing bias in model evaluation.

Three observation operators were used to probe varying levels of information completeness and nonlinearity:

\begin{table}[H]
\centering
\caption{Observation operator configurations.}
\label{tab:obs_modes}
\begin{tabularx}{0.9\textwidth}{l X X c}
\toprule
\textbf{Mode} & \textbf{Definition} & \textbf{Type} & \textbf{Dimensionality} \\
\midrule
$x$ & $H(\mathbf{x}) = [x]$ & Partial, linear & 1 \\
$xy$ & $H(\mathbf{x}) = [x, y]$ & Partial, linear & 2 \\
$x^2$ & $H(\mathbf{x}) = [x^2]$ & Non-linear & 1 \\
\bottomrule
\end{tabularx}
\end{table}

At each time step, Gaussian noise $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$ was added with $\sigma \in \{0.05, 0.10, 0.50, 1.00\}$. The $x$ mode provides minimal information, $xy$ offers redundancy and stronger constraints, while $x^2$ introduces nonlinearity that amplifies noise but removes sign information (see Appendix~\ref{sec:app_data} for detailed visualizations).

\subsubsection{Training Regimes and Neural Architectures}

Three training regimes were evaluated to isolate the effects of background conditioning and stochastic regularization:

\begin{itemize}
\item \textbf{Baseline (No-Mean)} -- The network receives only the observation sequence $y_{t-L+1:t}$ without background information ($\bar{x}$, $B$). This serves as a lower-bound reference for purely data-driven learning.

\item \textbf{FixedMean} -- The background mean $\bar{x}$ and covariance $B$ remain constant throughout training. While this regime shows smooth convergence in some settings, it frequently becomes unstable at moderate to high noise ($\sigma \geq 0.5$), exhibiting attractor escape and RMSE explosions (see Section~\ref{sec:4.2}).

\item \textbf{Resample} -- The background $\bar{x}$ and covariance $B$ are resampled at every epoch from the ensemble distribution. This introduces stochastic variability that acts as data-level regularization, significantly improving robustness and generalization \cite{ai_da_fablet}.
\end{itemize}

All models minimize the differentiable 3D-Var objective without analysis labels:
\[
\mathcal{L}_{\mathrm{DA}}=\frac{1}{N}\sum_t \left[\tfrac{1}{2}(\hat{x}^a_t-\bar{x}_t)^\top B^{-1}(\hat{x}^a_t-\bar{x}_t)
+\tfrac{1}{2}(H\hat{x}^a_t-y_t)^\top R^{-1}(H\hat{x}^a_t-y_t)\right].
\]

The true state $x_{\text{true}}$ is used only for offline RMSE evaluation and is never provided during training.

Three neural architectures were compared:

\begin{itemize}
\item \textbf{MLP} -- A three-layer feedforward network that concatenates the background mean $\bar{x}$ with the current observation $y_t$ to predict an additive correction: $\hat{x}^a_t = \bar{x} + f_\theta([\bar{x}, y_t])$.

\item \textbf{GRU} -- A recurrent model processing the observation window $y_{t-L+1:t}$ (typically $L=5$) through a single-layer GRU encoder (hidden size 64), followed by a feedforward correction head.

\item \textbf{LSTM} -- Similar to GRU but uses LSTM cells to maintain both hidden and cell states, allowing longer-range temporal dependencies.
\end{itemize}

All models share a 64-dimensional latent representation and were trained for 30 epochs using the Adam optimizer (learning rate $1 \times 10^{-3}$, batch size 256). Additional architectural and implementation details are provided in Section~\ref{sec:methods}.

\begin{table}[H]
\centering
\caption{Experimental configuration summary.}
\label{tab:setup}
\begin{tabularx}{0.85\textwidth}{l X}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Trajectories (train/test) & 1000 / 500 \\
Time steps per trajectory & 200 \\
$\Delta t$ & 0.01 \\
Observation modes & $x$, $xy$, $x^2$ \\
Noise $\sigma$ & 0.05, 0.10, 0.50, 1.00 \\
Sequence window $L$ & 5 \\
Architectures & MLP, GRU, LSTM \\
Hidden size & 64 \\
Batch size & 256 \\
Epochs & 30 \\
Loss function & 3D-Var objective (Eq. above) \\
\bottomrule
\end{tabularx}
\end{table}

\subsubsection{Evaluation Pipeline}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\linewidth]{pipe_eval.png}
\caption{Training and evaluation pipeline. The workflow encompasses Lorenz-63 simulation, observation corruption, background ensemble generation, neural network training via the 3D-Var objective, and post-assimilation evaluation against held-out test trajectories.}
\label{fig:pipeline}
\end{figure}

Figure~\ref{fig:pipeline} illustrates the complete experimental workflow. Raw trajectories are generated from the Lorenz-63 system, corrupted with observation noise, and paired with ensemble-derived background statistics. Neural networks learn the analysis mapping by minimizing $\mathcal{L}_{\mathrm{DA}}$, and final models are evaluated on unseen test trajectories to assess generalization. Detailed data and results directory structures are documented in Appendix~\ref{sec:app_data}.

\newpage

\subsection{Convergence and Training Dynamics}
\label{sec:4.2}

This section examines the convergence properties of the three training regimes---Baseline, FixedMean, and Resample---focusing on loss trajectories, stability, and the impact of background conditioning on optimization dynamics. While all regimes demonstrate smooth loss reduction during training, their generalization and robustness differ substantially, particularly under high observation noise.

\subsubsection{Comparative Convergence Behavior}

The convergence behavior across regimes reveals critical differences in stability and learning efficiency. The Baseline regime (no background information) shows steady but slow convergence, with validation losses consistently higher than training losses, indicating limited generalization when climatological context is absent. The FixedMean regime exhibits faster initial convergence due to the fixed prior anchor, achieving lower loss values within the first few epochs. However, this apparent stability is deceptive: at moderate to high noise levels ($\sigma \geq 0.5$), FixedMean frequently produces catastrophic failures during evaluation, including attractor escape and RMSE explosions that render the analysis unusable.

In contrast, the Resample regime demonstrates both rapid and robust convergence. By introducing stochastic variability in the background mean and covariance at each epoch, the model learns a more generalizable assimilation functional that remains stable across noise levels. Validation losses track training losses closely, and the narrow variance envelope across runs indicates highly reproducible optimization.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{4_2k_mean_convergence_envelopes.png}
\caption{Mean convergence envelopes across training regimes. The Resample regime (blue) achieves the fastest and most stable convergence, reducing loss to approximately 20\% of the initial value within the first epoch. FixedMean (orange) shows rapid initial descent but becomes unstable at high noise. Baseline (green) converges slowly and retains approximately 90\% of initial error by epoch 30. Shaded regions indicate standard deviation across observation modes and noise levels.}
\label{fig:convergence_envelopes}
\end{figure}

Figure~\ref{fig:convergence_envelopes} quantifies these trends. The Resample regime achieves the sharpest descent, stabilizing near 20\% of initial loss within 5 epochs, while Baseline retains approximately 90\% of its initial error even after 30 epochs. FixedMean appears competitive in terms of loss reduction, but this does not translate to reliable test performance, as discussed below.

\subsubsection{Handling Divergent Runs and Attractor Escape}

A critical challenge in training AI-DA models for chaotic systems is \textit{attractor escape}, where the learned analysis trajectory departs from the chaotic manifold, causing RMSE to grow by orders of magnitude. This failure mode is particularly prevalent in the FixedMean regime at high noise. We classify a run as diverged if the post-assimilation RMSE exceeds $10\times$ the median RMSE for that configuration or if the normalized Hausdorff distance exceeds 10.

In our experiments, observed divergence rates at $\sigma \geq 0.5$ were approximately 70--80\% for FixedMean and only 20--25\% for Resample. These failure rates underscore the fragility of static background priors under uncertainty. When summarizing results, we report mean $\pm$ standard deviation over non-diverged runs and explicitly state divergence rates. Robust statistics (median and IQR) that include all runs are provided in supplementary tables (Appendix~\ref{sec:app_training}).

The Resample regime's superior robustness stems from its stochastic regularization: by continuously exposing the network to diverse background realizations, it learns a correction functional that generalizes beyond the specific training contexts, preventing overfitting to a single climatological prior.

\subsubsection{Implications for Model Selection}

The comparative analysis establishes that:
\begin{enumerate}
\item \textbf{Baseline} provides a useful lower bound but lacks the context needed for stable assimilation in partially observed systems.
\item \textbf{FixedMean} converges quickly in loss but is fundamentally unreliable for operational use at $\sigma > 0.5$ due to high divergence rates.
\item \textbf{Resample} balances fast convergence, low validation error, and robust generalization, making it the recommended regime for chaotic data assimilation.
\end{enumerate}

Detailed loss curves for individual configurations are provided in Appendix~\ref{sec:app_training}, including multi-panel grids showing convergence per observation mode and noise level. Section~\ref{sec:4.3} examines the post-assimilation accuracy and stability metrics in greater depth.

\newpage

\subsection{Resample Regime -- Accuracy and Stability}
\label{sec:4.3}

Having established that the Resample regime provides the most stable training dynamics, this section evaluates its post-assimilation performance in detail. We focus on RMSE distributions, assimilation gain ($\Delta$RMSE), and cross-run stability to quantify how stochastic background resampling translates into robust state estimation.

\subsubsection{Post-Assimilation RMSE Across Architectures}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_3a_resample_rmse_distributions.png}
\caption{Post-assimilation RMSE distributions for MLP, GRU, and LSTM across noise levels in the Resample regime. Box plots show median, interquartile range, and outliers. All models maintain stable accuracy for $\sigma \leq 0.1$ (median RMSE $\approx 4$--5), but dispersion increases at higher noise, particularly for LSTM.}
\label{fig:resample_rmse}
\end{figure}

Figure~\ref{fig:resample_rmse} illustrates RMSE distributions for the three architectures. At low noise ($\sigma \leq 0.1$), all models achieve median RMSE values of approximately 4--5, with narrow interquartile ranges indicating consistent performance across runs. As noise increases, RMSE dispersion widens, particularly for LSTM, whose quartile range nearly doubles at $\sigma = 1.0$. This suggests stronger dependence on initialization and sample variability.

In contrast, the GRU exhibits tighter distributions and fewer outliers, demonstrating more stable assimilation under data perturbation. The MLP retains the lowest mean RMSE for the $x$ mode (Table~\ref{tab:resample_stats}), but its stability declines in the nonlinear $x^2$ regime, where temporal modeling provides a clear advantage.

\begin{table}[H]
\centering
\small
\caption{Cross-model RMSE and $\Delta$RMSE statistics under Resample regime (mean $\pm$ std across runs, excluding diverged cases).}
\label{tab:resample_stats}
\begin{tabularx}{0.95\textwidth}{l l c c c c}
\toprule
\textbf{Mode} & \textbf{Model} & \textbf{RMSE$_a$ Mean} & \textbf{RMSE$_a$ Std} & \textbf{$\Delta$RMSE \%} & \textbf{Std} \\
\midrule
$x$   & GRU  & 7.054 & 0.496 & $-0.079$ & 0.169 \\
      & LSTM & 7.011 & 0.332 & $-0.151$ & 0.207 \\
      & MLP  & 6.966 & 0.267 & $+0.022$ & 0.439 \\
$x^2$ & GRU  & 10.834 & 0.566 & $-0.669$ & 0.552 \\
      & LSTM & 11.060 & 0.660 & $-0.558$ & 0.406 \\
      & MLP  & 13.307 & 1.154 & $-0.923$ & 0.574 \\
$xy$  & GRU  & 7.788 & 1.150 & $-0.660$ & 0.960 \\
      & LSTM & 7.925 & 1.670 & $-0.707$ & 0.888 \\
      & MLP  & 7.952 & 0.665 & $-0.454$ & 0.346 \\
\bottomrule
\end{tabularx}
\end{table}

These results confirm that recurrent architectures (GRU, LSTM) generally outperform the MLP in nonlinear and coupled observation modes, where temporal context aids in resolving ambiguities and filtering noise.

\subsubsection{Assimilation Gain: $\Delta$RMSE Analysis}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_3b_delta_rmse_noise.png}
\caption{Mean $\Delta$RMSE (before assimilation $-$ after assimilation) per model as a function of observation noise $\sigma$. Positive values indicate improvement. At low noise ($\sigma \leq 0.1$), all architectures yield marginal positive gains. As noise increases, improvement decreases and occasionally becomes slightly negative for GRU and LSTM, suggesting mild sensitivity to stochastic perturbations. Overall scale of variation remains small ($< 1\%$), indicating stable assimilation across noise levels.}
\label{fig:delta_rmse_noise}
\end{figure}

Figure~\ref{fig:delta_rmse_noise} plots the mean assimilation gain $\Delta$RMSE = (RMSE$_{\text{before}}$ $-$ RMSE$_{\text{after}}$). At low noise, all three architectures achieve small but consistent improvements. As $\sigma$ increases, gains diminish and occasionally reverse slightly (negative $\Delta$RMSE) for recurrent models, likely due to overfitting or sensitivity to high-variance observations. However, the magnitude of degradation remains negligible ($< 1\%$), demonstrating that the learned assimilation functional remains well-conditioned even under extreme observational uncertainty.

The observation mode strongly influences improvement patterns (see Appendix Figure~\ref{fig:app_delta_modes}). The $xy$ and $x$ modes retain positive $\Delta$RMSE across all noise levels, while $x^2$ shows mild degradation beyond $\sigma = 0.1$, consistent with nonlinear measurement amplification of noise.

\subsubsection{Cross-Run Stability}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{4_3d_stability_vs_noise.png}
\caption{Cross-run stability measured as the standard deviation of $\Delta$RMSE improvement across resampling runs for each architecture. GRU shows the highest variability (peak $\approx 1\%$ at intermediate noise) before stabilizing, while MLP exhibits the most consistent behavior ($\approx 0.3\%$ variance across all $\sigma$). LSTM falls between the two.}
\label{fig:stability_noise}
\end{figure}

Figure~\ref{fig:stability_noise} quantifies run-to-run variability in assimilation gain. The MLP demonstrates the lowest variance ($\approx 0.3\%$) across all noise levels, indicating highly reproducible corrections despite its lack of temporal modeling. The GRU shows transient instability at moderate noise (peak $\approx 1\%$) before declining, while LSTM stabilizes at intermediate variance ($\approx 0.5\%$).

These findings suggest that while recurrent architectures adapt better to temporal patterns, their higher parameter count and sensitivity to initialization introduce run-to-run variability. In contrast, the MLP's simplicity yields consistent (if less flexible) behavior.

\subsubsection{Cross-Regime Correlation Analysis}

To assess whether improvements under Resample generalize across training regimes, we performed correlation analyses between Baseline, FixedMean, and Resample $\Delta$RMSE values. The results (detailed in Appendix Figures~\ref{fig:app_corr_baseline_fixed}, \ref{fig:app_corr_fixed_resample}, and \ref{fig:app_corr_baseline_resample}) reveal:

\begin{itemize}
\item Weak negative correlation between Baseline RMSE and improvement magnitude, suggesting diminishing returns when the initial state is far from the attractor.
\item Weak positive correlation ($\approx 0.25$--$0.35$) between FixedMean and Resample gains, indicating partial transfer of architecture-specific advantages across regimes.
\item Near-flat correlation between Baseline and Resample, implying that stochastic perturbations disrupt deterministic improvement patterns.
\end{itemize}

These findings confirm that while absolute improvement is regime-specific, the relative ranking of model robustness remains moderately preserved, underscoring the structural consistency of the learned assimilation operator.

\subsubsection{Summary}

The Resample regime achieves:
\begin{enumerate}
\item \textbf{Consistent accuracy} -- Median RMSE remains stable across noise levels, with graceful degradation at high $\sigma$.
\item \textbf{Architecture-agnostic robustness} -- All three models (MLP, GRU, LSTM) converge to comparable performance, emphasizing the importance of stochastic regularization over architectural complexity.
\item \textbf{Low cross-run variance} -- Assimilation gains remain bounded and reproducible, preventing catastrophic failures observed in FixedMean.
\end{enumerate}

These results establish Resample as the recommended training regime for operational AI-DA in chaotic systems. Section~\ref{sec:4.4} examines how observation mode structure interacts with these findings.

\newpage

\subsection{Observation-Mode Sensitivity}
\label{sec:4.4}

This section investigates how the structure of the observation operator---linear partial ($x$), bilinear coupled ($xy$), or nonlinear ($x^2$)---affects post-assimilation accuracy, improvement gain, and stability. By comparing performance across observation modes, we identify the relative importance of information richness versus nonlinearity in constraining the learned analysis functional.

\subsubsection{Post-Assimilation Accuracy by Observation Mode}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_4a_post_assimilation_rmse.png}
\caption{Post-assimilation RMSE distributions for all observation modes as a function of noise level $\sigma$. The $xy$ mode consistently achieves the lowest and most stable errors (median RMSE $\approx 4$), followed by $x$ ($\approx 4$--5), while $x^2$ exhibits markedly higher dispersion and error magnitudes ($\approx 5$--10) due to nonlinear noise amplification.}
\label{fig:obs_mode_rmse}
\end{figure}

Figure~\ref{fig:obs_mode_rmse} reveals a clear hierarchy in observation effectiveness:
\[
xy < x < x^2
\]

The $xy$ mode provides dual observations that strongly constrain the state, yielding median RMSE near 4 with narrow quartiles even at $\sigma = 1.0$. The $x$ mode, observing only a single component, achieves slightly higher but still stable errors ($\approx 4$--5). In contrast, the $x^2$ mode exhibits significantly larger dispersion and RMSE values rising from $\approx 5$ to $\approx 10$ as noise increases. This degradation reflects the multiplicative error amplification inherent in nonlinear observation mappings: additive noise translates into multiplicative state uncertainty.

The pattern confirms that \textit{information coupling outweighs observational nonlinearity} in enabling accurate assimilation. Coupled linear observations ($xy$) provide redundancy that resolves sign ambiguities and filters noise more effectively than increased measurement complexity.

\subsubsection{Assimilation Improvement Across Modes}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_4b_delta_rmse_improvement.png}
\caption{$\Delta$RMSE improvement (before $-$ after assimilation) across observation modes as a function of noise $\sigma$. At low noise ($\sigma \leq 0.1$), all modes achieve substantial gains ($\Delta \approx 0.4$--0.5). As noise increases, $x$ and $xy$ retain nearly constant positive gains, while $x^2$ drops below zero for $\sigma \geq 0.1$, indicating slight degradation due to nonlinear error amplification.}
\label{fig:obs_mode_delta}
\end{figure}

Figure~\ref{fig:obs_mode_delta} quantifies the assimilation gain $\Delta$RMSE for each observation mode. At low noise, all three modes achieve substantial improvements ($\Delta \approx 0.4$--0.5). However, as $\sigma$ increases, the trends diverge sharply:

\begin{itemize}
\item $x$ and $xy$ maintain nearly constant positive gains across all noise levels, demonstrating robustness to observational uncertainty.
\item $x^2$ drops below zero for $\sigma \geq 0.1$, indicating that nonlinear observation mappings not only fail to improve the analysis but can actively degrade it under moderate noise.
\end{itemize}

This decline arises because the nonlinear transformation amplifies observation noise through cross-terms in the error covariance, making the inverse problem increasingly ill-posed. The relative flatness of $x$ and $xy$ curves demonstrates that linear and coupled observations preserve corrective efficacy even under substantial perturbations.

\subsubsection{Cross-Architecture Performance by Mode}

Different architectures exhibit mode-specific advantages (see Appendix Figure~\ref{fig:app_cross_arch_mode}). For the $xy$ mode, all three architectures converge to comparable performance ($\approx 4.0$ RMSE), indicating that temporal coupling dominates architectural differences. Under $x$, LSTM achieves the lowest mean RMSE ($\approx 3.7$), likely due to its gating structure filtering local fluctuations in univariate sequences. For $x^2$, the MLP shows a modest advantage ($\approx 6.7$ vs. $7$--8 for recurrent models), suggesting that static nonlinear mappings benefit from feedforward representations.

These results highlight that \textit{architecture--mode alignment} matters more than overall capacity: recurrent networks excel when observations convey temporal context, while feedforward models adapt better to instantaneous nonlinear transformations.

\subsubsection{Stability and Variance Across Modes}

Run-to-run variability in assimilation gain, expressed as $\text{Std}(\Delta \text{RMSE})$, is lowest for $xy$ ($< 0.1$) across all noise levels, signifying stable convergence and reproducibility (see Appendix Figure~\ref{fig:app_variance_modes}). The $x$ mode shows slightly higher variance at low noise but decays with $\sigma$, indicating that noise averaging mitigates sensitivity. Conversely, $x^2$ exhibits pronounced variance (0.3--0.35), peaking near $\sigma = 0.5$, consistent with stochastic amplification through the nonlinear observation function.

This implies that the instability in nonlinear modes is primarily \textit{data-driven}, not model-driven: the observation operator itself injects variability that the assimilation framework cannot fully suppress.

\subsubsection{Summary}

The observation-mode analysis yields three main conclusions:
\begin{enumerate}
\item \textbf{Information richness vs. nonlinearity} -- Coupled observations ($xy$) enable the most accurate and stable assimilation. Nonlinear observations ($x^2$) raise RMSE and variance due to multiplicative noise amplification.
\item \textbf{Architecture--mode interaction} -- Recurrent models (GRU, LSTM) benefit from temporally structured inputs ($xy$), while feedforward networks (MLP) better handle static nonlinearities ($x^2$).
\item \textbf{Robustness under uncertainty} -- Even as $\sigma$ increases, $\Delta$RMSE remains bounded for linear modes, indicating a well-conditioned assimilation operator that generalizes across noise regimes.
\end{enumerate}

These findings inform the choice of observation strategy for operational AI-DA systems. Section~\ref{sec:4.5} examines how these effects propagate temporally through sequential state updates and attractor geometry preservation.

\newpage

\subsection{Temporal Assimilation and Attractor Geometry}
\label{sec:4.5}

While previous sections focused on aggregated RMSE metrics, effective data assimilation in chaotic systems requires temporal coherence and fidelity to the underlying attractor manifold. This section shifts analysis from instantaneous accuracy to the temporal evolution of errors, trajectory reconstruction quality, and the ability of learned models to preserve the geometric structure of the Lorenz-63 attractor during sequential assimilation steps.

\subsubsection{Temporal Error Evolution}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_5b_error_evolution_profiles.png}
\caption{Temporal evolution of Euclidean error over the assimilation window for FixedMean and Resample regimes at different noise levels. Resample (solid lines) demonstrates fast convergence ($\tau \approx 50$ time steps) to a low, stable error plateau across all noise levels. In contrast, FixedMean (dashed lines), particularly at $\sigma = 0.5$, exhibits large oscillations and sustained high error, underscoring its instability without dynamic background updates.}
\label{fig:error_evolution}
\end{figure}

Figure~\ref{fig:error_evolution} reveals stark differences in temporal error dynamics between regimes. The Resample regime achieves rapid error reduction within approximately 50 time steps (roughly one Lyapunov time for Lorenz-63), stabilizing at a low, steady-state error level across all noise settings. The error trajectory remains smooth and bounded, indicating that the learned correction functional successfully tracks the chaotic manifold.

In contrast, FixedMean exhibits erratic behavior at moderate to high noise. Error oscillations grow in amplitude, and the trajectory fails to stabilize, often diverging entirely from the true attractor. This temporal instability reflects the fundamental limitation of static background priors: without stochastic updates, the model cannot adapt to the diverse realizations of chaotic dynamics encountered during assimilation.

\subsubsection{Generalization to Unseen Trajectories}

A critical requirement for successful AI-DA in chaotic systems is \textbf{generalization}---the capacity to accurately reconstruct trajectories entirely absent from the training dataset. If the network merely memorizes training dynamics, performance on unseen trajectories will collapse.

To assess this, we evaluated all models on held-out test sets comprising trajectories never seen during training. The key observations confirm the necessity of dynamic background updates:

\begin{enumerate}
\item \textbf{Resample Fidelity} -- Models trained using Resample consistently maintain the correct Lorenz-63 attractor structure when reconstructing unseen trajectories. Normalized Hausdorff distances remain low ($\approx 0.316$--$0.323$ for $xy$ mode across all $\sigma$), compared to much larger values for Baseline ($\approx 1.085$) and FixedMean ($\approx 1.156$). Lyapunov exponents remain negative and close to the reference range, confirming coherent chaotic dynamics.

\item \textbf{FixedMean Failures} -- FixedMean frequently exhibits poor generalization on unseen trajectories, leading to \textit{attractor escape} where the analysis drifts toward fixed points or breaks chaotic dynamics entirely. One recorded failure showed RMSE escalating from 5.2 (before) to 156.8 (after assimilation) for the FixedMean-$x$-$\sigma = 0.5$ case, explicitly identifying the fixed background mean as insufficient for high noise conditions.

\item \textbf{Architectural Impact} -- Recurrent architectures (GRU/LSTM) show improved stability and generalization compared to memory-less MLPs, especially in noisy regimes where temporal context helps constrain long-term chaotic evolution. The MLP-$xy$-$\sigma = 1.0$ case exhibited \textit{phase drift}, where the model lacked temporal context to properly constrain the trajectory.
\end{enumerate}

Visual diagnostics of representative unseen trajectories (Appendix Figure~\ref{fig:app_unseen_traj}) confirm these patterns, showing that Resample preserves attractor geometry while FixedMean produces divergent, physically inconsistent paths.

\begin{table}[H]
\centering
\small
\caption{Resample regime, $xy$ mode: accuracy and geometry metrics versus noise (means across runs). Lower is better. Context: Baseline mean Hausdorff $\approx 1.085$; FixedMean mean Hausdorff $\approx 1.156$ (averaged across $\sigma$).}
\label{tab:xy_resample}
\begin{tabular}{c ccc ccc}
\toprule
& \multicolumn{3}{c}{RMSE $\downarrow$} & \multicolumn{3}{c}{Normalized Hausdorff $\downarrow$} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
$\sigma$ & MLP & GRU & LSTM & MLP & GRU & LSTM \\
\midrule
0.05 & 4.073 & 3.448 & 3.630 & 0.320 & 0.316 & 0.320 \\
0.10 & 3.925 & 3.883 & 4.018 & 0.323 & 0.320 & 0.322 \\
0.50 & 3.958 & 3.586 & 3.994 & 0.319 & 0.321 & 0.320 \\
1.00 & 4.000 & 4.552 & 4.078 & 0.321 & 0.321 & 0.322 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:xy_resample} quantifies these observations for the $xy$ mode, showing that Resample maintains both low RMSE and excellent geometric fidelity (low Hausdorff distance) across all noise levels.

\subsubsection{Attractor Geometry Preservation}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_5d_attractor_geometry.png}
\caption{Attractor geometry preservation via phase space projections ($X$--$Y$, $Y$--$Z$ planes) under different noise levels. Resample regime (left panels) maintains tight alignment with the true Lorenz-63 double-wing structure even at $\sigma = 0.5$ (median normalized Hausdorff $\approx 0.07$), whereas FixedMean (right panels) exhibits severe distortion and drift (median Hausdorff $\approx 1.20$), often collapsing toward fixed points or breaking chaotic dynamics entirely.}
\label{fig:attractor_geometry}
\end{figure}

Figure~\ref{fig:attractor_geometry} visualizes the preservation of attractor geometry through phase space projections. The Resample regime successfully maintains the characteristic double-wing structure of the Lorenz-63 attractor across noise levels, with trajectories tightly constrained to the true manifold. Even at moderate noise ($\sigma = 0.1$), the learned analysis remains confined to the correct geometric structure in the $X$--$Y$ and $Y$--$Z$ planes.

In contrast, FixedMean produces severely distorted phase portraits, with trajectories drifting away from the chaotic manifold toward fixed points or exhibiting fragmented, non-chaotic behavior. This geometric degradation explains the high RMSE and attractor escape observed in Section~\ref{sec:4.2}: the model learns an incorrect dynamical structure that fails to respect the underlying physics.

Quantitatively, Resample achieves median normalized Hausdorff distance $\tilde{H} \approx 0.07$ (IQR 0.05--0.11), while FixedMean yields $\tilde{H} \approx 1.20$ (IQR 0.80--1.90). This order-of-magnitude difference confirms that stochastic background resampling is essential for geometric fidelity in chaotic assimilation.

\subsubsection{Component-Wise Correction Patterns}

Analysis of component-wise residuals ($\Delta x = \hat{x}^a - \bar{x}$) provides insight into how architectures handle the three Lorenz-63 state variables ($X$, $Y$, $Z$). The attractor's anisotropy leads to distinct correction requirements:

\begin{itemize}
\item \textbf{$Z$ Component} -- All architectures exhibit smooth, monotonic correction of $Z$ residuals, reflecting the slow-decaying nature of initial $Z$ bias in the background. This is consistent with the Lorenz-63 dynamics where $Z$ evolves according to $\frac{dz}{dt} = xy - \beta z$.

\item \textbf{$X$ and $Y$ Components} -- Instantaneous corrections for $X$ and $Y$ show rapid fluctuations corresponding to their chaotic behavior. Recurrent networks (GRU/LSTM) produce lower-variance residuals ($\sigma_{\text{res}} \approx 0.1$--0.2) compared to the MLP, demonstrating that temporal awareness smooths corrections and prevents high-frequency artifacts.
\end{itemize}

Detailed component-wise residual plots are provided in Appendix Figure~\ref{fig:app_componentwise}.

\subsubsection{Summary}

The temporal and geometric analysis establishes that:
\begin{enumerate}
\item \textbf{Resample enables stable temporal evolution} -- Error converges rapidly and remains bounded, indicating successful trajectory tracking on the chaotic manifold.
\item \textbf{Generalization to unseen data is critical} -- Resample models maintain low RMSE and correct attractor geometry on held-out test trajectories, while FixedMean frequently fails catastrophically.
\item \textbf{Geometric fidelity is essential} -- Preservation of the Lorenz-63 double-wing structure (quantified via Hausdorff distance) is a necessary condition for physically consistent assimilation.
\end{enumerate}

These findings reinforce that stochastic background resampling is not merely a training trick but a fundamental requirement for learning robust, generalizable analysis functionals in chaotic systems. Section~\ref{sec:4.6} examines ablation studies to identify optimal design choices for operational implementation.

\newpage

\subsection{Ablation Studies and Practical Recommendations}
\label{sec:4.6}

To distill actionable insights for implementing AI-DA in operational settings, this section presents targeted ablation studies investigating the sensitivity of the system to key design parameters: background sampling strategy, temporal context length, and background covariance estimation accuracy. These studies directly inform practical recommendations for deploying learned assimilation functionals.

\subsubsection{Background Sampling Strategy and Noise Stability}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{4_6a_background_sampling_stability.png}
\caption{Impact of background sampling strategy on RMSE stability across noise levels. Resample (solid lines) demonstrates graceful degradation across all $\sigma$ values, with RMSE increasing smoothly from $\approx 4$ to $\approx 7$. FixedMean (dashed lines) exhibits catastrophic instability beyond $\sigma > 0.5$, with RMSE spiking dramatically and divergence rates reaching 70--80\% at high noise. The stochastic regularization provided by resampling prevents attractor escape and ensures robust performance even under extreme observational uncertainty.}
\label{fig:background_stability}
\end{figure}

Figure~\ref{fig:background_stability} quantifies the fundamental divergence in stability between FixedMean and Resample regimes. The Resample strategy introduces stochastic variability in the background mean $\bar{x}$ and covariance $B$, functioning as data-level regularization. This results in remarkably stable RMSE across all noise levels, degrading gracefully even up to $\sigma = 1.0$.

In contrast, the FixedMean regime, which relies on a constant prior, becomes fundamentally unstable beyond moderate noise ($\sigma > 0.5$). At high noise levels, FixedMean exhibits the \textit{attractor escape} failure mode, with RMSE values spiking from $\approx 5$ to over 150 in extreme cases. The failure rate (cases showing negative improvement or divergence) reaches 70--80\% for FixedMean compared to only 20--25\% for Resample at $\sigma \geq 0.5$.

\textbf{Recommendation:} Avoid FixedMean at $\sigma > 0.5$. The Resample (stochastic) regime is strongly recommended as it is robust to observation uncertainty and promotes reproducible assimilation trajectories. For operational systems, background statistics should be continuously updated from ensemble forecasts to prevent overfitting to static priors.

\subsubsection{Temporal Context (Sequence Length)}

Recurrent architectures leverage observation windows $y_{t-L+1:t}$ to infer temporal dependencies. An ablation study on sequence length reveals the optimal context (Appendix Figure~\ref{fig:app_seq_length}):

\begin{itemize}
\item Performance improves sharply up to $L = 10$ time steps, approximately capturing one Lyapunov time (the characteristic time scale for error doubling in Lorenz-63) \cite{lorenz_1963}.
\item Beyond $L = 10$ or $L = 15$, RMSE plateaus or slightly increases, indicating diminishing returns as older information becomes irrelevant due to chaos or introduces noise.
\item LSTM consistently achieves lower minimum RMSE than GRU across tested sequence lengths, although both RNNs significantly outperform the memory-less MLP.
\end{itemize}

\textbf{Recommendation:} Use sequence length $L = 10$--15 time steps for recurrent models. Longer sequences yield diminishing returns and increased computational cost without improving accuracy.

\subsubsection{Background Covariance Sensitivity}

The system's sensitivity to misestimation of the background error covariance $B$ was tested by scaling the true $B$ matrix by a factor $\lambda$ (Appendix Figure~\ref{fig:app_B_scaling}). This intentionally misestimated background uncertainty while keeping observation covariance $R$ constant.

The results validate the fundamental principle of variational data assimilation:

\begin{enumerate}
\item \textbf{Optimal performance at true $B$} -- Minimum post-assimilation RMSE is consistently achieved when $\lambda = 1.0$, confirming that the neural network successfully learns to approximate the minimizer of the 3D-Var cost function, which depends explicitly on $B^{-1}$.

\item \textbf{U-shaped error curve} -- Deviation from $\lambda = 1.0$ produces a characteristic U-shaped curve. When $B$ is underestimated ($\lambda < 1.0$), the system over-trusts the background, neglecting observational information and increasing analysis error. When $B$ is overestimated ($\lambda > 1.0$), noisy observations disproportionately influence the analysis, also elevating RMSE.

\item \textbf{Regime-specific robustness} -- Resample demonstrates substantially higher tolerance to $B$ misestimation than FixedMean (Appendix Figure~\ref{fig:app_B_regime}). Resample's U-shaped curve has a shallower, broader basin around $\lambda = 1.0$, while FixedMean's RMSE spikes dramatically with even small deviations.
\end{enumerate}

\textbf{Recommendation:} Validate $B$ accuracy to within $\pm 10\%$ of the true covariance scale. For operational systems, use ensemble-derived covariance estimates and update them regularly. The Resample regime's robustness to $B$ misestimation makes it preferable when covariance estimates are uncertain.

\subsubsection{Observation Sparsity}

The Resample-GRU model exhibits graceful degradation as observation sparsity increases (detailed in Appendix):

\begin{itemize}
\item RMSE increases modestly up to 25\% missing observations (only 29\% RMSE increase relative to full observations).
\item Beyond 50\% sparsity, performance degrades exponentially, confirming the necessity of reasonably dense observation networks.
\item FixedMean diverges rapidly when sparsity reaches 25\%.
\end{itemize}

\textbf{Recommendation:} Maintain observation coverage above 75\% for reliable performance. If higher sparsity is unavoidable, use ensemble averaging or data augmentation techniques.

\subsubsection{Practical Summary}

\renewcommand{\arraystretch}{1.4}

\begin{table}[H]
\centering
\caption{Practical recommendations for AI-Based Data Assimilation (AI-DA) in chaotic systems.}
\label{tab:recommendations}
\begin{tabularx}{\textwidth}{|l|l|X|}
\hline
\textbf{Parameter} & \textbf{Recommended Value} & \textbf{Rationale} \\
\hline
Background Regime & Resample (stochastic) & Robust to observation uncertainty; prevents attractor drift and divergence. \\
\hline
Observation Mode & $xy$ (dual observations) & Provides redundancy and resolves sign ambiguity; outperforms nonlinear modes. \\
\hline
Architecture & GRU / LSTM & Recurrent models exploit temporal context; GRU shows slightly better generalization in Resample. \\
\hline
Sequence Length ($L$) & 10--15 time steps & Captures approximately one Lyapunov time; longer sequences yield diminishing returns. \\
\hline
Noise Tolerance ($\sigma$) & $\sigma \leq 0.1$ & Reliable performance up to moderate noise; higher noise requires ensemble averaging. \\
\hline
$B$ Accuracy & $\pm 10\%$ of true scale & Critical for variational minimization; Resample tolerates larger errors than FixedMean. \\
\hline
Observation Coverage & $\geq 75\%$ & Below this threshold, performance degrades exponentially. \\
\hline
\end{tabularx}
\end{table}

Table~\ref{tab:recommendations} summarizes the key operational guidelines derived from the ablation studies. These recommendations provide a foundation for implementing AI-DA in real-world chaotic systems, balancing accuracy, robustness, and computational efficiency.

\newpage

\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}

\subsection*{A.1 Data and Pipeline Extras}
\addcontentsline{toc}{subsection}{A.1 Data and Pipeline Extras}
\label{sec:app_data}

This appendix section provides detailed visualizations and data structures referenced in Section~\ref{sec:4.1}.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{obsoperators.png}
\caption{(Appendix) Observation operators and sample noisy sequences for the three observation modes. The nonlinear $x^2$ mode amplifies dynamical magnitudes but removes sign information, illustrating a more ill-posed mapping problem.}
\label{fig:app_obs_operators}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{Backgroundstats.png}
\caption{(Appendix) Background statistics: covariance matrix $B$, its eigenvalue spectrum, and the $2\sigma$ ensemble contour in $X$--$Z$ space. These statistics quantify the uncertainty anisotropy of the Lorenz attractor.}
\label{fig:app_background_stats}
\end{figure}

\noindent
The data directory structure used in all experiments:

\begin{minted}[fontsize=\footnotesize, bgcolor=gray!5, frame=single]{text}
src/
├── data/
│   ├── generation.py
│   ├── dataset.py
│   ├── raw/
│   │   ├── train_traj.npy
│   │   ├── test_traj.npy
│   │   ├── B.npy
│   ├── obs/
│   │   ├── obs_x_n0.05.npy
│   │   ├── obs_xy_n*.npy
│   ├── splits/
│   │   ├── train_indices.npy
│   │   ├── val_indices.npy
│   │   ├── test_indices.npy
\end{minted}

\noindent
The results directory structure:

\begin{minted}[fontsize=\footnotesize, bgcolor=gray!5, frame=single]{text}
results/
├── baseline/
│   ├── baseline_no_mean_20251025_212806/
│   │   ├── figures/   # Loss curves, reconstructions, diagnostics
│   │   └── metrics/   # RMSE tables, JSON histories
├── fixedmean/
│   ├── run_20251008_133752/
│   │   ├── figures/
│   │   ├── metrics/
│   │   └── models/    # Saved .pth model weights
├── resample/
│   ├── run_20251008_134240/
│   │   ├── figures/
│   │   ├── metrics/
│   │   └── models/
\end{minted}

\newpage

\subsection*{A.2 Training Dynamics}
\addcontentsline{toc}{subsection}{A.2 Training Dynamics}
\label{sec:app_training}

This section contains detailed loss curves and convergence diagnostics referenced in Section~\ref{sec:4.2}.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{4_2a_baseline_loss.png}
\caption{(Appendix) Baseline training and validation loss across all observation modes and noise levels. Multi-panel grid shows smooth, monotonic convergence with minor variance across runs.}
\label{fig:app_baseline_loss}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{4_2b_mean_convergence.png}
\caption{(Appendix) Baseline mean convergence averaged across all settings. The relatively small variance band indicates consistent learning behavior across random seeds.}
\label{fig:app_baseline_mean}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{4_2c_baseline_rmse.png}
\caption{(Appendix) Baseline RMSE vs. noise level $\sigma$. Performance degrades gradually with higher noise, confirming that model capacity is constrained by observation quality.}
\label{fig:app_baseline_rmse}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\linewidth]{fixedmean_loss_grid_pub.png}
\caption{(Appendix) FixedMean training and validation loss grid. Despite smooth loss traces in some settings, FixedMean frequently becomes unstable at moderate/high noise ($\sigma \geq 0.5$), with post-assimilation RMSE explosions and attractor escape on unseen trajectories.}
\label{fig:app_fixedmean_loss}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{4_2e_mean_convergence_fixedmean.png}
\caption{(Appendix) FixedMean mean convergence across all settings. The narrow variance envelope indicates reproducible convergence during training, though this does not translate to stable test performance at high noise.}
\label{fig:app_fixedmean_mean}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{fixedmean_rmse_multipanel_pub.png}
\caption{(Appendix) FixedMean RMSE vs. noise level per architecture. All three models show graceful degradation during training, but GRU and LSTM maintain smoother, more consistent responses than MLP.}
\label{fig:app_fixedmean_rmse}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{fixedmean_rmse_before_after_multipanel_pub.png}
\caption{(Appendix) FixedMean RMSE before and after assimilation per architecture. Dashed lines denote pre-assimilation errors, solid lines show post-assimilation results. Recurrent architectures (GRU, LSTM) better internalize assimilation correction dynamics.}
\label{fig:app_fixedmean_before_after}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{fixedmean_delta_rmse_pub.png}
\caption{(Appendix) FixedMean $\Delta$RMSE (before $-$ after assimilation). Positive bars indicate improvement. All modes show consistent RMSE gains, with strongest improvements in nonlinear $x^2$ and mixed $xy$ settings.}
\label{fig:app_fixedmean_delta}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_2i_relative_rmse_fixedmean_vs_baseline.png}
\caption{(Appendix) Relative post-assimilation RMSE (FixedMean vs. Baseline). FixedMean achieves 40--60\% improvement over Baseline in controlled conditions, with largest gains for single-channel observations.}
\label{fig:app_relative_rmse}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{4_2j_delta_rmse_improvement.png}
\caption{(Appendix) $\Delta$RMSE improvement comparison between regimes. FixedMean achieves larger effective reduction than Baseline, especially at higher noise scenarios ($\sigma = 0.5$--1.0).}
\label{fig:app_delta_improvement}
\end{figure}

\newpage

\subsection*{A.3 Extended Resample Metrics}
\addcontentsline{toc}{subsection}{A.3 Extended Resample Metrics}
\label{sec:app_resample}

Additional diagnostics and correlation analyses for the Resample regime, referenced in Section~\ref{sec:4.3}.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{4_3b_diverging_bar_modes.png}
\caption{(Appendix) $\Delta$RMSE improvement across noise and observation modes in the Resample regime. Improvement patterns depend more strongly on observation mode than on architecture.}
\label{fig:app_delta_modes}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_3e_correlation_baseline_fixedmean.png}
\caption{(Appendix) Correlation between Baseline RMSE and improvement in FixedMean regime. Weak negative correlation indicates that higher baseline errors lead to plateauing or declining assimilation gains.}
\label{fig:app_corr_baseline_fixed}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_3f_correlation_fixedmean_resample.png}
\caption{(Appendix) Correlation between FixedMean and Resample improvement. Weak positive trend ($\approx 0.25$--0.35) indicates partial transfer consistency: architectures that adapt efficiently under controlled conditions tend to retain relative ranking with stochastic resampling.}
\label{fig:app_corr_fixed_resample}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_3g_correlation_baseline_resample.png}
\caption{(Appendix) Correlation between Baseline and Resample improvement. Near-flat slope reveals low cross-regime alignment---models showing stronger gains in deterministic conditions do not necessarily maintain them under random perturbations.}
\label{fig:app_corr_baseline_resample}
\end{figure}

\newpage

\subsection*{A.4 Observation-Mode Diagnostics}
\addcontentsline{toc}{subsection}{A.4 Observation-Mode Diagnostics}
\label{sec:app_obs_mode}

Extended observation mode sensitivity analysis referenced in Section~\ref{sec:4.4}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_4c_noise_scaling_rmse_ratio.png}
\caption{(Appendix) Noise scaling behavior: degradation ratio RMSE$_a$/RMSE$_b$ on logarithmic axes. The $xy$ mode remains nearly flat ($\approx 0.9 \to 0.95$), confirming linear scaling and high resilience. The $x^2$ mode rises sharply from $\approx 0.93$ to $\approx 1.02$ between $\sigma = 0.05$ and $0.1$, then plateaus---evidence of near-quadratic sensitivity in the low-noise regime.}
\label{fig:app_noise_scaling}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_4d_cross_architecture_mode_dependence.png}
\caption{(Appendix) Cross-architecture dependence on observation mode. For $xy$, all three architectures converge to comparable performance ($\approx 4.0$), demonstrating that temporal coupling dominates. Under $x$, LSTM achieves lowest RMSE ($\approx 3.7$). For $x^2$, MLP shows modest advantage ($\approx 6.7$ vs. 7--8 for recurrent models).}
\label{fig:app_cross_arch_mode}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{4_4e_variance_across_modes.png}
\caption{(Appendix) Standard deviation of $\Delta$RMSE across stochastic resampling for different observation modes. Variance is lowest for $xy$ ($< 0.1$) across all $\sigma$, signifying stable convergence. The $x^2$ mode exhibits pronounced variance (0.3--0.35), peaking near $\sigma = 0.5$.}
\label{fig:app_variance_modes}
\end{figure}

\newpage

\subsection*{A.5 Trajectory and Residual Reconstructions}
\addcontentsline{toc}{subsection}{A.5 Trajectory and Residual Reconstructions}
\label{sec:app_trajectory}

Detailed trajectory reconstruction and component-wise residual analysis referenced in Section~\ref{sec:4.5}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_5a_trajectory_fidelity_comparison.png}
\caption{(Appendix) Representative trajectory reconstructions under different observation modes showing fidelity to the true Lorenz-63 attractor. Resample regime maintains correct structure, while FixedMean exhibits drift and distortion.}
\label{fig:app_traj_fidelity}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_5c_unseen_trajectory_diagnostics.png}
\caption{(Appendix) Visualization of unseen trajectory reconstructions confirming generalization capacity of Resample vs. FixedMean regimes. Resample preserves chaotic dynamics on held-out test trajectories, while FixedMean frequently exhibits attractor escape.}
\label{fig:app_unseen_traj}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_5c_componentwise_residuals.png}
\caption{(Appendix) Component-wise residual patterns ($\Delta x = \hat{x}^a - \bar{x}$) for $X$, $Y$, and $Z$ components across architectures. Recurrent networks (GRU/LSTM) produce lower-variance $X$ and $Y$ residuals ($\sigma_{\text{res}} \approx 0.1$--0.2) compared to MLP, demonstrating that temporal awareness smooths corrections.}
\label{fig:app_componentwise}
\end{figure}

\newpage

\subsection*{A.6 Ablation Studies}
\addcontentsline{toc}{subsection}{A.6 Ablation Studies}
\label{sec:app_ablations}

Extended ablation study results referenced in Section~\ref{sec:4.6}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_6_2_sequence_length_ablation.png}
\caption{(Appendix) Effect of sequence length ($L$) on RMSE for GRU and LSTM architectures. Performance improves sharply up to $L = 10$ time steps (approximately one Lyapunov time), then plateaus or slightly increases. LSTM achieves lower minimum RMSE than GRU across tested lengths.}
\label{fig:app_seq_length}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_6_4_B_scaling_sensitivity.png}
\caption{(Appendix) Effect of background covariance ($B$) scaling factor ($\lambda$) on post-assimilation RMSE. Optimal performance is achieved when $\lambda = 1.0$ (true $B$), confirming that the network learns the underlying variational minimization problem. Deviation produces a characteristic U-shaped error curve.}
\label{fig:app_B_scaling}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_6_5a_B_sensitivity_regimes.png}
\caption{(Appendix) Regime-specific robustness to $B$ misestimation. Resample (solid) demonstrates substantially higher robustness with a shallower, lower basin around $\lambda = 1.0$. FixedMean (dashed) shows acute sensitivity with RMSE spiking dramatically as $B$ scaling deviates from true value.}
\label{fig:app_B_regime}
\end{figure}

\newpage

\section{Conclusion and Outlook}
\vspace{-0.3cm}
This study investigated the use of neural network architectures for data assimilation in the Lorenz–63 system, focusing on how temporal modeling and ensemble-based strategies support the reconstruction of chaotic trajectories. Experiments with feed-forward and recurrent architectures under fixed and resampled background conditions showed that models capturing temporal dependencies, such as Gated Recurrent Units (GRU) and Long Short-Term Memory (LSTM) networks, consistently outperformed the simpler Multi-Layer Perceptron (MLP). The GRU exhibited particularly stable and generalizable learning, indicating that its gating mechanisms effectively reduce overfitting and improve adaptability with limited data. Resampling the ensemble mean during training enhanced reconstruction quality by better representing evolving background uncertainty and the system’s stochasticity, whereas the fixed-mean formulation often produced divergent or flattened trajectories. While a hypothetical training-time penalty toward the true state (a ``truth-anchor'') could improve stability in contrived diagnostics, such an approach is not realistic for operational data assimilation as the true state is unavailable. No truth-anchor was used in the experiments reported here; all training minimized the self-supervised 3D-Var objective without access to $x_{\mathrm{true}}$.
This work emphasizes the need for rigorous experimental design that clearly separates training, validation, and testing while accounting for stochastic variability. Because single-run evaluations can misrepresent model performance, future studies should average results over multiple randomized trajectories to obtain statistically meaningful error estimates and robust performance metrics. Further exploration under varying observation noise levels has shown that increasing noise impacts architectures differently; thus, a sensitivity analysis across several amplitudes (\(\sigma = 0.05, 0.1, 0.5, 1.0\)) will help identify thresholds at which assimilation accuracy declines. Extending the framework to higher-dimensional systems, such as Lorenz–96, would test its scalability and applicability to complex geophysical settings, while incorporating varied observation operators and levels of partial observability would more closely emulate realistic atmospheric and oceanic assimilation conditions.Looking ahead, integrating data-driven models with traditional assimilation techniques presents a promising research direction. Embedding recurrent neural networks within hybrid frameworks such as the Ensemble Kalman Filter (EnKF) or four-dimensional variational assimilation (4D-Var) could merge physical consistency with adaptive learning, improving both accuracy and interpretability. Generative extensions like Variational Autoencoders or generative LSTMs further enable probabilistic trajectory estimation and explicit uncertainty quantification alongside deterministic prediction. Overall, this study provides methodological and conceptual insights at the intersection of machine learning and data assimilation, showing that the complementary strengths of temporal representation and stochastic resampling form a foundation for future hybrid and probabilistic frameworks capable of capturing the balance between determinism and chaos in nonlinear dynamical systems.

\newpage
\bibliographystyle{plainnat}
\begin{thebibliography}{99}
\bibitem{lorenz_1963} Lorenz, E. N. (1963). Deterministic nonperiodic flow. \textit{Journal of the Atmospheric Sciences, 20}(2), 130-141.
\bibitem{kalman_1960} Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. \textit{Journal of Basic Engineering, 82}(1), 35–45.
\bibitem{ai_da_fablet} Fablet, R., Ouala, S., \& Herzet, C. (2021). Learning variational data assimilation models and solvers. \textit{Journal of Advances in Modeling Earth Systems, 13}(3), e2020MS002256.
\bibitem{ensemble_methods} Evensen, G. (1994). Sequential data assimilation... Monte Carlo methods. \textit{Journal of Geophysical Research, 99}(C5), 10143-10162. Also Houtekamer, P. L., \& Mitchell, H. L. (1998). Data assimilation using an ensemble Kalman filter technique. \textit{Monthly Weather Review, 126}(3), 796-811.
\bibitem{neural_da_bocquet} Bocquet, M., Farchi, A., \& Malartic, Q. (2023). Neural incremental data assimilation. arXiv preprint arXiv:2406.15076.
\end{thebibliography}

\end{document}