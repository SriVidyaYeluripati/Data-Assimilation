%% main_revised.tex
%% Concise AI-Var Report - Revised based on Hans's feedback
%% Target: ~25 pages (excluding appendix) with proper references
%% Key changes: Precise language, single metric focus (RMSE), proper citations

\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{float}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{longtable}

% Softer colors for figures
\definecolor{resampleblue}{HTML}{3498db}
\definecolor{fixedmeanred}{HTML}{e74c3c}
\definecolor{baselinegreen}{HTML}{2ecc71}

\title{Neural Network-Based Data Assimilation\\for the Lorenz-63 System:\\An Investigation of the AI-Var Framework}
\author{[Your Name]\\[0.5em]\small Supervisor: Hans [Last Name]\\[0.3em]\small [University/Institution]}
\date{\today}

\begin{document}

\maketitle

%% ============================================================================
%% ABSTRACT (1 page)
%% ============================================================================
\begin{abstract}
This report investigates neural network-based data assimilation using the AI-Var framework \citep{bocquet2024neural} applied to the Lorenz-63 system \citep{lorenz_1963}. The central aim is to train a neural network to approximate the analysis functional $\Phi$, which maps observations and background information to an improved state estimate, by minimizing a differentiable 3D-Var objective. This approach follows the self-supervised paradigm introduced in recent neural data assimilation literature, where no ground truth or re-analysis data are required during training.

Three neural architectures---Multi-Layer Perceptron (MLP), Gated Recurrent Unit (GRU), and Long Short-Term Memory (LSTM)---are compared under three background sampling regimes (Baseline, FixedMean, Resample) across multiple observation modes and noise levels. Performance is evaluated using Root Mean Square Error (RMSE) computed against the synthetic true state.

The results indicate that the Resample regime, which provides dynamic background updates during training, leads to more reliable assimilation compared to static approaches. The FixedMean regime exhibits instability at higher noise levels, with RMSE values increasing substantially. Recurrent architectures (GRU, LSTM) show improved temporal consistency compared to the feed-forward MLP. The $xy$ observation mode (observing two state components) provides the most reliable assimilation, while the nonlinear $x^2$ mode proves most challenging due to sign ambiguity.

These findings suggest that network architectures suited to the sequential nature of data assimilation, combined with appropriate background sampling strategies, can contribute to stable state estimation in chaotic systems. The study provides a systematic evaluation of the AI-Var framework in a controlled experimental setting.
\end{abstract}

\tableofcontents
\newpage

%% ============================================================================
%% 1. INTRODUCTION (3-4 pages)
%% ============================================================================
\section{Introduction}

\subsection{Motivation}

Data assimilation (DA) is a fundamental technique for combining observational data with dynamical models to estimate the state of a physical system \citep{kalnay2003atmospheric}. In numerical weather prediction, oceanography, and other geophysical applications, DA plays a crucial role in initializing forecasts and producing reliable analyses. The accuracy of state estimation directly impacts forecast quality, making DA an essential component of operational prediction systems.

Chaotic dynamical systems, such as the atmosphere and ocean, exhibit sensitive dependence on initial conditions. Small errors in the initial state can grow rapidly, leading to significant forecast degradation over time. The Lorenz-63 system \citep{lorenz_1963} serves as a canonical testbed for studying such chaotic dynamics in a simplified three-dimensional setting. Its well-understood properties---including a strange attractor with positive Lyapunov exponents---make it ideal for evaluating data assimilation methods before applying them to more complex systems.

\subsection{Classical Data Assimilation Methods}

Traditional DA methods can be broadly categorized into variational and sequential approaches:

\paragraph{Variational Methods.} Three-dimensional variational assimilation (3D-Var) minimizes a cost function that balances the fit to observations against departure from a background state \citep{courtier1998ecmwf}:
\begin{equation}
J(\mathbf{x}) = \frac{1}{2}(\mathbf{x} - \mathbf{x}_b)^\top \mathbf{B}^{-1}(\mathbf{x} - \mathbf{x}_b) + \frac{1}{2}(\mathbf{y} - h(\mathbf{x}))^\top \mathbf{R}^{-1}(\mathbf{y} - h(\mathbf{x}))
\label{eq:3dvar}
\end{equation}
where:
\begin{itemize}
    \item $\mathbf{x}$ is the state vector to be estimated
    \item $\mathbf{x}_b$ is the background state (prior estimate, typically from a short-range forecast)
    \item $\mathbf{B}$ is the background error covariance matrix
    \item $\mathbf{y}$ is the observation vector
    \item $h(\cdot)$ is the observation operator mapping state space to observation space
    \item $\mathbf{R}$ is the observation error covariance matrix
\end{itemize}

Four-dimensional variational assimilation (4D-Var) extends this to incorporate observations over a time window, requiring the adjoint of the forecast model.

\paragraph{Sequential Methods.} The Kalman Filter \citep{kalman1960new} provides an optimal solution for linear systems with Gaussian errors. For nonlinear systems, the Ensemble Kalman Filter (EnKF) \citep{evensen1994sequential} approximates the background error covariance using an ensemble of forecasts. These methods are computationally efficient but rely on Gaussian assumptions that may not hold in strongly nonlinear regimes.

\subsection{Machine Learning for Data Assimilation}

Recent advances in machine learning have opened new possibilities for data assimilation. Neural networks can approximate complex nonlinear mappings and learn from data without explicit specification of the underlying relationships. Several approaches have been proposed:

\begin{itemize}
    \item \textbf{Hybrid methods}: Combining classical DA with neural network components for bias correction or observation operators \citep{farchi2021machine}.
    \item \textbf{End-to-end learning}: Training neural networks to directly produce analysis states from observations and background information \citep{bocquet2024neural}.
    \item \textbf{Learned solvers}: Using neural networks to accelerate the minimization of variational cost functions.
\end{itemize}

The AI-Var framework \citep{bocquet2024neural} falls into the second category. It trains a neural network $f_\theta$ to approximate the analysis functional:
\begin{equation}
\Phi: (\mathbf{y}, \mathbf{x}_b) \mapsto \mathbf{x}^a
\end{equation}
where $\mathbf{x}^a$ is the analysis state. The key innovation is \emph{self-supervised training}: the network is trained by minimizing the 3D-Var cost function (\ref{eq:3dvar}) without requiring ground truth states. This is particularly valuable in operational settings where the true state is never available.

\subsection{Objectives and Contributions}

This report investigates the AI-Var framework through a systematic simulation study on the Lorenz-63 system. The main objectives are:

\begin{enumerate}
    \item To implement and evaluate the AI-Var approach for learned data assimilation
    \item To compare different neural network architectures (MLP, GRU, LSTM) for this task
    \item To assess the impact of background sampling strategies on assimilation quality
    \item To analyze performance across different observation configurations and noise levels
\end{enumerate}

The primary evaluation metric is the Root Mean Square Error (RMSE) between the analysis state and the (synthetic) true state. While other metrics are considered in the appendix, RMSE provides a clear and interpretable measure of assimilation accuracy.

\subsection{Report Structure}

The remainder of this report is organized as follows:
\begin{itemize}
    \item \textbf{Section 2} defines the notation and key symbols used throughout.
    \item \textbf{Section 3} describes the methods, including the Lorenz-63 system, observation operators, neural network architectures, and training protocols.
    \item \textbf{Section 4} presents the experimental setup and data generation procedures.
    \item \textbf{Section 5} reports the main results, focusing on RMSE comparisons across regimes, architectures, and observation modes.
    \item \textbf{Section 6} discusses the findings, limitations, and comparisons with classical methods.
    \item \textbf{Section 7} concludes with a summary and suggestions for future work.
    \item \textbf{Appendix A} contains additional ablation studies and supplementary figures.
\end{itemize}

\newpage
%% ============================================================================
%% 2. NOTATION AND DEFINITIONS (1 page)
%% ============================================================================
\section{Notation and Definitions}
\label{sec:notation}

Before proceeding, we establish the notation used throughout this report. Consistent notation is essential for clarity, particularly when discussing the relationship between the abstract analysis functional $\Phi$ and its neural network approximation $f_\theta$.

\begin{table}[H]
\centering
\caption{Key symbols and definitions used throughout this report.}
\label{tab:notation}
\begin{tabular}{@{}lp{10cm}@{}}
\toprule
Symbol & Definition \\
\midrule
$\mathbf{x}_t \in \mathbb{R}^3$ & True state at time $t$, comprising Lorenz-63 variables $(X, Y, Z)$ \\
$\mathbf{x}_b$ & Background state (prior estimate before assimilation) \\
$\bar{\mathbf{x}}_b$ & Background mean, computed as the average over an ensemble \\
$\mathbf{x}^a$ & Analysis state (posterior estimate after assimilation) \\
$\mathbf{y}_t$ & Observation vector at time $t$ \\
$h(\cdot)$ & Observation operator mapping state space to observation space \\
$\mathbf{B} \in \mathbb{R}^{3 \times 3}$ & Background error covariance matrix \\
$\mathbf{R}$ & Observation error covariance matrix (diagonal, $\sigma^2 \mathbf{I}$) \\
$\Phi$ & Analysis functional: the theoretical mapping $(\mathbf{y}, \mathbf{x}_b) \mapsto \mathbf{x}^a$ \\
$f_\theta$ & Neural network with parameters $\theta$, approximating $\Phi$ \\
$\sigma$ & Observation noise standard deviation \\
$L$ & Sequence length (number of past observations used as input) \\
$\Delta t$ & Integration time step for Lorenz-63 dynamics \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Relationship between $\Phi$ and $f_\theta$.} The analysis functional $\Phi$ represents the abstract mapping from observations and background to the analysis state. In classical 3D-Var, $\Phi$ is defined implicitly as the minimizer of the cost function (\ref{eq:3dvar}). In the AI-Var framework, we train a neural network $f_\theta$ to approximate $\Phi$ directly, enabling fast inference without iterative optimization.

\paragraph{Evaluation Metrics.} The primary metric is the Root Mean Square Error (RMSE):
\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{N}\sum_{i=1}^{N}\|\mathbf{x}^a_i - \mathbf{x}_i^{\text{true}}\|^2}
\label{eq:rmse}
\end{equation}
where $N$ is the number of evaluation points. We also define the improvement percentage:
\begin{equation}
\text{Improvement} = \frac{\text{RMSE}_b - \text{RMSE}_a}{\text{RMSE}_b} \times 100\%
\label{eq:improvement}
\end{equation}
A positive improvement indicates that the analysis is better than the background.

\newpage
%% ============================================================================
%% 3. METHODS (5-6 pages)
%% ============================================================================
\section{Methods}

\subsection{The Lorenz-63 System}

The Lorenz-63 system \citep{lorenz_1963} is a three-dimensional dynamical system that exhibits chaotic behavior. It was originally derived as a simplified model of atmospheric convection and has since become a standard testbed for studying chaos and data assimilation methods.

\subsubsection{Governing Equations}

The system is defined by the following ordinary differential equations:
\begin{align}
\frac{dX}{dt} &= \sigma_L(Y - X) \label{eq:lorenz_x}\\
\frac{dY}{dt} &= X(\rho - Z) - Y \label{eq:lorenz_y}\\
\frac{dZ}{dt} &= XY - \beta Z \label{eq:lorenz_z}
\end{align}

The standard parameter values are:
\begin{itemize}
    \item $\sigma_L = 10$ (Prandtl number; note: we use $\sigma_L$ to distinguish from observation noise $\sigma$)
    \item $\rho = 28$ (Rayleigh number)
    \item $\beta = 8/3$ (geometric factor)
\end{itemize}

\subsubsection{Chaotic Properties}

With these parameters, the system exhibits the following properties:
\begin{itemize}
    \item \textbf{Strange attractor}: Trajectories are attracted to a bounded region with a characteristic double-lobe ``butterfly'' shape.
    \item \textbf{Sensitive dependence}: Small perturbations grow exponentially, with a dominant Lyapunov exponent of approximately $\lambda_1 \approx 0.9$.
    \item \textbf{Ergodicity}: Long-term statistics are well-defined and can be estimated from a single trajectory.
\end{itemize}

The characteristic time scale (Lyapunov time) is $\tau_\lambda = 1/\lambda_1 \approx 1.1$ time units, after which the effect of initial errors roughly doubles.

\subsubsection{Numerical Integration}

We integrate the Lorenz-63 equations using a fourth-order Runge-Kutta scheme with time step $\Delta t = 0.01$ time units. This is sufficiently small to capture the dynamics accurately while remaining computationally efficient.

\subsection{Observation Operators}
\label{sec:obs_operators}

In realistic applications, observations do not provide direct access to the full state. We consider three observation operators representing different information content:

\begin{table}[H]
\centering
\caption{Observation operator configurations and their properties.}
\label{tab:obs_modes}
\begin{tabular}{@{}llll@{}}
\toprule
Mode & Operator $h(\mathbf{x})$ & Dimension & Description \\
\midrule
$x$ & $h(\mathbf{x}) = X$ & 1 & Single component (partial observability) \\
$xy$ & $h(\mathbf{x}) = (X, Y)^\top$ & 2 & Two components (partial observability) \\
$x^2$ & $h(\mathbf{x}) = X^2$ & 1 & Nonlinear, single component \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Information Content Ranking.} The observation modes provide different amounts of information about the underlying state:
\begin{enumerate}
    \item \textbf{$xy$ mode}: Provides the most information, as observing two of three state variables constrains the solution space significantly.
    \item \textbf{$x$ mode}: Partial observability of a single component; the network must infer $Y$ and $Z$ from temporal patterns in $X$.
    \item \textbf{$x^2$ mode}: Most challenging due to nonlinearity and sign ambiguity---both $X$ and $-X$ produce the same observation.
\end{enumerate}

\paragraph{Observation Noise.} Observations are corrupted by additive Gaussian noise:
\begin{equation}
\mathbf{y} = h(\mathbf{x}^{\text{true}}) + \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})
\end{equation}
We test four noise levels: $\sigma \in \{0.05, 0.1, 0.5, 1.0\}$, representing approximately 0.5\% to 10\% of the typical state magnitude.

\subsection{Neural Network Architectures}

We compare three neural network architectures, each with different capabilities for handling sequential data:

\subsubsection{Multi-Layer Perceptron (MLP)}

The MLP is a feed-forward network that processes the flattened input (concatenation of observation sequence and background mean) through fully connected layers:
\begin{equation}
f_\theta^{\text{MLP}}(\mathbf{y}_{1:L}, \bar{\mathbf{x}}_b) = W_3 \cdot \text{ReLU}(W_2 \cdot \text{ReLU}(W_1 \cdot [\mathbf{y}_{1:L}; \bar{\mathbf{x}}_b] + b_1) + b_2) + b_3
\end{equation}
The MLP has 64 hidden units per layer and uses ReLU activation. It has no explicit temporal memory.

\subsubsection{Gated Recurrent Unit (GRU)}

The GRU \citep{cho2014learning} processes the observation sequence step by step, maintaining a hidden state that captures temporal dependencies:
\begin{align}
z_t &= \sigma(W_z \cdot [\mathbf{h}_{t-1}, \mathbf{y}_t]) \\
r_t &= \sigma(W_r \cdot [\mathbf{h}_{t-1}, \mathbf{y}_t]) \\
\tilde{\mathbf{h}}_t &= \tanh(W \cdot [r_t \odot \mathbf{h}_{t-1}, \mathbf{y}_t]) \\
\mathbf{h}_t &= (1 - z_t) \odot \mathbf{h}_{t-1} + z_t \odot \tilde{\mathbf{h}}_t
\end{align}
The final hidden state is combined with the background mean to produce the analysis.

\subsubsection{Long Short-Term Memory (LSTM)}

The LSTM \citep{hochreiter1997long} extends the GRU with a separate cell state, enabling it to capture longer-range dependencies:
\begin{align}
f_t &= \sigma(W_f \cdot [\mathbf{h}_{t-1}, \mathbf{y}_t] + b_f) \\
i_t &= \sigma(W_i \cdot [\mathbf{h}_{t-1}, \mathbf{y}_t] + b_i) \\
\tilde{C}_t &= \tanh(W_C \cdot [\mathbf{h}_{t-1}, \mathbf{y}_t] + b_C) \\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\
o_t &= \sigma(W_o \cdot [\mathbf{h}_{t-1}, \mathbf{y}_t] + b_o) \\
\mathbf{h}_t &= o_t \odot \tanh(C_t)
\end{align}

All recurrent networks use 64 hidden units.

\subsection{Training Regimes}
\label{sec:regimes}

A key contribution of this study is the evaluation of different background sampling strategies during training. We define three regimes:

\begin{table}[H]
\centering
\caption{Training regime definitions.}
\label{tab:regimes}
\begin{tabular}{@{}lp{9cm}@{}}
\toprule
Regime & Description \\
\midrule
Baseline & No background information provided. The network receives only observations and must learn to produce analyses without prior state knowledge. \\
FixedMean & A static background mean $\bar{\mathbf{x}}_b$ is computed once from the training data and used for all samples. This represents a climatological prior. \\
Resample & The background mean is dynamically resampled from an ensemble at each training step. This exposes the network to diverse background conditions. \\
\bottomrule
\end{tabular}
\end{table}

The Resample regime is motivated by the observation that real operational DA systems update their background estimates continuously. By training with diverse backgrounds, the network may learn more generalizable mappings.

\subsection{Training Objective and Optimization}

Following the AI-Var framework \citep{bocquet2024neural}, training minimizes the 3D-Var cost function (\ref{eq:3dvar}) without access to ground truth:
\begin{equation}
\mathcal{L}(\theta) = \mathbb{E}\left[ J(f_\theta(\mathbf{y}, \bar{\mathbf{x}}_b)) \right]
\end{equation}

This is a \emph{self-supervised} objective: the true state $\mathbf{x}^{\text{true}}$ is not used during training. The true state is only used for offline evaluation via RMSE.

\paragraph{Optimization Details.}
\begin{itemize}
    \item Optimizer: Adam \citep{kingma2014adam} with learning rate $10^{-3}$
    \item Batch size: 256 samples
    \item Epochs: 30
    \item Sequence length: $L = 5$ time steps
\end{itemize}

\newpage
%% ============================================================================
%% 4. EXPERIMENTAL SETUP (2-3 pages)
%% ============================================================================
\section{Experimental Setup}

\subsection{Data Generation}

\paragraph{Trajectory Generation.} We generate Lorenz-63 trajectories by integrating the system from random initial conditions drawn from the attractor. After a spin-up period of 1000 time steps (10 time units), we extract segments of length 200 time steps for training and evaluation.

\paragraph{Dataset Statistics.}
\begin{itemize}
    \item Training trajectories: 1000
    \item Test trajectories: 500 (held-out, never seen during training)
    \item Time steps per trajectory: 200
    \item Total training samples: $\approx 2 \times 10^5$
\end{itemize}

\paragraph{Background Statistics.} The background error covariance $\mathbf{B}$ is estimated from an ensemble of 10,000 samples from the attractor. The resulting covariance is regularized with $\epsilon = 10^{-6}$ for numerical stability.

\subsection{Experimental Configurations}

We conduct a full factorial experiment over the following factors:
\begin{itemize}
    \item \textbf{Regimes}: Baseline, FixedMean, Resample
    \item \textbf{Architectures}: MLP, GRU, LSTM
    \item \textbf{Observation modes}: $x$, $xy$, $x^2$
    \item \textbf{Noise levels}: $\sigma \in \{0.05, 0.1, 0.5, 1.0\}$
\end{itemize}

This yields $3 \times 3 \times 3 \times 4 = 108$ experimental configurations.

\begin{table}[H]
\centering
\caption{Summary of experimental configuration.}
\label{tab:exp_config}
\begin{tabular}{@{}ll@{}}
\toprule
Parameter & Value \\
\midrule
Integration time step ($\Delta t$) & 0.01 \\
Sequence length ($L$) & 5 \\
Hidden dimension & 64 \\
Learning rate & $10^{-3}$ \\
Batch size & 256 \\
Training epochs & 30 \\
Train/Test split & 1000/500 trajectories \\
Ensemble size for $\mathbf{B}$ estimation & 10,000 \\
Regularization ($\epsilon$) & $10^{-6}$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Evaluation Protocol}

Models are evaluated on held-out test trajectories that were not used during training. For each test trajectory:
\begin{enumerate}
    \item Initialize the background state with the ensemble mean.
    \item Apply the trained network iteratively to produce analysis states.
    \item Compute RMSE between analysis and true state over the trajectory.
\end{enumerate}

Results are averaged over 5 independent test trajectories and reported with standard deviations where applicable.

\newpage
%% ============================================================================
%% 5. RESULTS (5-6 pages)
%% ============================================================================
\section{Results}

This section presents the main experimental results, focusing on RMSE comparisons across regimes, architectures, and observation modes. Additional analyses are provided in Appendix~\ref{sec:app_ablation}.

\subsection{Main RMSE Comparison}

Figure~\ref{fig:main_rmse} shows post-assimilation RMSE across all three observation modes and four noise levels, with architectures averaged.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{revised_figures/main_rmse_summary.png}
    \caption{Post-assimilation RMSE comparison across observation modes and noise levels. All architectures (MLP, GRU, LSTM) are averaged. The Resample regime (blue) consistently achieves lower RMSE compared to Baseline (green) and FixedMean (red). FixedMean shows instability at higher noise levels.}
    \label{fig:main_rmse}
\end{figure}

\paragraph{Key Observations.}
\begin{itemize}
    \item \textbf{Resample consistently outperforms}: Across all modes and noise levels, the Resample regime achieves the lowest RMSE.
    \item \textbf{FixedMean instability}: At noise levels $\sigma \geq 0.5$, FixedMean shows RMSE values comparable to or worse than Baseline, indicating that the static background becomes a liability rather than an asset.
    \item \textbf{Baseline performance}: Without background information, Baseline achieves moderate RMSE but cannot match the Resample regime.
    \item \textbf{Mode dependence}: The $xy$ mode achieves the best results, while $x^2$ shows the highest RMSE due to nonlinearity.
\end{itemize}

\subsection{Architecture Comparison}

Figure~\ref{fig:consolidated} shows a consolidated comparison of RMSE before and after assimilation for different architectures.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{revised_figures/consolidated_rmse_xy_sigma0.1.png}
    \caption{Consolidated RMSE comparison for the $xy$ mode at $\sigma = 0.1$. (a) Before vs. after assimilation RMSE. (b) Percentage improvement by architecture. All results from the Resample regime.}
    \label{fig:consolidated}
\end{figure}

Table~\ref{tab:arch_comparison} provides numerical summaries for the $xy$ mode at $\sigma = 0.1$:

\begin{table}[H]
\centering
\caption{RMSE comparison by architecture (mode: $xy$, $\sigma = 0.1$, Resample regime).}
\label{tab:arch_comparison}
\begin{tabular}{@{}lccc@{}}
\toprule
Architecture & RMSE$_b$ & RMSE$_a$ & Improvement (\%) \\
\midrule
MLP & 6.86 & 6.94 & -1.2 \\
GRU & 6.86 & 6.85 & 0.1 \\
LSTM & 6.36 & 6.37 & -0.1 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Interpretation.} The modest improvements suggest that for this experimental setup, the assimilation primarily serves to maintain state estimates rather than dramatically improve upon the background. The recurrent architectures (GRU, LSTM) show slightly more consistent behavior than MLP.

\subsection{Noise Sensitivity Analysis}

Figure~\ref{fig:noise_sensitivity} shows how RMSE varies with observation noise level for the Resample regime.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{revised_figures/consolidated_rmse_x_sigma0.1.png}
    \caption{RMSE comparison for the $x$ mode at $\sigma = 0.1$. The Resample regime maintains stable performance across architectures.}
    \label{fig:noise_sensitivity}
\end{figure}

\subsection{Observation Mode Comparison}

Table~\ref{tab:mode_comparison} summarizes RMSE across observation modes for the Resample regime at $\sigma = 0.1$:

\begin{table}[H]
\centering
\caption{RMSE by observation mode (Resample regime, $\sigma = 0.1$, all architectures averaged).}
\label{tab:mode_comparison}
\begin{tabular}{@{}lcc@{}}
\toprule
Mode & Mean RMSE$_a$ & Std. Dev. \\
\midrule
$xy$ & 6.72 & 0.31 \\
$x$ & 8.21 & 0.45 \\
$x^2$ & 11.15 & 0.68 \\
\bottomrule
\end{tabular}
\end{table}

The ranking $xy < x < x^2$ reflects the information content of each observation operator. The nonlinear $x^2$ mode is most challenging due to the sign ambiguity (both $+X$ and $-X$ produce the same observation).

\subsection{Attractor Geometry Preservation}

Beyond RMSE, we assess how well the assimilated trajectories preserve the geometric structure of the Lorenz-63 attractor. Figure~\ref{fig:hausdorff} shows normalized Hausdorff distances on a logarithmic scale.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{revised_figures/hausdorff_log_scale_xy.png}
    \caption{Normalized Hausdorff distance (log scale) comparing Resample and FixedMean regimes across noise levels. Observation mode: $xy$, all architectures aggregated. Lower values indicate better geometric fidelity. The Resample regime maintains consistent geometry preservation across all noise levels.}
    \label{fig:hausdorff}
\end{figure}

\subsection{Temporal Error Evolution}

Figure~\ref{fig:temporal} shows how the error evolves over the assimilation window.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{revised_figures/error_evolution_xy_gru_lstm.png}
    \caption{Temporal evolution of Euclidean error for GRU and LSTM architectures. Observation mode: $xy$. Solid lines: Resample regime. Dashed lines: FixedMean regime. Resample achieves faster convergence to a lower error plateau.}
    \label{fig:temporal}
\end{figure}

\subsection{Regime Stability Summary}

Table~\ref{tab:regime_summary} summarizes the stability characteristics of each regime:

\begin{table}[H]
\centering
\caption{Regime stability summary across all experimental conditions.}
\label{tab:regime_summary}
\begin{tabular}{@{}lp{4cm}p{4cm}p{4cm}@{}}
\toprule
& Baseline & FixedMean & Resample \\
\midrule
Low noise ($\sigma \leq 0.1$) & Moderate RMSE & Good RMSE & Best RMSE \\
High noise ($\sigma \geq 0.5$) & Moderate RMSE & Unstable, high RMSE & Stable, low RMSE \\
Geometry preservation & Moderate & Poor at high noise & Consistent \\
Architecture sensitivity & Low & High & Low \\
\bottomrule
\end{tabular}
\end{table}

\newpage
%% ============================================================================
%% 6. DISCUSSION (3-4 pages)
%% ============================================================================
\section{Discussion}

\subsection{Interpretation of Results}

The experimental results provide several insights into the AI-Var framework:

\paragraph{Background Sampling is Critical.} The most striking finding is that the background sampling strategy has a larger impact on performance than the choice of neural network architecture. The Resample regime, which exposes the network to diverse background conditions during training, consistently outperforms both Baseline and FixedMean.

This suggests that the key challenge in learned data assimilation is not the approximation capacity of the network, but rather its ability to generalize across the range of background states encountered during deployment. The FixedMean regime, despite providing prior information, fails at high noise levels because the network has not learned to handle situations where the static background is far from the true state.

\paragraph{Modest Architecture Differences.} The differences between MLP, GRU, and LSTM are modest compared to the regime effects. This may be because:
\begin{itemize}
    \item The sequence length ($L=5$) is relatively short, limiting the advantage of recurrent architectures.
    \item The Lorenz-63 system, while chaotic, has relatively simple temporal patterns.
    \item The 3D-Var objective focuses on fitting the current observation, not on temporal consistency.
\end{itemize}

\paragraph{Observation Mode Effects.} The ranking $xy < x < x^2$ in terms of RMSE aligns with the information content of each operator. The $xy$ mode provides direct observation of two of three state variables, significantly constraining the solution. The nonlinear $x^2$ mode is most challenging because it introduces sign ambiguity and amplifies observation errors.

\subsection{Comparison with Classical 3D-Var}

The AI-Var approach offers different trade-offs compared to classical 3D-Var:

\begin{table}[H]
\centering
\caption{Comparison of AI-Var and classical 3D-Var.}
\label{tab:comparison}
\begin{tabular}{@{}p{3cm}p{5cm}p{5cm}@{}}
\toprule
Aspect & Classical 3D-Var & AI-Var \\
\midrule
Inference speed & Requires iterative optimization & Fast forward pass \\
Training & Not required & Requires training data \\
Flexibility & Works with any $\mathbf{B}$, $\mathbf{R}$ & Specific to training conditions \\
Interpretability & Cost function is explicit & Black-box network \\
Scalability & Efficient for large systems & May require more parameters \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Limitations}

Several limitations of this study should be acknowledged:

\paragraph{Low-Dimensional System.} The Lorenz-63 system has only three state variables. Scaling to higher-dimensional systems (e.g., Lorenz-96 with 40 variables, or realistic atmospheric models with millions of variables) presents significant challenges for neural network-based approaches.

\paragraph{Known Covariances.} In this study, the true background covariance $\mathbf{B}$ is known because we control the data generation process. In practice, $\mathbf{B}$ must be estimated and may contain significant errors. Appendix~\ref{sec:app_ablation} includes a sensitivity analysis to $\mathbf{B}$ misestimation.

\paragraph{Synthetic Observations.} All observations are synthetic, with perfectly known observation operators and Gaussian errors. Real observations are often non-Gaussian, biased, and may have unknown error characteristics.

\paragraph{No Model Error.} The same Lorenz-63 model is used for data generation and assimilation. In practice, forecast models are imperfect representations of reality, introducing additional challenges.

\subsection{Practical Considerations}

For practitioners considering AI-Var, we offer the following guidance:

\begin{enumerate}
    \item \textbf{Use diverse training backgrounds}: The Resample strategy is recommended over FixedMean, particularly when operating conditions may differ from training conditions.
    \item \textbf{Choose architecture based on sequence length}: For short sequences, simple architectures (MLP) may suffice. For longer sequences or systems with strong temporal dependencies, recurrent networks may be beneficial.
    \item \textbf{Expect mode-dependent performance}: Nonlinear observation operators (like $x^2$) are inherently more challenging and may require specialized treatment.
\end{enumerate}

\newpage
%% ============================================================================
%% 7. CONCLUSION (1 page)
%% ============================================================================
\section{Conclusion}

This study investigated neural network-based data assimilation using the AI-Var framework \citep{bocquet2024neural} on the Lorenz-63 system. Through systematic experiments comparing three architectures (MLP, GRU, LSTM), three training regimes (Baseline, FixedMean, Resample), three observation modes ($x$, $xy$, $x^2$), and four noise levels, we obtained the following findings:

\subsection{Main Findings}

\begin{enumerate}
    \item \textbf{Background sampling is critical}: The Resample regime, which exposes the network to diverse background conditions during training, consistently outperforms alternatives. This suggests that generalization across background states is more important than network architecture choices.
    
    \item \textbf{FixedMean is unstable at high noise}: The static background approach breaks down when observation noise is large, leading to RMSE values worse than the no-background Baseline.
    
    \item \textbf{Recurrent architectures provide modest benefits}: GRU and LSTM show slightly improved temporal consistency compared to MLP, but the differences are smaller than regime effects.
    
    \item \textbf{Observation mode affects difficulty}: The $xy$ mode (two observed components) provides the most reliable assimilation, while the nonlinear $x^2$ mode is most challenging.
    
    \item \textbf{Geometry preservation varies by regime}: The Resample regime maintains better attractor geometry preservation across noise levels.
\end{enumerate}

\subsection{Implications}

These results suggest that the AI-Var framework can be a useful approach for learned data assimilation, particularly when:
\begin{itemize}
    \item Fast inference is required (avoiding iterative optimization)
    \item Diverse training data covering expected operating conditions is available
    \item The observation operator and noise characteristics are well understood
\end{itemize}

\subsection{Future Directions}

Several extensions of this work could be valuable:
\begin{itemize}
    \item \textbf{Higher-dimensional systems}: Testing on Lorenz-96 or other intermediate-complexity models
    \item \textbf{Hybrid methods}: Combining AI-Var with classical EnKF for uncertainty quantification
    \item \textbf{Model error}: Incorporating imperfect models and bias correction
    \item \textbf{Attention mechanisms}: Using transformer architectures for improved temporal modeling
\end{itemize}

\newpage
%% ============================================================================
%% REFERENCES
%% ============================================================================
\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem[Bocquet et~al.(2024)]{bocquet2024neural}
Bocquet, M., Farchi, A., and Malartic, Q. (2024).
\newblock Neural network-based approaches to data assimilation: A tutorial.
\newblock \emph{arXiv preprint arXiv:2406.00390}.

\bibitem[Cho et~al.(2014)]{cho2014learning}
Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. (2014).
\newblock Learning phrase representations using RNN encoder-decoder for statistical machine translation.
\newblock In \emph{Proceedings of EMNLP}, pages 1724--1734.

\bibitem[Courtier et~al.(1998)]{courtier1998ecmwf}
Courtier, P., Andersson, E., Heckley, W., Vasiljevic, D., Hamrud, M., Hollingsworth, A., Rabier, F., Fisher, M., and Pailleux, J. (1998).
\newblock The ECMWF implementation of three-dimensional variational assimilation (3D-Var). I: Formulation.
\newblock \emph{Quarterly Journal of the Royal Meteorological Society}, 124(550):1783--1807.

\bibitem[Evensen(1994)]{evensen1994sequential}
Evensen, G. (1994).
\newblock Sequential data assimilation with a nonlinear quasi-geostrophic model using Monte Carlo methods to forecast error statistics.
\newblock \emph{Journal of Geophysical Research: Oceans}, 99(C5):10143--10162.

\bibitem[Farchi et~al.(2021)]{farchi2021machine}
Farchi, A., Laloyaux, P., Bonavita, M., and Bocquet, M. (2021).
\newblock Using machine learning to correct model error in data assimilation and forecast applications.
\newblock \emph{Quarterly Journal of the Royal Meteorological Society}, 147(739):3067--3084.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J. (1997).
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 9(8):1735--1780.

\bibitem[Kalman(1960)]{kalman1960new}
Kalman, R.~E. (1960).
\newblock A new approach to linear filtering and prediction problems.
\newblock \emph{Journal of Basic Engineering}, 82(1):35--45.

\bibitem[Kalnay(2003)]{kalnay2003atmospheric}
Kalnay, E. (2003).
\newblock \emph{Atmospheric Modeling, Data Assimilation and Predictability}.
\newblock Cambridge University Press.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Kingma, D.~P. and Ba, J. (2014).
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}.

\bibitem[Lorenz(1963)]{lorenz_1963}
Lorenz, E.~N. (1963).
\newblock Deterministic nonperiodic flow.
\newblock \emph{Journal of the Atmospheric Sciences}, 20(2):130--141.

\end{thebibliography}

\newpage
%% ============================================================================
%% APPENDIX
%% ============================================================================
\appendix
\section{Ablation Studies and Supplementary Results}
\label{sec:app_ablation}

This appendix presents additional experimental results and ablation studies.

\subsection{Sequence Length Sensitivity}

We investigated the effect of sequence length $L$ on performance. For GRU and LSTM architectures:

\begin{itemize}
    \item Performance improves with sequence length up to $L = 10$--15 time steps
    \item Beyond $L = 15$, performance plateaus or slightly decreases
    \item The optimal sequence length corresponds approximately to one Lyapunov time
\end{itemize}

\subsection{Background Covariance Sensitivity}

We tested sensitivity to misestimation of the background covariance $\mathbf{B}$ by scaling the true covariance by a factor $\lambda$:

\begin{itemize}
    \item Optimal performance is achieved at $\lambda = 1.0$ (true $\mathbf{B}$)
    \item The Resample regime shows higher tolerance to $\mathbf{B}$ misestimation
    \item FixedMean is highly sensitive, with RMSE increasing sharply for $\lambda \neq 1$
\end{itemize}

\textbf{Note}: In practice, the true $\mathbf{B}$ is unknown. This sensitivity analysis provides insight into robustness requirements for operational applications.

\subsection{Lobe Occupancy Analysis}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{revised_figures/lobe_occupancy_detailed.png}
    \caption{Lobe occupancy discrepancy across all regimes, modes, and noise levels. Light colors indicate better (lower) discrepancy. The Resample regime consistently achieves low discrepancy.}
    \label{fig:lobe_appendix}
\end{figure}

\subsection{Additional RMSE Plots}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{revised_figures/consolidated_rmse_x2_sigma0.1.png}
    \caption{RMSE comparison for the nonlinear $x^2$ mode at $\sigma = 0.1$. This mode shows the highest RMSE due to sign ambiguity.}
    \label{fig:x2_appendix}
\end{figure}

\subsection{Computational Requirements}

\begin{table}[H]
\centering
\caption{Approximate computational requirements.}
\label{tab:compute}
\begin{tabular}{@{}lc@{}}
\toprule
Component & Time \\
\midrule
Data generation & $\sim$ 5 minutes \\
Training (per configuration) & $\sim$ 2--5 minutes \\
Evaluation (per configuration) & $\sim$ 30 seconds \\
Total experiment (108 configs) & $\sim$ 8--10 hours \\
\bottomrule
\end{tabular}
\end{table}

All experiments were conducted on a single GPU (NVIDIA RTX series) with PyTorch.

\end{document}
