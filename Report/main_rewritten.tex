\documentclass[12pt,a4paper]{article}
% ----------------- Packages -----------------
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{sfmath}
\usepackage{upgreek}
\usepackage{enumitem}
\usepackage{float}
\usepackage{tabularx}
\usepackage[square, numbers, sort&compress]{natbib}
\usepackage{array}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage{minted}
\usepackage{tikz}
\usepackage{newunicodechar}
\newunicodechar{└}{\textSFxx}

\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usetikzlibrary{arrows.meta, positioning}
\usepackage{standalone}
% \usepackage{textgreek}
\newunicodechar{│}{|}

\tikzset{
  startstop/.style = {ellipse, draw=black, fill=green!20, text width=3cm, align=center, minimum height=1cm},
  process/.style   = {rectangle, draw=black, fill=blue!10, text width=4cm, align=center, rounded corners, minimum height=1cm},
  decision/.style  = {diamond, draw=black, fill=orange!15, text width=3cm, align=center, minimum height=1cm},
  arrow/.style     = {thick, ->, >=Latex}
}

\geometry{margin=1in}
\setstretch{1.2}
\setcounter{secnumdepth}{3}
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}
% ----------------- Document -----------------
\DeclareUnicodeCharacter{251C}{|}
\DeclareUnicodeCharacter{2500}{\textemdash}
\begin{document}
% ---------- Title Page ----------
\begin{titlepage}
\centering
\vspace*{2cm}
{\LARGE\bfseries AI-Based Data Assimilation: Learning the Functional of Analysis Estimation \par}
\vspace{0.5cm}
{\Large A Pilot Study on the Lorenz-63 System under Chaotic and Stochastic Dynamics \par}
\vspace{2cm}
{\large \textbf{Author:} Sri Vidya Yeluripati \par}
\vspace{0.3cm}
{\large \textbf{Supervisor:} Prof.\ Dr.\ Claudia Strauch \par}
\vspace{0.3cm}
{\large \textbf{Course:} Master Practical (WS 2024/25) \par}
\vspace{0.3cm}
{\large \textbf{Date:} 24 November 2025 \par}
\vspace{0.3cm}
{\large \textbf{Code Repository:} \href{https://github.com/SriVidyaYeluripati/Data-Assimilation}{GitHub} \par}
\vfill
\end{titlepage}
\tableofcontents
\newpage

%=============================================================================
% ABSTRACT
%=============================================================================
\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

Data assimilation is central to state estimation in nonlinear dynamical systems arising in atmospheric and ocean sciences, where chaotic sensitivity to initial conditions limits forecasting accuracy. Classical variational methods such as three-dimensional variational assimilation (3D-Var) require iterative minimization of a cost function, while ensemble-based approaches like the Ensemble Kalman Filter (EnKF) employ Gaussian approximations of the forecast distribution.  Machine learning-based data assimilation, exemplified by the AI-Var framework introduced by Fablet et al.\ \cite{ai_da_fablet}, replaces explicit variational optimization with a learned mapping. Specifically, a neural network $f_\theta$ is trained to approximate the analysis functional $\Phi$---the mapping from observations, a background state, and their respective error covariances to the maximum a-posteriori (MAP) state estimate---by minimizing a differentiable 3D-Var objective in a self-supervised manner. Crucially, no ground truth or re-analysis data are used during training; the true state is employed only for offline evaluation.

This pilot study investigates adaptations of the AI-Var scheme on the canonical Lorenz-63 system via a simulation study, testing stability and generalization under increasing observation noise ($\sigma_{\mathrm{obs}} \in \{0.05, 0.10, 0.50, 1.00\}$) and three observation operators: two partial linear operators ($h(x) = x_1$ and $h(x) = (x_1, x_2)$) and one nonlinear operator ($h(x) = x_1^2$). Three neural architectures---a Multilayer Perceptron (MLP), Gated Recurrent Unit (GRU), and Long Short-Term Memory (LSTM)---are compared for their capacity to approximate the analysis functional under different background conditioning strategies.

Two preliminary findings emerge from this investigation. First, recurrent architectures (GRU and LSTM) tend to produce lower test error than the memory-less MLP, although the differences are modest and regime-dependent, leaving the question of optimal architecture inconclusive. Second, a stochastic resampling strategy for the employed background mean substantially improves generalization and prevents catastrophic failures such as attractor escape observed under fixed-background training. Performance is evaluated via test RMSE computed against the synthetic true state, used only for offline diagnostics, along with geometric metrics including normalized Hausdorff distance and lobe occupancy discrepancy.

These results suggest that the AI-Var scheme can be improved with network architectures catering to the sequential nature of the data assimilation problem; it can successfully emulate variational data assimilation for low-dimensional chaotic systems when temporal modeling and stochastic regularization are incorporated. However, the evidence remains preliminary, and further investigation is needed to establish which architectural choices generalize to higher-dimensional settings.

\newpage

%=============================================================================
% SECTION 1: INTRODUCTION
%=============================================================================
\section{Introduction}
\label{sec:intro}

State estimation in chaotic dynamical systems---crucial for accurate forecasting---is a fundamental challenge in many disciplines in science and engineering, particularly in numerical weather prediction and oceanography. Chaotic systems, epitomized by the Lorenz-63 atmospheric convection model \cite{lorenz_1963}, exhibit sensitive dependence on initial conditions: even infinitesimal perturbations in the initial state grow exponentially over time, rendering long-term deterministic prediction practically impossible. To mitigate this divergence, data assimilation techniques optimally combine noisy observations with the model's short-term forecast (the \emph{background} state) to produce a refined \emph{analysis} state, thereby constraining the estimated trajectory to the true dynamical manifold and reducing forecast uncertainty.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\linewidth]{lorenz.png}
\caption{The Lorenz-63 attractor exhibits the characteristic double-wing (butterfly) structure arising from chaotic dynamics with standard parameters $\sigma=10$, $\rho=28$, $\beta=8/3$. Sample trajectories illustrate the sensitive dependence on initial conditions that motivates data assimilation.}
\label{fig:lorenz}
\end{figure}

Classical data assimilation methods rely on knowledge of the physical model and generally assume Gaussian error distributions. Variational approaches such as 3D-Var formulate the analysis problem as minimizing a cost function that balances fidelity to the background forecast against agreement with observations; this requires iterative numerical solvers and depends on accurate specification of the background error covariance matrix $B$ (quantifying uncertainty in the prior forecast) and the observation error covariance matrix $R$ (quantifying measurement uncertainty). Ensemble-based methods such as the Ensemble Kalman Filter \cite{ensemble_methods} employ Monte Carlo sampling to approximate the forecast distribution but still invoke Gaussian approximations at the update step. Both approaches incur substantial computational cost during inference, particularly in high-dimensional operational settings.

Machine learning-based data assimilation, such as the AI-Var framework introduced by Fablet et al.\ \cite{ai_da_fablet}, replaces explicit variational optimization with a learned surrogate. A parametric neural network $f_\theta$ is trained to approximate the analysis functional $\Phi$---the mapping from observations $y$, background information $(\bar{x}, B)$, and observation statistics $(h, R)$ to the MAP state estimate $x^a$ that minimizes the 3D-Var cost. While training with re-analysis data is a common option, this study instead investigates self-supervised training: the network learns by minimizing the variational cost directly, without access to ground truth or re-analysis labels. The true state $x_{\text{true}}$, available here due to the nature of the simulation study, is used only for offline evaluation and never during training.

A critical challenge in this problem setting is that the true state is unobservable in operational applications, requiring the network to learn from the combination of both sources of information---observations and background forecasts---rather than from supervised labels. This pilot study systematically investigates how different neural architectures approximate the analysis functional within a sequential adaptation of the AI-Var framework, testing the hypothesis that temporal sequence modeling may enhance stability and coherence in chaotic, partially observed systems. The investigation is conducted via a controlled simulation study on the Lorenz-63 system, which provides ground truth trajectories for rigorous evaluation.

\subsection{Notation and Definitions}
\label{sec:notation}

Before proceeding, we establish notation used throughout this manuscript. Let $x \in \mathbb{R}^3$ denote the state vector of the Lorenz-63 system with components $(x_1, x_2, x_3)$. At each time step $t$, we have:

\begin{table}[H]
\centering
\small
\begin{tabularx}{0.95\textwidth}{l X}
\toprule
\textbf{Symbol} & \textbf{Definition} \\
\midrule
$x_t^{\text{true}}$ & True (latent) state at time $t$, known only in simulation \\
$\bar{x}_t$ & Background mean (prior forecast), computed from an ensemble \\
$y_t$ & Observation vector at time $t$, related to $x_t^{\text{true}}$ via $y_t = h(x_t^{\text{true}}) + \varepsilon_t$ \\
$h(\cdot)$ & Observation operator mapping state to observation space \\
$B$ & Background error covariance matrix (quantifies prior uncertainty) \\
$R$ & Observation error covariance matrix (quantifies measurement noise) \\
$\varepsilon_t$ & Observation noise, $\varepsilon_t \sim \mathcal{N}(0, R)$ \\
$x_t^a$ & Analysis state (posterior estimate after assimilation) \\
$\hat{x}_t^a$ & Neural network output approximating the analysis state \\
$\Phi$ & Analysis functional: abstract mapping from $(y, \bar{x}, h, B, R) \mapsto x^a$ \\
$f_\theta$ & Neural network with parameters $\theta$ approximating $\Phi$ \\
$L$ & Sequence window length for recurrent architectures (typically 5) \\
$\sigma_{\text{obs}}$ & Standard deviation of observation noise \\
\bottomrule
\end{tabularx}
\caption{Notation used throughout this manuscript. The analysis functional $\Phi$ represents the theoretical mapping from assimilation inputs to the optimal analysis state; $f_\theta$ denotes its parametric neural network approximation.}
\label{tab:notation}
\end{table}

Capital letters $X$, $Y$, $Z$ are used informally when referring to the three state components; lowercase $x_1$, $x_2$, $x_3$ or simply $x$, $y$, $z$ (when context is clear) denote the same components in equations. The distinction between the analysis functional $\Phi$ (the theoretical optimal mapping) and the neural network $f_\theta$ (its learned approximation) is maintained throughout: we seek to train $f_\theta$ such that $\hat{x}^a = f_\theta(\bar{x}, y, \ldots) \approx \Phi(y, \bar{x}, h, B, R)$.

\subsection{Project Scope and Aims}
\label{sec:scope}

The central aim of this pilot study is to develop and evaluate a machine learning-based data assimilation framework for the AI-Var approach \cite{ai_da_fablet}, utilizing the Lorenz-63 system as a controlled testbed. Specifically, the investigation addresses the following questions:

\paragraph{Architectural Benchmarking.} Does temporal context improve assimilation performance? Three architectures are compared: a memory-less MLP that processes only the current observation, and two recurrent networks (GRU and LSTM) that process observation sequences $y_{t-L+1:t}$ to incorporate temporal dependencies.

\paragraph{Observation Mode Sensitivity.} How does the structure of the observation operator affect reconstruction accuracy? Three operators are tested: two partial linear operators providing different amounts of state information, and one nonlinear operator ($h(x) = x_1^2$) that introduces ill-posedness by removing sign information.

\paragraph{Noise Tolerance.} At what noise levels does the learned assimilation degrade or fail? Performance is evaluated across $\sigma_{\text{obs}} \in \{0.05, 0.10, 0.50, 1.00\}$ to characterize method limitations.

\paragraph{Background Conditioning Strategy.} Is dynamic background resampling necessary for stable learning? Two regimes are compared: FixedMean (static background throughout training) and Resample (stochastic background updates each epoch).

The experimental configuration is summarized in Table~\ref{tab:hyperparams}. All experiments follow a controlled protocol with fixed random seeds, identical dataset partitions, and standardized evaluation metrics to ensure reproducibility and fair comparison.

\begin{table}[H]
\centering
\small
\begin{tabularx}{0.85\textwidth}{l X}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Trajectories (train/test) & 1000 / 500 \\
Time steps per trajectory & 200 \\
Integration step $\Delta t$ & 0.01 \\
Observation modes & $h(x)=x_1$, $h(x)=(x_1,x_2)$, $h(x)=x_1^2$ \\
Noise levels $\sigma_{\text{obs}}$ & 0.05, 0.10, 0.50, 1.00 \\
Sequence window $L$ & 5 \\
Architectures & MLP, GRU, LSTM \\
Hidden dimension & 64 (32 for Baseline MLP) \\
Batch size & 256 \\
Training epochs & 30 \\
Optimizer & Adam (learning rate $10^{-3}$) \\
\bottomrule
\end{tabularx}
\caption{Experimental configuration and hyperparameters used throughout this study.}
\label{tab:hyperparams}
\end{table}

\subsection{Contributions and Limitations}
\label{sec:contributions}

This pilot study provides first insights for future exploration of machine learning-based data assimilation by:

\paragraph{(i)} Demonstrating that self-supervised training of neural networks on the 3D-Var objective can produce analysis estimates competitive with or superior to the background forecast for the Lorenz-63 system, without requiring ground truth labels.

\paragraph{(ii)} Providing systematic benchmarks comparing feed-forward and recurrent architectures across multiple observation modes and noise levels, though the evidence for architectural superiority remains inconclusive.

\paragraph{(iii)} Identifying stochastic background resampling as important for stable training and generalization, with fixed-background approaches exhibiting high failure rates at moderate to high noise.

\paragraph{(iv)} Characterizing failure modes, including attractor escape and geometric distortion, that emerge when the learned functional fails to track chaotic dynamics.

The study has clear limitations. Results are specific to the three-dimensional Lorenz-63 system and may not transfer to higher-dimensional settings. The comparison does not include classical numerical solvers as baselines, precluding direct efficiency comparisons. Observed differences between architectures are often modest and may not achieve statistical significance with the current sample sizes. The self-supervised framework investigated here excludes potentially beneficial semi-supervised approaches that incorporate re-analysis data. These limitations motivate the outlook for future work presented in Section~\ref{sec:outlook}.

\newpage

%=============================================================================
% SECTION 2: MATHEMATICAL FORMULATION
%=============================================================================
\section{Mathematical Formulation}
\label{sec:math}

This section presents the mathematical foundations underlying both classical variational data assimilation and its neural network approximation. We introduce the variational principle, define the analysis functional, and formulate the self-supervised learning objective.

\subsection{The Variational Principle in Data Assimilation}
\label{sec:variational}

Data assimilation combines information from two sources: a prior forecast (background) and observations. In the variational framework, the analysis state $x^a$ is obtained by minimizing a cost function $J(x)$ that penalizes deviation from both sources according to their respective uncertainties.

The 3D-Var cost function is defined as
\begin{equation}
J(x) = \frac{1}{2}(x - \bar{x})^\top B^{-1}(x - \bar{x}) + \frac{1}{2}(h(x) - y)^\top R^{-1}(h(x) - y),
\label{eq:3dvar}
\end{equation}
where $\bar{x} \in \mathbb{R}^n$ is the background state, $y \in \mathbb{R}^m$ is the observation vector, $h: \mathbb{R}^n \to \mathbb{R}^m$ is the observation operator, $B \in \mathbb{R}^{n \times n}$ is the background error covariance matrix, and $R \in \mathbb{R}^{m \times m}$ is the observation error covariance matrix. The first term measures the Mahalanobis distance from the background, weighted by the inverse background covariance; the second term measures the weighted observation misfit.

The analysis $x^a$ is the minimizer of this cost:
\begin{equation}
x^a = \arg\min_x J(x).
\label{eq:analysis}
\end{equation}
When $h$ is linear (i.e., $h(x) = Hx$ for some matrix $H$), the cost function is quadratic and the minimizer admits a closed-form solution. In nonlinear cases, iterative methods such as gradient descent or quasi-Newton algorithms are required.

The covariance matrices $B$ and $R$ encode critical information about the relative reliability of background and observations. To provide available knowledge about the underlying system, these matrices must be specified or estimated: $B$ is typically derived from ensemble statistics or climatological data, while $R$ is determined by instrument characteristics. In this study, $B$ is computed empirically from an ensemble of Lorenz-63 trajectories, and $R = \sigma_{\text{obs}}^2 I$ is diagonal with variance determined by the noise level.

\subsection{The Analysis Functional}
\label{sec:functional}

We define the \emph{analysis functional} $\Phi$ as the mapping from assimilation inputs to the optimal analysis state:
\begin{equation}
\Phi: (y, \bar{x}, h, B, R) \mapsto x^a = \arg\min_x J(x).
\label{eq:functional}
\end{equation}
This functional encapsulates the solution to the variational problem for any given configuration of inputs and covariances. In classical DA, evaluating $\Phi$ requires solving the optimization problem~\eqref{eq:analysis} during each assimilation cycle---a computationally intensive operation that scales poorly with system dimension.

The AI-Var approach \cite{ai_da_fablet} proposes to learn a parametric approximation $f_\theta \approx \Phi$ using neural networks. Once trained, $f_\theta$ can evaluate the analysis in a single forward pass, potentially offering significant computational savings at inference time. The key insight is that $f_\theta$ need not solve the optimization explicitly; instead, it learns to directly map inputs to outputs that minimize the variational cost.

\subsection{Self-Supervised Learning Objective}
\label{sec:objective}

The neural network $f_\theta$ is trained without analysis labels by minimizing the 3D-Var cost evaluated at its output. Given a training set of observation-background pairs $\{(y_t, \bar{x}_t)\}_{t=1}^N$, the training loss is
\begin{equation}
\mathcal{L}_{\text{DA}}(\theta) = \frac{1}{N}\sum_{t=1}^{N} J(f_\theta(\bar{x}_t, y_{t-L+1:t})),
\label{eq:loss}
\end{equation}
where $J$ is the 3D-Var cost~\eqref{eq:3dvar} and $y_{t-L+1:t}$ denotes the observation window of length $L$ for recurrent architectures (for the MLP, only $y_t$ is used).

Expanding the loss explicitly:
\begin{equation}
\mathcal{L}_{\text{DA}}(\theta) = \frac{1}{N}\sum_{t=1}^{N} \left[
\frac{1}{2}(\hat{x}^a_t - \bar{x}_t)^\top B^{-1}(\hat{x}^a_t - \bar{x}_t)
+ \frac{1}{2}(h(\hat{x}^a_t) - y_t)^\top R^{-1}(h(\hat{x}^a_t) - y_t)
\right],
\label{eq:loss_expanded}
\end{equation}
where $\hat{x}^a_t = f_\theta(\bar{x}_t, y_{t-L+1:t})$ is the network output.

This formulation is \emph{self-supervised}: no ground truth state $x^{\text{true}}$ or pre-computed analysis $x^a$ is required during training. The network learns to balance the two terms of the cost function through gradient descent, effectively learning the structure of the variational problem from data. The true state is used only for offline evaluation metrics such as RMSE, enabling rigorous assessment of reconstruction accuracy without introducing supervised bias during training.

For numerical stability, we solve the linear systems involving $B^{-1}$ and $R^{-1}$ via Cholesky factorization rather than explicit matrix inversion. The background covariance $B$ is regularized as $B \leftarrow B + \varepsilon I$ with $\varepsilon = 10^{-6}$ to ensure positive definiteness.

\subsection{The Lorenz-63 System}
\label{sec:lorenz}

The Lorenz-63 system \cite{lorenz_1963} is defined by three coupled ordinary differential equations modeling atmospheric convection:
\begin{align}
\frac{dx_1}{dt} &= \sigma(x_2 - x_1), \\
\frac{dx_2}{dt} &= x_1(\rho - x_3) - x_2, \\
\frac{dx_3}{dt} &= x_1 x_2 - \beta x_3,
\end{align}
with standard chaotic parameters $\sigma = 10$, $\rho = 28$, and $\beta = 8/3$. The system exhibits the characteristic double-wing (butterfly) attractor shown in Figure~\ref{fig:lorenz}, with trajectories alternating between left ($x_1 < 0$) and right ($x_1 > 0$) lobes via crossings of the separatrix at $x_1 = 0$.

Trajectories are integrated using a fourth-order Runge-Kutta scheme with fixed time step $\Delta t = 0.01$. The positive Lyapunov exponent $\lambda \approx 0.9$ implies that small perturbations grow exponentially with e-folding time $\tau_\lambda \approx 1.1$ time units (approximately 110 integration steps), establishing the characteristic time scale over which forecasts diverge without observational correction.

\subsection{Observation Operators}
\label{sec:obs_operators}

Three observation operators $h: \mathbb{R}^3 \to \mathbb{R}^m$ are investigated, providing varying levels of information about the state:

\begin{table}[H]
\centering
\small
\begin{tabularx}{0.9\textwidth}{l c X c}
\toprule
\textbf{Mode} & \textbf{Definition} & \textbf{Characteristics} & \textbf{Dimension} \\
\midrule
$x$ & $h(x) = x_1$ & Partial linear; observes only first component & 1 \\
$xy$ & $h(x) = (x_1, x_2)$ & Partial linear; observes two components, provides coupling & 2 \\
$x^2$ & $h(x) = x_1^2$ & Nonlinear; removes sign information, amplifies noise & 1 \\
\bottomrule
\end{tabularx}
\caption{Observation operator configurations. The $xy$ mode provides the most state information; the $x^2$ mode is most challenging due to nonlinearity and sign ambiguity.}
\label{tab:obs_operators}
\end{table}

The operators are ordered by information content: $xy$ provides the most constraint on the state (observing two of three components), $x$ provides partial information, and $x^2$ provides the least information while introducing nonlinear ill-posedness (both $+|x_1|$ and $-|x_1|$ produce the same observation). All observations are corrupted by additive Gaussian noise: $y_t = h(x_t^{\text{true}}) + \varepsilon_t$ with $\varepsilon_t \sim \mathcal{N}(0, \sigma_{\text{obs}}^2 I)$.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{obsoperators.png}
\caption{Observation operators and sample noisy sequences for the three observation modes. The nonlinear $x^2$ mode amplifies dynamical magnitudes but removes sign information, creating a more ill-posed inverse problem.}
\label{fig:obs_operators}
\end{figure}

\newpage

%=============================================================================
% SECTION 3: METHODS AND EXPERIMENTAL SETUP
%=============================================================================
\section{Methods and Experimental Setup}
\label{sec:methods}

This section describes the experimental methodology, including data generation, neural network architectures, training procedures, and evaluation protocols. The experimental design emphasizes controlled comparisons that isolate the effects of architectural choice and background conditioning strategy.

\subsection{Data Generation}
\label{sec:data_gen}

All data are derived from the Lorenz-63 system integrated as described in Section~\ref{sec:lorenz}. A total of 1500 trajectories are generated, each containing 200 time steps after discarding an initial transient of 1000 steps to ensure the trajectory lies on the attractor. Trajectories are initialized from random points on the attractor obtained by sampling a long reference trajectory at intervals exceeding the decorrelation time.

The dataset is partitioned into 1000 training trajectories and 500 test trajectories. Observations are generated by applying the observation operator to the true state and adding Gaussian noise: $y_t = h(x_t^{\text{true}}) + \varepsilon_t$ with $\varepsilon_t \sim \mathcal{N}(0, \sigma_{\text{obs}}^2 I)$. Four noise levels are used to create artificial observations from the ground-truth trajectory: $\sigma_{\text{obs}} \in \{0.05, 0.10, 0.50, 1.00\}$.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{Dataset.png}
\caption{Dataset distribution across train and test splits for all observation modes and noise levels. Each configuration contributes approximately $2.94 \times 10^5$ effective samples, ensuring balanced evaluation across experimental conditions.}
\label{fig:dataset_dist}
\end{figure}

\subsection{Background Statistics}
\label{sec:background}

Background priors are generated from an independent climatological ensemble. Concretely, an ensemble of size $E = 10000$ is constructed by sampling states from a long Lorenz-63 trajectory at intervals exceeding the decorrelation time. The climatological mean $\bar{x}_{\text{clim}}$ and covariance $B_{\text{clim}}$ are computed from this ensemble.

Two background conditioning regimes are investigated:

\paragraph{FixedMean Regime.} The background mean $\bar{x}$ and covariance $B$ are set to the static climatological values $(\bar{x}_{\text{clim}}, B_{\text{clim}})$ and remain constant throughout training. This configuration stresses the network by forcing it to correct from a fixed, potentially biased prior.

\paragraph{Resample Regime.} At each training minibatch, $m = 32$ ensemble members are sampled (with replacement) and the batch background mean $\bar{x}$ and empirical covariance $B$ are computed from those samples. This stochastic variability acts as data-level regularization, preventing the model from overfitting to a single background realization and forcing it to learn a generalized correction functional.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{Backgroundstats.png}
\caption{Background statistics computed from a resampled ensemble: covariance matrix $B$, its eigenvalue spectrum, and the $2\sigma$ ensemble contour in $x_1$--$x_3$ space. These statistics quantify the anisotropic uncertainty structure of the Lorenz attractor.}
\label{fig:background_stats}
\end{figure}

For offline diagnostics, the true analysis $x^{a,*}$ (the actual minimizer of the 3D-Var cost) is computed via gradient-based optimization (L-BFGS). This reference is used only for evaluation and never during training, maintaining the self-supervised character of the learning framework.

\subsection{Neural Network Architectures}
\label{sec:architectures}

Four model configurations are benchmarked, with architectural details summarized in Table~\ref{tab:arch_summary}. All primary models (MLP, GRU, LSTM) share a 64-dimensional hidden representation for fair comparison; the Baseline MLP uses a smaller 32-dimensional hidden layer as a minimal diagnostic control.

\begin{table}[H]
\centering
\small
\begin{tabularx}{0.95\textwidth}{l X c c}
\toprule
\textbf{Model} & \textbf{Description} & \textbf{Activation} & \textbf{Hidden} \\
\midrule
Baseline MLP & Two linear layers; maps $y_t \mapsto \hat{x}^a_t$; no background input & tanh & 32 \\
MLP & Three dense layers; input $[\bar{x}_t, y_t]$; predicts correction $\Delta x$ & ReLU & 64 \\
GRU & Single-layer GRU over $y_{t-L+1:t}$; concat $[\bar{x}_t, h_t]$ + MLP head & ReLU & 64 \\
LSTM & Single-layer LSTM over $y_{t-L+1:t}$; concat $[\bar{x}_t, h_t]$ + MLP head & ReLU & 64 \\
\bottomrule
\end{tabularx}
\caption{Architecture configurations. The Baseline MLP excludes background information entirely and serves as a diagnostic control. Primary architectures condition on both observations and background.}
\label{tab:arch_summary}
\end{table}

\paragraph{Baseline MLP (No-Mean).} This minimal configuration excludes all background information ($\bar{x}$, $B$), learning a direct mapping from observations to analysis: $\hat{x}^a = f_\theta(y_t)$. Its purpose is to quantify the benefit of incorporating prior information, isolating assimilation behavior that emerges solely from observations.

\paragraph{MLP.} The primary MLP concatenates the background mean $\bar{x}_t$ and current observation $y_t$, predicting an additive correction: $\hat{x}^a_t = \bar{x}_t + f_\theta([\bar{x}_t, y_t])$. This architecture serves as a lightweight reference that can approximate the 3D-Var update in a single forward pass without temporal context.

\paragraph{GRU and LSTM.} Recurrent architectures process an observation sequence $y_{t-L+1:t}$ (with $L = 5$) through a single-layer encoder, producing a hidden state $h_t$ that aggregates temporal information. This hidden state is concatenated with the background mean $\bar{x}_t$ and passed through a feed-forward head to predict the analysis increment: $\hat{x}^a_t = \bar{x}_t + g_\theta([\bar{x}_t, h_t])$. The GRU uses gated recurrent units; the LSTM additionally maintains a cell state for potentially longer-range dependencies.

All inputs are standardized (zero mean, unit variance) using training-split statistics. Outputs are denormalized to the original state space before computing evaluation metrics.

\subsection{Training Procedure}
\label{sec:training}

All models are trained using the Adam optimizer with learning rate $10^{-3}$ for exactly 30 epochs. No learning rate scheduler or early stopping is applied; final-epoch weights are saved for each configuration. Training minimizes the self-supervised loss~\eqref{eq:loss_expanded}, with batch size 256.

Each model-regime pair is trained independently for every combination of observation mode ($x$, $xy$, $x^2$) and noise level ($\sigma_{\text{obs}} \in \{0.05, 0.10, 0.50, 1.00\}$), yielding $3 \times 4 \times 4 = 48$ trained models per regime. Model weights, configuration files, RNG seeds, and Git commit hashes are archived for reproducibility.

\subsection{Evaluation Metrics}
\label{sec:metrics}

The primary quantitative metric is the root mean square error (RMSE) between the network output and true state:
\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{N}\sum_{i=1}^{N} \|\hat{x}^a_i - x^{\text{true}}_i\|^2},
\label{eq:rmse}
\end{equation}
computed over all three state components and averaged across test trajectories ($K = 500$ trajectories $\times$ $S = 3$ noise realizations per trajectory $= 1500$ samples).

Two improvement metrics contextualize RMSE gains:

\paragraph{Improvement relative to background (primary).}
\begin{equation}
\text{Improvement}_{\text{bg}}(\%) = 100 \cdot \frac{\text{RMSE}_b - \text{RMSE}_a}{\text{RMSE}_b + \varepsilon}, \quad \varepsilon = 10^{-12},
\label{eq:improvement}
\end{equation}
where $\text{RMSE}_b$ is the background error (before assimilation) and $\text{RMSE}_a$ is the analysis error (after assimilation). High values indicate substantial error reduction; note that 90\% improvement corresponds to $\text{RMSE}_a$ being one-tenth of $\text{RMSE}_b$.

\paragraph{Improvement relative to FixedMean (secondary).} When comparing regimes:
\begin{equation}
\text{Improvement}_{\text{FM}}(\%) = 100 \cdot \left(1 - \frac{\text{RMSE}_{\text{model}}}{\text{RMSE}_{\text{FixedMean}}}\right).
\end{equation}

For geometric fidelity assessment, we employ the \emph{normalized Hausdorff distance}:
\begin{equation}
\tilde{H}_{\text{global}} = \frac{H(\mathcal{A}_{\text{analysis}}, \mathcal{A}_{\text{truth}})}{\text{diam}(\mathcal{A}_{\text{truth, global}})},
\end{equation}
where $H$ is the symmetric Hausdorff distance and the denominator is a fixed reference diameter computed from long truth trajectories. This metric quantifies geometric alignment between analysis and truth manifolds.

Additionally, \emph{lobe occupancy discrepancy} measures global topology preservation:
\begin{equation}
\Delta_{\text{lobe}} = \left| \frac{n_{\text{left}}^{\text{analysis}}}{N} - \frac{n_{\text{left}}^{\text{truth}}}{N} \right|,
\end{equation}
where $n_{\text{left}}$ counts time steps in the left lobe ($x_1 < 0$). Low $\Delta_{\text{lobe}}$ indicates correct separatrix crossing behavior and balanced wing visitation.

We report mean $\pm$ standard deviation for normally distributed quantities and median with interquartile range (IQR) for quantities affected by outliers (e.g., when attractor escape occurs). A run is classified as diverged if post-assimilation RMSE exceeds $10\times$ the median for that configuration or if $\tilde{H}_{\text{global}} > 10$.

\subsection{Reproducibility}
\label{sec:reproducibility}

All experiments were executed from the code repository referenced on the title page. Each run archives the full configuration, RNG seed, and Git commit hash. Python and PyTorch versions are recorded in the repository's \texttt{environment.yml}. A utility \texttt{set\_seed(seed)} initializes Python, NumPy, and PyTorch RNGs; deterministic CUDA flags are set where available. Minimal reproduction of main figures is possible from stored metrics files without retraining.

\newpage

%=============================================================================
% SECTION 4: RESULTS
%=============================================================================
\section{Experiments and Results}
\label{sec:results}

This section presents experimental findings organized by increasing specificity: convergence behavior, aggregate accuracy, observation mode sensitivity, temporal coherence, geometric fidelity, and ablation studies. Throughout, we emphasize both successful outcomes and limitations, acknowledging where results remain inconclusive.

\subsection{Convergence and Training Dynamics}
\label{sec:convergence}

The convergence behavior across training regimes reveals substantial differences in stability and learning efficiency. Figure~\ref{fig:convergence_envelopes} shows mean loss trajectories across all configurations.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{4_2k_mean_convergence_envelopes.png}
\caption{Mean convergence envelopes across training regimes. The Resample regime (blue) achieves the fastest and most stable convergence, reducing loss to approximately 20\% of the initial value within the first epoch. FixedMean (orange) shows rapid initial descent but exhibits instability at high noise levels during evaluation. Baseline (green) converges slowly, retaining approximately 90\% of initial error after 30 epochs. Shaded regions indicate standard deviation across observation modes and noise levels.}
\label{fig:convergence_envelopes}
\end{figure}

The Baseline regime (no background information) exhibits steady but slow convergence, with validation losses consistently higher than training losses, indicating limited generalization capacity when prior information is absent. The FixedMean regime shows faster initial convergence due to the fixed prior anchor, achieving lower training loss within the first few epochs. However, this apparent stability is deceptive: at moderate to high noise levels ($\sigma_{\text{obs}} \geq 0.5$), FixedMean frequently produces catastrophic failures during evaluation, including attractor escape and RMSE values exceeding $10\times$ the baseline.

The Resample regime demonstrates both rapid and stable convergence. By introducing stochastic variability in the background mean and covariance at each epoch, the model learns a more generalizable correction functional that remains stable across noise levels. Validation losses track training losses closely, and the narrow variance envelope indicates reproducible optimization dynamics.

\paragraph{Divergence Rates.} At $\sigma_{\text{obs}} \geq 0.5$, observed divergence rates are approximately 70--80\% for FixedMean and only 20--25\% for Resample. When summarizing results, we report mean $\pm$ standard deviation over non-diverged runs and state divergence rates explicitly.

\subsection{Resample Regime: Accuracy and Stability}
\label{sec:resample_accuracy}

Having established that the Resample regime provides the most stable training dynamics, we evaluate its post-assimilation performance in detail.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_3a_resample_rmse_distributions.png}
\caption{Post-assimilation RMSE distributions for MLP, GRU, and LSTM across noise levels in the Resample regime ($xy$ mode). Box plots show median, interquartile range, and outliers. All models maintain stable accuracy for $\sigma_{\text{obs}} \leq 0.1$ (median RMSE $\approx 4$--5), with dispersion increasing at higher noise.}
\label{fig:resample_rmse}
\end{figure}

Figure~\ref{fig:resample_rmse} illustrates RMSE distributions for the three primary architectures. At low noise ($\sigma_{\text{obs}} \leq 0.1$), all models achieve median RMSE values of approximately 4--5, with narrow interquartile ranges indicating consistent performance. As noise increases, RMSE dispersion widens, particularly for LSTM, whose quartile range nearly doubles at $\sigma_{\text{obs}} = 1.0$.

Table~\ref{tab:resample_stats} provides detailed statistics. The GRU exhibits tighter distributions and fewer outliers than LSTM, suggesting more stable behavior under data perturbation. The MLP achieves the lowest mean RMSE in some configurations, though differences between architectures are modest and often within uncertainty bounds. These results do not conclusively establish architectural superiority; rather, they suggest that all three approaches are viable for this low-dimensional problem.

\begin{table}[H]
\centering
\small
\caption{Cross-model RMSE statistics under Resample regime (mean $\pm$ std across runs, excluding diverged cases).}
\label{tab:resample_stats}
\begin{tabularx}{0.95\textwidth}{l l c c c c}
\toprule
\textbf{Mode} & \textbf{Model} & \textbf{RMSE$_a$ Mean} & \textbf{RMSE$_a$ Std} & \textbf{$\Delta$RMSE \%} & \textbf{Std} \\
\midrule
$x$   & GRU  & 7.054 & 0.496 & $-0.079$ & 0.169 \\
      & LSTM & 7.011 & 0.332 & $-0.151$ & 0.207 \\
      & MLP  & 6.966 & 0.267 & $+0.022$ & 0.439 \\
$x^2$ & GRU  & 10.834 & 0.566 & $-0.669$ & 0.552 \\
      & LSTM & 11.060 & 0.660 & $-0.558$ & 0.406 \\
      & MLP  & 13.307 & 1.154 & $-0.923$ & 0.574 \\
$xy$  & GRU  & 7.788 & 1.150 & $-0.660$ & 0.960 \\
      & LSTM & 7.925 & 1.670 & $-0.707$ & 0.888 \\
      & MLP  & 7.952 & 0.665 & $-0.454$ & 0.346 \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Observation Mode Sensitivity}
\label{sec:obs_sensitivity}

The structure of the observation operator substantially affects reconstruction accuracy. Figure~\ref{fig:obs_mode_rmse} reveals a clear hierarchy:
\[
\text{RMSE}(xy) < \text{RMSE}(x) < \text{RMSE}(x^2).
\]

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_4a_post_assimilation_rmse.png}
\caption{Post-assimilation RMSE distributions for all observation modes as a function of noise level $\sigma_{\text{obs}}$, aggregated across architectures. The $xy$ mode achieves the lowest and most stable errors (median RMSE $\approx 4$), followed by $x$ ($\approx 4$--5), while $x^2$ exhibits markedly higher dispersion and error magnitudes ($\approx 5$--10) due to nonlinear noise amplification.}
\label{fig:obs_mode_rmse}
\end{figure}

The $xy$ mode provides dual observations that strongly constrain the state, yielding median RMSE near 4 with narrow quartiles even at $\sigma_{\text{obs}} = 1.0$. The $x$ mode, observing only a single component, achieves slightly higher but still stable errors. The $x^2$ mode exhibits significantly larger dispersion and RMSE values rising from approximately 5 to 10 as noise increases.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{4_4b_delta_rmse_improvement.png}
\caption{Assimilation improvement ($\Delta$RMSE = RMSE$_b$ $-$ RMSE$_a$) across observation modes as a function of noise. At low noise, all modes achieve substantial gains. As noise increases, $x$ and $xy$ retain positive gains, while $x^2$ approaches zero improvement at $\sigma_{\text{obs}} \geq 0.5$.}
\label{fig:obs_mode_delta}
\end{figure}

The pattern confirms that information coupling outweighs observational complexity: coupled linear observations ($xy$) provide redundancy that resolves ambiguities and filters noise more effectively than increased measurement sophistication. The nonlinear $x^2$ mode suffers from multiplicative error amplification: additive observation noise translates into multiplicative state uncertainty through the squared transformation.

\subsection{Temporal Dynamics and Attractor Geometry}
\label{sec:temporal}

Effective data assimilation in chaotic systems requires temporal coherence and fidelity to the underlying attractor manifold. Figure~\ref{fig:error_evolution} shows temporal error evolution across regimes.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_5b_error_evolution_profiles.png}
\caption{Temporal evolution of Euclidean error over the assimilation window for FixedMean and Resample regimes at different noise levels ($xy$ mode, GRU architecture). Resample (solid lines) demonstrates fast convergence ($\tau \approx 50$ time steps) to a low, stable error plateau across all noise levels. FixedMean (dashed lines), particularly at $\sigma_{\text{obs}} = 0.5$, exhibits large oscillations and sustained high error.}
\label{fig:error_evolution}
\end{figure}

The Resample regime achieves rapid error reduction within approximately 50 time steps (roughly half a Lyapunov time), stabilizing at a low steady-state level. The error trajectory remains smooth and bounded, indicating that the learned correction functional successfully tracks the chaotic manifold. In contrast, FixedMean exhibits erratic behavior at moderate to high noise, with error oscillations growing in amplitude and trajectories frequently diverging from the true attractor.

\paragraph{Attractor Geometry.} Global-normalized Hausdorff distances quantify geometric fidelity across regimes.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{attractor_metrics_resample_fixedmean.png}
\caption{Global-normalized Hausdorff distances ($\tilde{H}_{\text{global}}$) for Resample and FixedMean regimes at four noise levels ($xy$ mode). Each point represents median $\tilde{H}_{\text{global}}$ across test trajectories; error bars indicate IQR. Resample consistently achieves low geometric deviation ($\tilde{H}_{\text{global}} \approx 0.32$) with tight dispersion. FixedMean exhibits substantially higher deviation ($\tilde{H}_{\text{global}} \approx 1.50$) and increased variance at $\sigma_{\text{obs}} \geq 0.50$.}
\label{fig:global_hausdorff}
\end{figure}

Resample maintains $\tilde{H}_{\text{global}} \approx 0.32$ across all noise levels---approximately $4.7\times$ lower than FixedMean. The flat response to increasing noise confirms that dynamic background updates provide a noise-tolerant geometric anchor.

\paragraph{Lobe Occupancy.} The lobe occupancy discrepancy $\Delta_{\text{lobe}}$ validates global topology preservation.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{lobe_occupancy_diff_heatmap.png}
\caption{Lobe occupancy discrepancy ($\Delta_{\text{lobe}}$) for all regimes and observation modes across noise levels. Light cells indicate near-perfect lobe matching ($\Delta_{\text{lobe}} \approx 0$); dark cells indicate substantial imbalance. Resample achieves uniformly low discrepancies across modes, indicating correct separatrix crossings. FixedMean shows moderate bias ($\Delta_{\text{lobe}} \approx 0.10$--0.15) at higher noise.}
\label{fig:lobe_discrepancy}
\end{figure}

Resample achieves near-zero lobe occupancy discrepancy ($\Delta_{\text{lobe}} < 0.02$) across all observation modes and noise levels, confirming that dynamic background updates enable correct separatrix crossing and balanced wing visitation. FixedMean exhibits substantial lobe imbalances at higher noise, with approximately 15\% of trajectories incorrectly accumulating in one lobe at $\sigma_{\text{obs}} = 0.50$ under the $x$ mode.

\begin{table}[H]
\centering
\small
\caption{Resample regime, $xy$ mode: accuracy and geometry metrics versus noise (means across runs).}
\label{tab:xy_resample}
\begin{tabular}{c ccc ccc}
\toprule
& \multicolumn{3}{c}{RMSE $\downarrow$} & \multicolumn{3}{c}{Normalized Hausdorff $\downarrow$} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
$\sigma_{\text{obs}}$ & MLP & GRU & LSTM & MLP & GRU & LSTM \\
\midrule
0.05 & 4.073 & 3.448 & 3.630 & 0.320 & 0.316 & 0.320 \\
0.10 & 3.925 & 3.883 & 4.018 & 0.323 & 0.320 & 0.322 \\
0.50 & 3.958 & 3.586 & 3.994 & 0.319 & 0.321 & 0.320 \\
1.00 & 4.000 & 4.552 & 4.078 & 0.321 & 0.321 & 0.322 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Studies}
\label{sec:ablations}

Targeted ablation studies investigate sensitivity to key design parameters.

\paragraph{Background Sampling Strategy.} Figure~\ref{fig:background_stability} quantifies the stability advantage of stochastic resampling.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{4_6a_background_sampling_stability.png}
\caption{Impact of background sampling strategy on RMSE stability across noise levels. Resample (solid lines) demonstrates graceful degradation, with RMSE increasing smoothly from $\approx 4$ to $\approx 7$. FixedMean (dashed lines) exhibits catastrophic instability beyond $\sigma_{\text{obs}} > 0.5$, with RMSE spikes and divergence rates reaching 70--80\%.}
\label{fig:background_stability}
\end{figure}

The Resample strategy introduces stochastic variability that acts as regularization, resulting in stable RMSE across all noise levels with graceful degradation even at $\sigma_{\text{obs}} = 1.0$. FixedMean becomes fundamentally unreliable beyond moderate noise, with high divergence rates and attractor escape.

\paragraph{Background Covariance Sensitivity.} The system's sensitivity to misspecification of $B$ was tested by scaling the true covariance by a factor $\lambda$. Results (see Appendix Figure~\ref{fig:app_B_scaling}) show a characteristic U-shaped error curve with minimum at $\lambda = 1.0$, confirming that the network learns the structure of the variational problem. Resample demonstrates substantially higher tolerance to $B$ misestimation than FixedMean.

\paragraph{Summary of Practical Recommendations.} Table~\ref{tab:recommendations} consolidates findings into actionable guidelines.

\begin{table}[H]
\centering
\caption{Practical recommendations for AI-based data assimilation in chaotic systems.}
\label{tab:recommendations}
\begin{tabularx}{\textwidth}{|l|l|X|}
\hline
\textbf{Parameter} & \textbf{Recommendation} & \textbf{Rationale} \\
\hline
Background Regime & Resample & Prevents attractor escape; tolerates noise and $B$ misestimation \\
\hline
Observation Mode & Coupled linear ($xy$) & Provides redundancy; resolves ambiguities \\
\hline
Architecture & GRU or LSTM & Temporal context may help; results inconclusive \\
\hline
Sequence Length & 10--15 steps & Captures $\sim$1 Lyapunov time; diminishing returns beyond \\
\hline
$B$ Accuracy & $\pm 10\%$ of true scale & Critical for variational minimization \\
\hline
\end{tabularx}
\end{table}

\newpage

%=============================================================================
% SECTION 5: DISCUSSION
%=============================================================================
\section{Discussion}
\label{sec:discussion}

This study investigated neural network architectures for data assimilation in the Lorenz-63 system, focusing on how temporal modeling and ensemble-based strategies affect reconstruction of chaotic trajectories. The findings provide preliminary evidence supporting several hypotheses while leaving key questions unresolved.

\subsection{Architectural Comparisons: Inconclusive Evidence}

The comparison between feed-forward and recurrent architectures yielded mixed results. Under the Resample regime, GRU and LSTM models generally achieved lower RMSE than the MLP, particularly in the $xy$ observation mode where temporal coupling between state components provides exploitable structure. However, the differences were often modest---typically 5--15\% in relative RMSE---and varied with noise level and observation mode. In some configurations, the MLP matched or exceeded recurrent performance, particularly for the $x$ mode at low noise.

These results do not conclusively establish that temporal context is necessary or sufficient for improved assimilation in this setting. The Lorenz-63 system's low dimensionality may limit the benefit of sequence modeling: with only three state variables and dense observations, the instantaneous information content may be sufficient for accurate reconstruction. The hypothesis that recurrent architectures provide meaningful advantages may require testing in higher-dimensional systems where temporal correlations carry more information.

\subsection{The Importance of Stochastic Regularization}

The clearest finding concerns background conditioning strategy. The Resample regime substantially outperformed FixedMean across nearly all metrics: lower mean RMSE, narrower variance, lower divergence rates, and better geometric fidelity. The improvement was most pronounced at moderate to high noise levels ($\sigma_{\text{obs}} \geq 0.5$), where FixedMean exhibited 70--80\% failure rates compared to 20--25\% for Resample.

This pattern suggests that stochastic background resampling provides a form of implicit regularization that prevents the network from overfitting to a single prior realization. By exposing the model to diverse background configurations during training, Resample forces the learned functional to generalize across the distribution of possible priors, rather than memorizing corrections specific to a fixed background. This interpretation aligns with general principles of dropout and data augmentation in deep learning, applied here at the level of the data assimilation inputs.

\subsection{Failure Modes and Their Implications}

Two primary failure modes were observed:

\paragraph{Attractor Escape.} Under FixedMean at high noise, analysis trajectories frequently departed from the chaotic attractor, drifting toward fixed points or exhibiting non-chaotic dynamics. This failure manifests as dramatically elevated RMSE (often $>100$) and large Hausdorff distances. The static background mean appears insufficient to anchor the learned functional when observation noise overwhelms the signal; without stochastic updates, the network cannot adapt to diverse realizations of chaotic dynamics.

\paragraph{Lobe Imbalance.} Even when trajectories remained bounded, FixedMean often produced lobe occupancy discrepancies indicating preferential accumulation in one wing of the attractor. This suggests that the learned functional fails to correctly model separatrix crossings, producing trajectories that qualitatively differ from truth despite maintaining finite RMSE.

Both failure modes underscore the importance of geometric fidelity metrics beyond aggregate RMSE. A model achieving moderate RMSE may nonetheless fail to reproduce essential dynamical features of the chaotic system.

\subsection{Limitations and Caveats}

Several limitations temper the conclusions of this study.

\paragraph{Low Dimensionality.} The Lorenz-63 system has only three state variables, far below the $10^6$--$10^9$ dimensions of operational weather and ocean models. Architectural choices that show no advantage in this toy setting might prove critical in high-dimensional applications where sparsity, localization, and temporal structure become essential.

\paragraph{Lack of Classical Baselines.} This study does not compare against optimized classical solvers (e.g., iterative 3D-Var minimization). Consequently, we cannot assess whether the learned approach offers computational advantages or accuracy improvements over existing methods.

\paragraph{Single System, Single Loss.} All experiments used the same dynamical system and loss function. Generalization to other chaotic systems, observation models, or loss formulations remains untested.

\paragraph{Statistical Power.} With single training runs per configuration and modest test set sizes, some observed differences may not achieve statistical significance. Future work should incorporate multiple random seeds and formal hypothesis testing.

\newpage

%=============================================================================
% SECTION 6: OUTLOOK
%=============================================================================
\section{Outlook}
\label{sec:outlook}

This pilot study opens several directions for future investigation.

\paragraph{Extension to Higher-Dimensional Systems.} The Lorenz-96 model \cite{lorenz_1963} provides a natural next testbed, with configurable dimension (typically 40 state variables) and tunable chaoticity. This setting would reveal whether architectural advantages emerge at higher dimension and test scalability of the training procedure. Extension to simplified atmospheric or oceanic models would further assess operational relevance.

\paragraph{Hybrid Approaches.} Integrating neural network components within classical data assimilation frameworks offers a promising research direction. For example, learned preconditioners could accelerate variational minimization, or neural networks could estimate flow-dependent background error covariances within an EnKF framework. Such hybrid approaches might combine the physical consistency of classical methods with the adaptability of learned components.

\paragraph{Probabilistic Extensions.} The current framework produces point estimates of the analysis state. Probabilistic extensions using variational autoencoders or normalizing flows could generate ensemble forecasts with calibrated uncertainty, enabling direct comparison with EnKF and smoother approaches. Such methods would also facilitate uncertainty quantification for downstream applications.

\paragraph{Semi-Supervised Learning.} While this study focused exclusively on self-supervised training, incorporating available re-analysis data in a semi-supervised framework might improve accuracy without requiring fully labeled datasets. The tradeoff between self-supervised and supervised approaches merits systematic investigation.

\paragraph{Observational Strategies.} The substantial performance differences across observation modes suggest that optimal sensor placement and observation network design could significantly impact learned assimilation quality. Adaptive observation strategies that account for the learned functional's sensitivity might improve operational performance.

\newpage

%=============================================================================
% REFERENCES
%=============================================================================
\addcontentsline{toc}{section}{References}
\bibliographystyle{plainnat}
\begin{thebibliography}{99}
\bibitem{lorenz_1963} Lorenz, E.\ N.\ (1963). Deterministic nonperiodic flow. \textit{Journal of the Atmospheric Sciences}, 20(2), 130--141.
\bibitem{kalman_1960} Kalman, R.\ E.\ (1960). A new approach to linear filtering and prediction problems. \textit{Journal of Basic Engineering}, 82(1), 35--45.
\bibitem{ai_da_fablet} Fablet, R., Ouala, S., \& Herzet, C.\ (2021). Learning variational data assimilation models and solvers. \textit{Journal of Advances in Modeling Earth Systems}, 13(3), e2020MS002256.
\bibitem{ensemble_methods} Evensen, G.\ (1994). Sequential data assimilation with a nonlinear quasi-geostrophic model using Monte Carlo methods to forecast error statistics. \textit{Journal of Geophysical Research}, 99(C5), 10143--10162.
\bibitem{neural_da_bocquet} Bocquet, M., Farchi, A., \& Malartic, Q.\ (2023). Neural incremental data assimilation. arXiv preprint arXiv:2406.15076.
\end{thebibliography}

\newpage
\appendix

\addcontentsline{toc}{section}{Appendix}

\subsection*{A.1 Data and Pipeline Extras}
\addcontentsline{toc}{subsection}{A.1 Data and Pipeline Extras}
\label{sec:app_data}

This appendix section provides detailed visualizations and data structures referenced in Section~\ref{sec:methods}.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{obsoperators.png}
\caption{(Appendix) Observation operators and sample noisy sequences for the three observation modes. The nonlinear $x^2$ mode amplifies dynamical magnitudes but removes sign information, illustrating a more ill-posed mapping problem.}
\label{fig:app_obs_operators}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{Backgroundstats.png}
\caption{(Appendix) Background statistics: covariance matrix $B$, its eigenvalue spectrum, and the $2\sigma$ ensemble contour in $X$--$Z$ space. These statistics quantify the uncertainty anisotropy of the Lorenz attractor.}
\label{fig:app_background_stats}
\end{figure}
\newpage
\noindent
The data directory structure used in all experiments:

\begin{minted}[fontsize=\footnotesize, bgcolor=gray!5, frame=single]{text}
src/
├── data/
│   ├── generation.py
│   ├── dataset.py
│   ├── raw/
│   │   ├── train_traj.npy
│   │   ├── test_traj.npy
│   │   ├── B.npy
│   ├── obs/
│   │   ├── obs_x_n0.05.npy
│   │   ├── obs_xy_n*.npy
│   ├── splits/
│   │   ├── train_indices.npy
│   │   ├── val_indices.npy
│   │   ├── test_indices.npy
\end{minted}

\noindent
The results directory structure:

\begin{minted}[fontsize=\footnotesize, bgcolor=gray!5, frame=single]{text}
results/
├── baseline/
│   ├── baseline_no_mean_20251025_212806/
│   │   ├── figures/   # Loss curves, reconstructions, diagnostics
│   │   └── metrics/   # RMSE tables, JSON histories
├── fixedmean/
│   ├── run_20251008_133752/
│   │   ├── figures/
│   │   ├── metrics/
│   │   └── models/    # Saved .pth model weights
├── resample/
│   ├── run_20251008_134240/
│   │   ├── figures/
│   │   ├── metrics/
│   │   └── models/
\end{minted}

\newpage

\subsection*{A.2 Training Dynamics}
\addcontentsline{toc}{subsection}{A.2 Training Dynamics}
\label{sec:app_training}

This section contains detailed loss curves and convergence diagnostics referenced in Section~\ref{sec:convergence}.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{4_2a_baseline_loss.png}
\caption{(Appendix) Baseline training and validation loss across all observation modes and noise levels. Multi-panel grid shows smooth, monotonic convergence with minor variance across runs.}
\label{fig:app_baseline_loss}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{4_2b_mean_convergence.png}
\caption{(Appendix) Baseline mean convergence averaged across all settings. The relatively small variance band indicates consistent learning behavior across random seeds.}
\label{fig:app_baseline_mean}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{4_2c_baseline_rmse.png}
\caption{(Appendix) Baseline RMSE vs. noise level $\sigma$. Performance degrades gradually with higher noise, confirming that model capacity is constrained by observation quality.}
\label{fig:app_baseline_rmse}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\linewidth]{fixedmean_loss_grid_pub.png}
\caption{(Appendix) FixedMean training and validation loss grid. Despite smooth loss traces in some settings, FixedMean frequently becomes unstable at moderate/high noise ($\sigma \geq 0.5$), with post-assimilation RMSE explosions and attractor escape on unseen trajectories.}
\label{fig:app_fixedmean_loss}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{4_2e_mean_convergence_fixedmean.png}
\caption{(Appendix) FixedMean mean convergence across all settings. The narrow variance envelope indicates reproducible convergence during training, though this does not translate to stable test performance at high noise.}
\label{fig:app_fixedmean_mean}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{fixedmean_rmse_multipanel_pub.png}
\caption{(Appendix) FixedMean RMSE vs. noise level per architecture. All three models show graceful degradation during training, but GRU and LSTM maintain smoother, more consistent responses than MLP.}
\label{fig:app_fixedmean_rmse}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{fixedmean_rmse_before_after_multipanel_pub.png}
\caption{(Appendix) FixedMean RMSE before and after assimilation per architecture. Dashed lines denote pre-assimilation errors, solid lines show post-assimilation results. Recurrent architectures (GRU, LSTM) better internalize assimilation correction dynamics.}
\label{fig:app_fixedmean_before_after}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{fixedmean_delta_rmse_pub.png}
\caption{(Appendix) FixedMean $\Delta$RMSE (before $-$ after assimilation). Positive bars indicate improvement. All modes show consistent RMSE gains, with strongest improvements in nonlinear $x^2$ and mixed $xy$ settings.}
\label{fig:app_fixedmean_delta}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_2i_relative_rmse_fixedmean_vs_baseline.png}
\caption{(Appendix) Relative post-assimilation RMSE (FixedMean vs. Baseline). FixedMean achieves 40--60\% improvement over Baseline in controlled conditions, with largest gains for single-channel observations.}
\label{fig:app_relative_rmse}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{4_2j_delta_rmse_improvement.png}
\caption{(Appendix) $\Delta$RMSE improvement comparison between regimes. FixedMean achieves larger effective reduction than Baseline, especially at higher noise scenarios ($\sigma = 0.5$--1.0).}
\label{fig:app_delta_improvement}
\end{figure}

\newpage

\subsection*{A.3 Extended Attractor Geometry Diagnostics}
\addcontentsline{toc}{subsection}{A.3 Extended Attractor Geometry Diagnostics}
\label{sec:app_attractor}

This appendix provides comprehensive visual diagnostics for attractor geometry preservation, complementing the quantitative metrics presented in Section~\ref{sec:temporal}.

\subsubsection*{A.3.1 Phase-Space Projections and Hausdorff Metrics}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{fig_attractor_geometry_side_by_side.png}
\caption{(Appendix) Phase--space projections (XY, YZ) for three assimilation regimes (Resample, FixedMean, Baseline) at two noise levels ($\sigma=0.1,0.5$). Each panel overlays truth (gray) and analysis (blue); inset text gives normalized Hausdorff distance $\tilde{H}$ where $\tilde{H}=H/\text{diam}(\mathcal{A}_{\text{truth}})$ and $H$ is the symmetric Hausdorff between truth and analysis trajectories (after downsampling to $\leq 6000$ points). Resample maintains close geometric adherence ($\tilde{H}\approx0.32$) across noise, Baseline drifts moderately ($\tilde{H}\approx1.08$), and FixedMean exhibits substantial structural distortion ($\tilde{H}\approx1.48$--1.64).}
\label{fig:app_attractor_geometry_panels}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.92\textwidth]{fig_attractor_geometry_metrics.png}
\caption{(Appendix) (a) Distribution of normalized Hausdorff distances $\tilde{H}$ across noise levels ($\sigma\in\{0.05,0.1,0.5,1.0\}$) for each regime. (b) Scaling of $\tilde{H}$ with noise $\sigma$ (log scale). Medians (IQR): Resample $0.32$ (0.32--0.32), Baseline $1.08$ (1.08--1.09), FixedMean $1.50$ (1.42--1.55). Lower $\tilde{H}$ indicates better geometric adherence.}
\label{fig:app_attractor_geometry_metrics}
\end{figure}

\subsubsection*{A.3.2 Architecture-Specific Regime Comparison}

\begin{figure}[H]
\centering
\includegraphics[width=0.92\textwidth]{attractor_metrics_resample_fixedmean.png}
\caption{(Appendix) Detailed comparison of global-normalized Hausdorff distances for Resample versus FixedMean regimes across architectures (MLP, GRU, LSTM) and noise levels. Left panel shows distributions at $\sigma=0.10$; right panel shows $\sigma=0.50$. Resample (blue) consistently outperforms FixedMean (red) across all architectures, with GRU and LSTM showing slight advantages in noise robustness. At higher noise ($\sigma=0.50$), FixedMean distributions broaden significantly, indicating geometric instability, while Resample remains tightly concentrated near $\tilde{H}_{\text{global}} \approx 0.32$. This confirms that architectural choice is secondary to regime selection: dynamic background updates are the primary driver of geometric fidelity.}
\label{fig:app_attractor_metrics_detailed}
\end{figure}
\subsubsection*{A.3.3 Multi-Regime Attractor Grids}

% =====================================================
% BASELINE
% =====================================================

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth, keepaspectratio]{ATTRACTOR_GRID_Baseline_x.png}
\includegraphics[width=\textwidth, keepaspectratio]{ATTRACTOR_GRID_Baseline_xy.png}
\includegraphics[width=\textwidth, keepaspectratio]{ATTRACTOR_GRID_Baseline_x2.png}
\caption{(Appendix) Attractor geometry for the Baseline regime across three observation modes ($x$, $xy$, and $x^2$) and four noise levels ($\sigma \in \{0.05, 0.10, 0.50, 1.00\}$). Each grid contains XY and YZ projections with truth (gray) and analysis (blue) trajectories. Baseline maintains partial geometric fidelity at low noise but exhibits increasing lobe imbalance and drift at higher noise, particularly for the $x^2$ mode where nonlinear observations reduce separatrix information.}
\label{fig:app_attractor_grid_baseline}
\end{figure}

% =====================================================
% FIXEDMEAN
% =====================================================

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth, keepaspectratio]{ATTRACTOR_GRID_FixedMean_x.png}
\includegraphics[width=\textwidth, keepaspectratio]{ATTRACTOR_GRID_FixedMean_xy.png}
\end{figure}
\begin{figure}[H]
\includegraphics[width=\textwidth, keepaspectratio]{ATTRACTOR_GRID_FixedMean_x2.png}
\caption{(Appendix) Attractor geometry for the FixedMean regime across observation modes. FixedMean displays substantial geometric degradation at moderate and high noise levels. Analysis trajectories (blue) deviate from truth (gray), often collapsing toward fixed points or producing fragmented, non-chaotic shapes. These distortions are consistent with the elevated lobe imbalance values observed in Fig.~\ref{fig:lobe_discrepancy}.}
\label{fig:app_attractor_grid_fixedmean}
\end{figure}


% =====================================================
% RESAMPLE
% =====================================================

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth, keepaspectratio]{ATTRACTOR_GRID_Resample_x.png}
\includegraphics[width=\textwidth, keepaspectratio]{ATTRACTOR_GRID_Resample_xy.png}
\end{figure}
\begin{figure}[H]
\includegraphics[width=\textwidth, keepaspectratio]{ATTRACTOR_GRID_Resample_x2.png}
\caption{(Appendix) Attractor geometry for the Resample regime across observation modes. Resample shows excellent geometric fidelity: analysis trajectories (blue) closely follow truth (gray) across all noise levels. Balanced wing visitation and correct separatrix crossings confirm global topology preservation, even under the challenging $x^2$ mode and $\sigma=1.00$.}
\label{fig:app_attractor_grid_resample}
\end{figure}

% =====================================================
% SUMMARY PARAGRAPH
% =====================================================

The composite grids provide compelling visual evidence supporting the quantitative metrics in
Section~\ref{sec:temporal}. Resample consistently achieves the closest alignment between truth and analysis, with balanced wing visitation and stable geometric structure across all conditions. FixedMean produces the weakest performance, with fragmented attractors, lobe bias, and frequent geometric collapse at higher noise. Baseline lies between the two extremes, maintaining reasonable local alignment at low noise but failing to preserve full double-lobe coverage as noise increases.

\newpage

\subsection*{A.4 Extended Resample Metrics}
\addcontentsline{toc}{subsection}{A.4 Extended Resample Metrics}
\label{sec:app_resample}

Additional diagnostics and correlation analyses for the Resample regime, referenced in Section~\ref{sec:resample_accuracy}.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{4_3b_diverging_bar_modes.png}
\caption{(Appendix) $\Delta$RMSE improvement across noise and observation modes in the Resample regime. Improvement patterns depend more strongly on observation mode than on architecture.}
\label{fig:app_delta_modes}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_3e_correlation_baseline_fixedmean.png}
\caption{(Appendix) Correlation between Baseline RMSE and improvement in FixedMean regime. Weak negative correlation indicates that higher baseline errors lead to plateauing or declining assimilation gains.}
\label{fig:app_corr_baseline_fixed}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_3f_correlation_fixedmean_resample.png}
\caption{(Appendix) Correlation between FixedMean and Resample improvement. Weak positive trend ($\approx 0.25$--0.35) indicates partial transfer consistency: architectures that adapt efficiently under controlled conditions tend to retain relative ranking with stochastic resampling.}
\label{fig:app_corr_fixed_resample}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_3g_correlation_baseline_resample.png}
\caption{(Appendix) Correlation between Baseline and Resample improvement. Near-flat slope reveals low cross-regime alignment---models showing stronger gains in deterministic conditions do not necessarily maintain them under random perturbations.}
\label{fig:app_corr_baseline_resample}
\end{figure}

\newpage

\subsection*{A.5 Observation-Mode Diagnostics}
\addcontentsline{toc}{subsection}{A.5 Observation-Mode Diagnostics}
\label{sec:app_obs_mode}

Extended observation mode sensitivity analysis referenced in Section~\ref{sec:obs_sensitivity}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_4c_noise_scaling_rmse_ratio.png}
\caption{(Appendix) Noise scaling behavior: degradation ratio RMSE$_a$/RMSE$_b$ on logarithmic axes. The $xy$ mode remains nearly flat ($\approx 0.9 \to 0.95$), confirming linear scaling and high resilience. The $x^2$ mode rises sharply from $\approx 0.93$ to $\approx 1.02$ between $\sigma = 0.05$ and $0.1$, then plateaus---evidence of near-quadratic sensitivity in the low-noise regime.}
\label{fig:app_noise_scaling}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_4d_cross_architecture_mode_dependence.png}
\caption{(Appendix) Cross-architecture dependence on observation mode. For $xy$, all three architectures converge to comparable performance ($\approx 4.0$), demonstrating that temporal coupling dominates. Under $x$, LSTM achieves lowest RMSE ($\approx 3.7$). For $x^2$, MLP shows modest advantage ($\approx 6.7$ vs. 7--8 for recurrent models).}
\label{fig:app_cross_arch_mode}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{4_4e_variance_across_modes.png}
\caption{(Appendix) Standard deviation of $\Delta$RMSE across stochastic resampling for different observation modes. Variance is lowest for $xy$ ($< 0.1$) across all $\sigma$, signifying stable convergence. The $x^2$ mode exhibits pronounced variance (0.3--0.35), peaking near $\sigma = 0.5$.}
\label{fig:app_variance_modes}
\end{figure}

\newpage

\subsection*{A.6 Trajectory and Residual Reconstructions}
\addcontentsline{toc}{subsection}{A.6 Trajectory and Residual Reconstructions}
\label{sec:app_trajectory}

Detailed trajectory reconstruction and component-wise residual analysis referenced in Section~\ref{sec:temporal}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_5a_trajectory_fidelity_comparison.png}
\caption{(Appendix) Representative trajectory reconstructions under different observation modes showing fidelity to the true Lorenz-63 attractor. Resample regime maintains correct structure, while FixedMean exhibits drift and distortion.}
\label{fig:app_traj_fidelity}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_5c_unseen_trajectory_diagnostics.png}
\caption{(Appendix) Visualization of unseen trajectory reconstructions confirming generalization capacity of Resample vs. FixedMean regimes. Resample preserves chaotic dynamics on held-out test trajectories, while FixedMean frequently exhibits attractor escape.}
\label{fig:app_unseen_traj}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_5c_componentwise_residuals.png}
\caption{(Appendix) Component-wise residual patterns ($\Delta x = \hat{x}^a - \bar{x}$) for $X$, $Y$, and $Z$ components across architectures. Recurrent networks (GRU/LSTM) produce lower-variance $X$ and $Y$ residuals ($\sigma_{\text{res}} \approx 0.1$--0.2) compared to MLP, demonstrating that temporal awareness smooths corrections.}
\label{fig:app_componentwise}
\end{figure}

\newpage

\subsection*{A.7 Ablation Studies}
\addcontentsline{toc}{subsection}{A.7 Ablation Studies}
\label{sec:app_ablations}

Extended ablation study results referenced in Section~\ref{sec:ablations}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_6_2_sequence_length_ablation.png}
\caption{(Appendix) Effect of sequence length ($L$) on RMSE for GRU and LSTM architectures. Performance improves sharply up to $L = 10$ time steps (approximately one Lyapunov time), then plateaus or slightly increases. LSTM achieves lower minimum RMSE than GRU across tested lengths.}
\label{fig:app_seq_length}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_6_4_B_scaling_sensitivity.png}
\caption{(Appendix) Effect of background covariance ($B$) scaling factor ($\lambda$) on post-assimilation RMSE. Optimal performance is achieved when $\lambda = 1.0$ (true $B$), confirming that the network learns the underlying variational minimization problem. Deviation produces a characteristic U-shaped error curve.}
\label{fig:app_B_scaling}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{4_6_5a_B_sensitivity_regimes.png}
\caption{(Appendix) Regime-specific robustness to $B$ misestimation. Resample (solid) demonstrates substantially higher robustness with a shallower, lower basin around $\lambda = 1.0$. FixedMean (dashed) shows acute sensitivity with RMSE spiking dramatically as $B$ scaling deviates from true value.}
\label{fig:app_B_regime}
\end{figure}
\end{document}
