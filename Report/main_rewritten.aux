\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{ai_da_fablet}
\@writefile{toc}{\contentsline {section}{Abstract}{3}{Doc-Start}\protected@file@percent }
\citation{lorenz_1963}
\citation{ensemble_methods}
\citation{ai_da_fablet}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{4}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{4}{Introduction}{section.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The Lorenz-63 attractor exhibits the characteristic double-wing (butterfly) structure arising from chaotic dynamics with standard parameters $\sigma =10$, $\rho =28$, $\beta =8/3$. Sample trajectories illustrate the sensitive dependence on initial conditions that motivates data assimilation.}}{4}{figure.1}\protected@file@percent }
\newlabel{fig:lorenz}{{1}{4}{The Lorenz-63 attractor exhibits the characteristic double-wing (butterfly) structure arising from chaotic dynamics with standard parameters $\sigma =10$, $\rho =28$, $\beta =8/3$. Sample trajectories illustrate the sensitive dependence on initial conditions that motivates data assimilation}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Notation and Definitions}{5}{subsection.1.1}\protected@file@percent }
\newlabel{sec:notation}{{1.1}{5}{Notation and Definitions}{subsection.1.1}{}}
\citation{ai_da_fablet}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Notation used throughout this manuscript. The analysis functional $\Phi $ represents the theoretical mapping from assimilation inputs to the optimal analysis state; $f_\theta $ denotes its parametric neural network approximation.}}{6}{table.1}\protected@file@percent }
\newlabel{tab:notation}{{1}{6}{Notation used throughout this manuscript. The analysis functional $\Phi $ represents the theoretical mapping from assimilation inputs to the optimal analysis state; $f_\theta $ denotes its parametric neural network approximation}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Project Scope and Aims}{6}{subsection.1.2}\protected@file@percent }
\newlabel{sec:scope}{{1.2}{6}{Project Scope and Aims}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Architectural Benchmarking.}{6}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Observation Mode Sensitivity.}{6}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Noise Tolerance.}{7}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Background Conditioning Strategy.}{7}{subsection.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Experimental configuration and hyperparameters used throughout this study.}}{7}{table.2}\protected@file@percent }
\newlabel{tab:hyperparams}{{2}{7}{Experimental configuration and hyperparameters used throughout this study}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Contributions and Limitations}{7}{subsection.1.3}\protected@file@percent }
\newlabel{sec:contributions}{{1.3}{7}{Contributions and Limitations}{subsection.1.3}{}}
\@writefile{toc}{\contentsline {paragraph}{(i)}{7}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(ii)}{7}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(iii)}{7}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(iv)}{8}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Mathematical Formulation}{9}{section.2}\protected@file@percent }
\newlabel{sec:math}{{2}{9}{Mathematical Formulation}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The Variational Principle in Data Assimilation}{9}{subsection.2.1}\protected@file@percent }
\newlabel{sec:variational}{{2.1}{9}{The Variational Principle in Data Assimilation}{subsection.2.1}{}}
\newlabel{eq:3dvar}{{1}{9}{The Variational Principle in Data Assimilation}{equation.2.1}{}}
\newlabel{eq:analysis}{{2}{9}{The Variational Principle in Data Assimilation}{equation.2.2}{}}
\citation{ai_da_fablet}
\citation{lorenz_1963}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The Analysis Functional}{10}{subsection.2.2}\protected@file@percent }
\newlabel{sec:functional}{{2.2}{10}{The Analysis Functional}{subsection.2.2}{}}
\newlabel{eq:functional}{{3}{10}{The Analysis Functional}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Self-Supervised Learning Objective}{10}{subsection.2.3}\protected@file@percent }
\newlabel{sec:objective}{{2.3}{10}{Self-Supervised Learning Objective}{subsection.2.3}{}}
\newlabel{eq:loss}{{4}{10}{Self-Supervised Learning Objective}{equation.2.4}{}}
\newlabel{eq:loss_expanded}{{5}{10}{Self-Supervised Learning Objective}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}The Lorenz-63 System}{11}{subsection.2.4}\protected@file@percent }
\newlabel{sec:lorenz}{{2.4}{11}{The Lorenz-63 System}{subsection.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Observation Operators}{11}{subsection.2.5}\protected@file@percent }
\newlabel{sec:obs_operators}{{2.5}{11}{Observation Operators}{subsection.2.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Observation operator configurations. The $xy$ mode provides the most state information; the $x^2$ mode is most challenging due to nonlinearity and sign ambiguity.}}{11}{table.3}\protected@file@percent }
\newlabel{tab:obs_operators}{{3}{11}{Observation operator configurations. The $xy$ mode provides the most state information; the $x^2$ mode is most challenging due to nonlinearity and sign ambiguity}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Observation operators and sample noisy sequences for the three observation modes. The nonlinear $x^2$ mode amplifies dynamical magnitudes but removes sign information, creating a more ill-posed inverse problem.}}{12}{figure.2}\protected@file@percent }
\newlabel{fig:obs_operators}{{2}{12}{Observation operators and sample noisy sequences for the three observation modes. The nonlinear $x^2$ mode amplifies dynamical magnitudes but removes sign information, creating a more ill-posed inverse problem}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods and Experimental Setup}{13}{section.3}\protected@file@percent }
\newlabel{sec:methods}{{3}{13}{Methods and Experimental Setup}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data Generation}{13}{subsection.3.1}\protected@file@percent }
\newlabel{sec:data_gen}{{3.1}{13}{Data Generation}{subsection.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Dataset distribution across train and test splits for all observation modes and noise levels. Each configuration contributes approximately $2.94 \times 10^5$ effective samples, ensuring balanced evaluation across experimental conditions.}}{13}{figure.3}\protected@file@percent }
\newlabel{fig:dataset_dist}{{3}{13}{Dataset distribution across train and test splits for all observation modes and noise levels. Each configuration contributes approximately $2.94 \times 10^5$ effective samples, ensuring balanced evaluation across experimental conditions}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Background Statistics}{13}{subsection.3.2}\protected@file@percent }
\newlabel{sec:background}{{3.2}{13}{Background Statistics}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{FixedMean Regime.}{13}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Resample Regime.}{14}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Background statistics computed from a resampled ensemble: covariance matrix $B$, its eigenvalue spectrum, and the $2\sigma $ ensemble contour in $x_1$--$x_3$ space. These statistics quantify the anisotropic uncertainty structure of the Lorenz attractor.}}{14}{figure.4}\protected@file@percent }
\newlabel{fig:background_stats}{{4}{14}{Background statistics computed from a resampled ensemble: covariance matrix $B$, its eigenvalue spectrum, and the $2\sigma $ ensemble contour in $x_1$--$x_3$ space. These statistics quantify the anisotropic uncertainty structure of the Lorenz attractor}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Neural Network Architectures}{14}{subsection.3.3}\protected@file@percent }
\newlabel{sec:architectures}{{3.3}{14}{Neural Network Architectures}{subsection.3.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Architecture configurations. The Baseline MLP excludes background information entirely and serves as a diagnostic control. Primary architectures condition on both observations and background.}}{15}{table.4}\protected@file@percent }
\newlabel{tab:arch_summary}{{4}{15}{Architecture configurations. The Baseline MLP excludes background information entirely and serves as a diagnostic control. Primary architectures condition on both observations and background}{table.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Baseline MLP (No-Mean).}{15}{table.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{MLP.}{15}{table.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{GRU and LSTM.}{15}{table.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Training Procedure}{15}{subsection.3.4}\protected@file@percent }
\newlabel{sec:training}{{3.4}{15}{Training Procedure}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Evaluation Metrics}{16}{subsection.3.5}\protected@file@percent }
\newlabel{sec:metrics}{{3.5}{16}{Evaluation Metrics}{subsection.3.5}{}}
\newlabel{eq:rmse}{{9}{16}{Evaluation Metrics}{equation.3.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Improvement relative to background (primary).}{16}{equation.3.9}\protected@file@percent }
\newlabel{eq:improvement}{{10}{16}{Improvement relative to background (primary)}{equation.3.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Improvement relative to FixedMean (secondary).}{16}{equation.3.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Reproducibility}{17}{subsection.3.6}\protected@file@percent }
\newlabel{sec:reproducibility}{{3.6}{17}{Reproducibility}{subsection.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments and Results}{18}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{18}{Experiments and Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Convergence and Training Dynamics}{18}{subsection.4.1}\protected@file@percent }
\newlabel{sec:convergence}{{4.1}{18}{Convergence and Training Dynamics}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Mean convergence envelopes across training regimes. The Resample regime (blue) achieves the fastest and most stable convergence, reducing loss to approximately 20\% of the initial value within the first epoch. FixedMean (orange) shows rapid initial descent but exhibits instability at high noise levels during evaluation. Baseline (green) converges slowly, retaining approximately 90\% of initial error after 30 epochs. Shaded regions indicate standard deviation across observation modes and noise levels.}}{18}{figure.5}\protected@file@percent }
\newlabel{fig:convergence_envelopes}{{5}{18}{Mean convergence envelopes across training regimes. The Resample regime (blue) achieves the fastest and most stable convergence, reducing loss to approximately 20\% of the initial value within the first epoch. FixedMean (orange) shows rapid initial descent but exhibits instability at high noise levels during evaluation. Baseline (green) converges slowly, retaining approximately 90\% of initial error after 30 epochs. Shaded regions indicate standard deviation across observation modes and noise levels}{figure.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Divergence Rates.}{19}{figure.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Resample Regime: Accuracy and Stability}{19}{subsection.4.2}\protected@file@percent }
\newlabel{sec:resample_accuracy}{{4.2}{19}{Resample Regime: Accuracy and Stability}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Post-assimilation RMSE distributions for MLP, GRU, and LSTM across noise levels in the Resample regime ($xy$ mode). Box plots show median, interquartile range, and outliers. All models maintain stable accuracy for $\sigma _{\text  {obs}} \leq 0.1$ (median RMSE $\approx 4$--5), with dispersion increasing at higher noise.}}{19}{figure.6}\protected@file@percent }
\newlabel{fig:resample_rmse}{{6}{19}{Post-assimilation RMSE distributions for MLP, GRU, and LSTM across noise levels in the Resample regime ($xy$ mode). Box plots show median, interquartile range, and outliers. All models maintain stable accuracy for $\sigma _{\text {obs}} \leq 0.1$ (median RMSE $\approx 4$--5), with dispersion increasing at higher noise}{figure.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Cross-model RMSE statistics under Resample regime (mean $\pm $ std across runs, excluding diverged cases).}}{20}{table.5}\protected@file@percent }
\newlabel{tab:resample_stats}{{5}{20}{Cross-model RMSE statistics under Resample regime (mean $\pm $ std across runs, excluding diverged cases)}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Observation Mode Sensitivity}{20}{subsection.4.3}\protected@file@percent }
\newlabel{sec:obs_sensitivity}{{4.3}{20}{Observation Mode Sensitivity}{subsection.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Post-assimilation RMSE distributions for all observation modes as a function of noise level $\sigma _{\text  {obs}}$, aggregated across architectures. The $xy$ mode achieves the lowest and most stable errors (median RMSE $\approx 4$), followed by $x$ ($\approx 4$--5), while $x^2$ exhibits markedly higher dispersion and error magnitudes ($\approx 5$--10) due to nonlinear noise amplification.}}{21}{figure.7}\protected@file@percent }
\newlabel{fig:obs_mode_rmse}{{7}{21}{Post-assimilation RMSE distributions for all observation modes as a function of noise level $\sigma _{\text {obs}}$, aggregated across architectures. The $xy$ mode achieves the lowest and most stable errors (median RMSE $\approx 4$), followed by $x$ ($\approx 4$--5), while $x^2$ exhibits markedly higher dispersion and error magnitudes ($\approx 5$--10) due to nonlinear noise amplification}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Assimilation improvement ($\Delta $RMSE = RMSE$_b$ $-$ RMSE$_a$) across observation modes as a function of noise. At low noise, all modes achieve substantial gains. As noise increases, $x$ and $xy$ retain positive gains, while $x^2$ approaches zero improvement at $\sigma _{\text  {obs}} \geq 0.5$.}}{22}{figure.8}\protected@file@percent }
\newlabel{fig:obs_mode_delta}{{8}{22}{Assimilation improvement ($\Delta $RMSE = RMSE$_b$ $-$ RMSE$_a$) across observation modes as a function of noise. At low noise, all modes achieve substantial gains. As noise increases, $x$ and $xy$ retain positive gains, while $x^2$ approaches zero improvement at $\sigma _{\text {obs}} \geq 0.5$}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Temporal Dynamics and Attractor Geometry}{22}{subsection.4.4}\protected@file@percent }
\newlabel{sec:temporal}{{4.4}{22}{Temporal Dynamics and Attractor Geometry}{subsection.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Temporal evolution of Euclidean error over the assimilation window for FixedMean and Resample regimes at different noise levels ($xy$ mode, GRU architecture). Resample (solid lines) demonstrates fast convergence ($\tau \approx 50$ time steps) to a low, stable error plateau across all noise levels. FixedMean (dashed lines), particularly at $\sigma _{\text  {obs}} = 0.5$, exhibits large oscillations and sustained high error.}}{23}{figure.9}\protected@file@percent }
\newlabel{fig:error_evolution}{{9}{23}{Temporal evolution of Euclidean error over the assimilation window for FixedMean and Resample regimes at different noise levels ($xy$ mode, GRU architecture). Resample (solid lines) demonstrates fast convergence ($\tau \approx 50$ time steps) to a low, stable error plateau across all noise levels. FixedMean (dashed lines), particularly at $\sigma _{\text {obs}} = 0.5$, exhibits large oscillations and sustained high error}{figure.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Attractor Geometry.}{23}{figure.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Global-normalized Hausdorff distances ($\tilde  {H}_{\text  {global}}$) for Resample and FixedMean regimes at four noise levels ($xy$ mode). Each point represents median $\tilde  {H}_{\text  {global}}$ across test trajectories; error bars indicate IQR. Resample consistently achieves low geometric deviation ($\tilde  {H}_{\text  {global}} \approx 0.32$) with tight dispersion. FixedMean exhibits substantially higher deviation ($\tilde  {H}_{\text  {global}} \approx 1.50$) and increased variance at $\sigma _{\text  {obs}} \geq 0.50$.}}{24}{figure.10}\protected@file@percent }
\newlabel{fig:global_hausdorff}{{10}{24}{Global-normalized Hausdorff distances ($\tilde {H}_{\text {global}}$) for Resample and FixedMean regimes at four noise levels ($xy$ mode). Each point represents median $\tilde {H}_{\text {global}}$ across test trajectories; error bars indicate IQR. Resample consistently achieves low geometric deviation ($\tilde {H}_{\text {global}} \approx 0.32$) with tight dispersion. FixedMean exhibits substantially higher deviation ($\tilde {H}_{\text {global}} \approx 1.50$) and increased variance at $\sigma _{\text {obs}} \geq 0.50$}{figure.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Lobe Occupancy.}{24}{figure.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Lobe occupancy discrepancy ($\Delta _{\text  {lobe}}$) for all regimes and observation modes across noise levels. Light cells indicate near-perfect lobe matching ($\Delta _{\text  {lobe}} \approx 0$); dark cells indicate substantial imbalance. Resample achieves uniformly low discrepancies across modes, indicating correct separatrix crossings. FixedMean shows moderate bias ($\Delta _{\text  {lobe}} \approx 0.10$--0.15) at higher noise.}}{25}{figure.11}\protected@file@percent }
\newlabel{fig:lobe_discrepancy}{{11}{25}{Lobe occupancy discrepancy ($\Delta _{\text {lobe}}$) for all regimes and observation modes across noise levels. Light cells indicate near-perfect lobe matching ($\Delta _{\text {lobe}} \approx 0$); dark cells indicate substantial imbalance. Resample achieves uniformly low discrepancies across modes, indicating correct separatrix crossings. FixedMean shows moderate bias ($\Delta _{\text {lobe}} \approx 0.10$--0.15) at higher noise}{figure.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Resample regime, $xy$ mode: accuracy and geometry metrics versus noise (means across runs).}}{25}{table.6}\protected@file@percent }
\newlabel{tab:xy_resample}{{6}{25}{Resample regime, $xy$ mode: accuracy and geometry metrics versus noise (means across runs)}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Ablation Studies}{25}{subsection.4.5}\protected@file@percent }
\newlabel{sec:ablations}{{4.5}{25}{Ablation Studies}{subsection.4.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Background Sampling Strategy.}{26}{subsection.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Impact of background sampling strategy on RMSE stability across noise levels. Resample (solid lines) demonstrates graceful degradation, with RMSE increasing smoothly from $\approx 4$ to $\approx 7$. FixedMean (dashed lines) exhibits catastrophic instability beyond $\sigma _{\text  {obs}} > 0.5$, with RMSE spikes and divergence rates reaching 70--80\%.}}{26}{figure.12}\protected@file@percent }
\newlabel{fig:background_stability}{{12}{26}{Impact of background sampling strategy on RMSE stability across noise levels. Resample (solid lines) demonstrates graceful degradation, with RMSE increasing smoothly from $\approx 4$ to $\approx 7$. FixedMean (dashed lines) exhibits catastrophic instability beyond $\sigma _{\text {obs}} > 0.5$, with RMSE spikes and divergence rates reaching 70--80\%}{figure.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Background Covariance Sensitivity.}{26}{figure.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Practical Recommendations.}{26}{figure.12}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Practical recommendations for AI-based data assimilation in chaotic systems.}}{27}{table.7}\protected@file@percent }
\newlabel{tab:recommendations}{{7}{27}{Practical recommendations for AI-based data assimilation in chaotic systems}{table.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{28}{section.5}\protected@file@percent }
\newlabel{sec:discussion}{{5}{28}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Architectural Comparisons: Inconclusive Evidence}{28}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}The Importance of Stochastic Regularization}{28}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Failure Modes and Their Implications}{29}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Attractor Escape.}{29}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Lobe Imbalance.}{29}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Limitations and Caveats}{29}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Low Dimensionality.}{29}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Lack of Classical Baselines.}{29}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Single System, Single Loss.}{29}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Statistical Power.}{29}{subsection.5.4}\protected@file@percent }
\citation{lorenz_1963}
\@writefile{toc}{\contentsline {section}{\numberline {6}Outlook}{30}{section.6}\protected@file@percent }
\newlabel{sec:outlook}{{6}{30}{Outlook}{section.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Extension to Higher-Dimensional Systems.}{30}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hybrid Approaches.}{30}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Probabilistic Extensions.}{30}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Semi-Supervised Learning.}{30}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Observational Strategies.}{30}{section.6}\protected@file@percent }
\bibstyle{plainnat}
\bibcite{lorenz_1963}{{1}{}{{}}{{}}}
\bibcite{kalman_1960}{{2}{}{{}}{{}}}
\bibcite{ai_da_fablet}{{3}{}{{}}{{}}}
\bibcite{ensemble_methods}{{4}{}{{}}{{}}}
\bibcite{neural_da_bocquet}{{5}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{References}{31}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Appendix}{32}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{A.1 Data and Pipeline Extras}{32}{section*.1}\protected@file@percent }
\newlabel{sec:app_data}{{6}{32}{}{section*.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces (Appendix) Observation operators and sample noisy sequences for the three observation modes. The nonlinear $x^2$ mode amplifies dynamical magnitudes but removes sign information, illustrating a more ill-posed mapping problem.}}{32}{figure.13}\protected@file@percent }
\newlabel{fig:app_obs_operators}{{13}{32}{(Appendix) Observation operators and sample noisy sequences for the three observation modes. The nonlinear $x^2$ mode amplifies dynamical magnitudes but removes sign information, illustrating a more ill-posed mapping problem}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces (Appendix) Background statistics: covariance matrix $B$, its eigenvalue spectrum, and the $2\sigma $ ensemble contour in $X$--$Z$ space. These statistics quantify the uncertainty anisotropy of the Lorenz attractor.}}{32}{figure.14}\protected@file@percent }
\newlabel{fig:app_background_stats}{{14}{32}{(Appendix) Background statistics: covariance matrix $B$, its eigenvalue spectrum, and the $2\sigma $ ensemble contour in $X$--$Z$ space. These statistics quantify the uncertainty anisotropy of the Lorenz attractor}{figure.14}{}}
\@writefile{toc}{\contentsline {subsection}{A.2 Training Dynamics}{34}{figure.14}\protected@file@percent }
\newlabel{sec:app_training}{{6}{34}{}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces (Appendix) Baseline training and validation loss across all observation modes and noise levels. Multi-panel grid shows smooth, monotonic convergence with minor variance across runs.}}{34}{figure.15}\protected@file@percent }
\newlabel{fig:app_baseline_loss}{{15}{34}{(Appendix) Baseline training and validation loss across all observation modes and noise levels. Multi-panel grid shows smooth, monotonic convergence with minor variance across runs}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces (Appendix) Baseline mean convergence averaged across all settings. The relatively small variance band indicates consistent learning behavior across random seeds.}}{34}{figure.16}\protected@file@percent }
\newlabel{fig:app_baseline_mean}{{16}{34}{(Appendix) Baseline mean convergence averaged across all settings. The relatively small variance band indicates consistent learning behavior across random seeds}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces (Appendix) Baseline RMSE vs. noise level $\sigma $. Performance degrades gradually with higher noise, confirming that model capacity is constrained by observation quality.}}{35}{figure.17}\protected@file@percent }
\newlabel{fig:app_baseline_rmse}{{17}{35}{(Appendix) Baseline RMSE vs. noise level $\sigma $. Performance degrades gradually with higher noise, confirming that model capacity is constrained by observation quality}{figure.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces (Appendix) FixedMean training and validation loss grid. Despite smooth loss traces in some settings, FixedMean frequently becomes unstable at moderate/high noise ($\sigma \geq 0.5$), with post-assimilation RMSE explosions and attractor escape on unseen trajectories.}}{36}{figure.18}\protected@file@percent }
\newlabel{fig:app_fixedmean_loss}{{18}{36}{(Appendix) FixedMean training and validation loss grid. Despite smooth loss traces in some settings, FixedMean frequently becomes unstable at moderate/high noise ($\sigma \geq 0.5$), with post-assimilation RMSE explosions and attractor escape on unseen trajectories}{figure.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces (Appendix) FixedMean mean convergence across all settings. The narrow variance envelope indicates reproducible convergence during training, though this does not translate to stable test performance at high noise.}}{37}{figure.19}\protected@file@percent }
\newlabel{fig:app_fixedmean_mean}{{19}{37}{(Appendix) FixedMean mean convergence across all settings. The narrow variance envelope indicates reproducible convergence during training, though this does not translate to stable test performance at high noise}{figure.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces (Appendix) FixedMean RMSE vs. noise level per architecture. All three models show graceful degradation during training, but GRU and LSTM maintain smoother, more consistent responses than MLP.}}{37}{figure.20}\protected@file@percent }
\newlabel{fig:app_fixedmean_rmse}{{20}{37}{(Appendix) FixedMean RMSE vs. noise level per architecture. All three models show graceful degradation during training, but GRU and LSTM maintain smoother, more consistent responses than MLP}{figure.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces (Appendix) FixedMean RMSE before and after assimilation per architecture. Dashed lines denote pre-assimilation errors, solid lines show post-assimilation results. Recurrent architectures (GRU, LSTM) better internalize assimilation correction dynamics.}}{38}{figure.21}\protected@file@percent }
\newlabel{fig:app_fixedmean_before_after}{{21}{38}{(Appendix) FixedMean RMSE before and after assimilation per architecture. Dashed lines denote pre-assimilation errors, solid lines show post-assimilation results. Recurrent architectures (GRU, LSTM) better internalize assimilation correction dynamics}{figure.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces (Appendix) FixedMean $\Delta $RMSE (before $-$ after assimilation). Positive bars indicate improvement. All modes show consistent RMSE gains, with strongest improvements in nonlinear $x^2$ and mixed $xy$ settings.}}{38}{figure.22}\protected@file@percent }
\newlabel{fig:app_fixedmean_delta}{{22}{38}{(Appendix) FixedMean $\Delta $RMSE (before $-$ after assimilation). Positive bars indicate improvement. All modes show consistent RMSE gains, with strongest improvements in nonlinear $x^2$ and mixed $xy$ settings}{figure.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces (Appendix) Relative post-assimilation RMSE (FixedMean vs. Baseline). FixedMean achieves 40--60\% improvement over Baseline in controlled conditions, with largest gains for single-channel observations.}}{39}{figure.23}\protected@file@percent }
\newlabel{fig:app_relative_rmse}{{23}{39}{(Appendix) Relative post-assimilation RMSE (FixedMean vs. Baseline). FixedMean achieves 40--60\% improvement over Baseline in controlled conditions, with largest gains for single-channel observations}{figure.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces (Appendix) $\Delta $RMSE improvement comparison between regimes. FixedMean achieves larger effective reduction than Baseline, especially at higher noise scenarios ($\sigma = 0.5$--1.0).}}{39}{figure.24}\protected@file@percent }
\newlabel{fig:app_delta_improvement}{{24}{39}{(Appendix) $\Delta $RMSE improvement comparison between regimes. FixedMean achieves larger effective reduction than Baseline, especially at higher noise scenarios ($\sigma = 0.5$--1.0)}{figure.24}{}}
\@writefile{toc}{\contentsline {subsection}{A.3 Extended Attractor Geometry Diagnostics}{40}{figure.24}\protected@file@percent }
\newlabel{sec:app_attractor}{{6}{40}{}{figure.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces (Appendix) Phase--space projections (XY, YZ) for three assimilation regimes (Resample, FixedMean, Baseline) at two noise levels ($\sigma =0.1,0.5$). Each panel overlays truth (gray) and analysis (blue); inset text gives normalized Hausdorff distance $\tilde  {H}$ where $\tilde  {H}=H/\text  {diam}(\mathcal  {A}_{\text  {truth}})$ and $H$ is the symmetric Hausdorff between truth and analysis trajectories (after downsampling to $\leq 6000$ points). Resample maintains close geometric adherence ($\tilde  {H}\approx 0.32$) across noise, Baseline drifts moderately ($\tilde  {H}\approx 1.08$), and FixedMean exhibits substantial structural distortion ($\tilde  {H}\approx 1.48$--1.64).}}{40}{figure.25}\protected@file@percent }
\newlabel{fig:app_attractor_geometry_panels}{{25}{40}{(Appendix) Phase--space projections (XY, YZ) for three assimilation regimes (Resample, FixedMean, Baseline) at two noise levels ($\sigma =0.1,0.5$). Each panel overlays truth (gray) and analysis (blue); inset text gives normalized Hausdorff distance $\tilde {H}$ where $\tilde {H}=H/\text {diam}(\mathcal {A}_{\text {truth}})$ and $H$ is the symmetric Hausdorff between truth and analysis trajectories (after downsampling to $\leq 6000$ points). Resample maintains close geometric adherence ($\tilde {H}\approx 0.32$) across noise, Baseline drifts moderately ($\tilde {H}\approx 1.08$), and FixedMean exhibits substantial structural distortion ($\tilde {H}\approx 1.48$--1.64)}{figure.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces (Appendix) (a) Distribution of normalized Hausdorff distances $\tilde  {H}$ across noise levels ($\sigma \in \{0.05,0.1,0.5,1.0\}$) for each regime. (b) Scaling of $\tilde  {H}$ with noise $\sigma $ (log scale). Medians (IQR): Resample $0.32$ (0.32--0.32), Baseline $1.08$ (1.08--1.09), FixedMean $1.50$ (1.42--1.55). Lower $\tilde  {H}$ indicates better geometric adherence.}}{41}{figure.26}\protected@file@percent }
\newlabel{fig:app_attractor_geometry_metrics}{{26}{41}{(Appendix) (a) Distribution of normalized Hausdorff distances $\tilde {H}$ across noise levels ($\sigma \in \{0.05,0.1,0.5,1.0\}$) for each regime. (b) Scaling of $\tilde {H}$ with noise $\sigma $ (log scale). Medians (IQR): Resample $0.32$ (0.32--0.32), Baseline $1.08$ (1.08--1.09), FixedMean $1.50$ (1.42--1.55). Lower $\tilde {H}$ indicates better geometric adherence}{figure.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces (Appendix) Detailed comparison of global-normalized Hausdorff distances for Resample versus FixedMean regimes across architectures (MLP, GRU, LSTM) and noise levels. Left panel shows distributions at $\sigma =0.10$; right panel shows $\sigma =0.50$. Resample (blue) consistently outperforms FixedMean (red) across all architectures, with GRU and LSTM showing slight advantages in noise robustness. At higher noise ($\sigma =0.50$), FixedMean distributions broaden significantly, indicating geometric instability, while Resample remains tightly concentrated near $\tilde  {H}_{\text  {global}} \approx 0.32$. This confirms that architectural choice is secondary to regime selection: dynamic background updates are the primary driver of geometric fidelity.}}{41}{figure.27}\protected@file@percent }
\newlabel{fig:app_attractor_metrics_detailed}{{27}{41}{(Appendix) Detailed comparison of global-normalized Hausdorff distances for Resample versus FixedMean regimes across architectures (MLP, GRU, LSTM) and noise levels. Left panel shows distributions at $\sigma =0.10$; right panel shows $\sigma =0.50$. Resample (blue) consistently outperforms FixedMean (red) across all architectures, with GRU and LSTM showing slight advantages in noise robustness. At higher noise ($\sigma =0.50$), FixedMean distributions broaden significantly, indicating geometric instability, while Resample remains tightly concentrated near $\tilde {H}_{\text {global}} \approx 0.32$. This confirms that architectural choice is secondary to regime selection: dynamic background updates are the primary driver of geometric fidelity}{figure.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces (Appendix) Attractor geometry for the Baseline regime across three observation modes ($x$, $xy$, and $x^2$) and four noise levels ($\sigma \in \{0.05, 0.10, 0.50, 1.00\}$). Each grid contains XY and YZ projections with truth (gray) and analysis (blue) trajectories. Baseline maintains partial geometric fidelity at low noise but exhibits increasing lobe imbalance and drift at higher noise, particularly for the $x^2$ mode where nonlinear observations reduce separatrix information.}}{42}{figure.28}\protected@file@percent }
\newlabel{fig:app_attractor_grid_baseline}{{28}{42}{(Appendix) Attractor geometry for the Baseline regime across three observation modes ($x$, $xy$, and $x^2$) and four noise levels ($\sigma \in \{0.05, 0.10, 0.50, 1.00\}$). Each grid contains XY and YZ projections with truth (gray) and analysis (blue) trajectories. Baseline maintains partial geometric fidelity at low noise but exhibits increasing lobe imbalance and drift at higher noise, particularly for the $x^2$ mode where nonlinear observations reduce separatrix information}{figure.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces (Appendix) Attractor geometry for the FixedMean regime across observation modes. FixedMean displays substantial geometric degradation at moderate and high noise levels. Analysis trajectories (blue) deviate from truth (gray), often collapsing toward fixed points or producing fragmented, non-chaotic shapes. These distortions are consistent with the elevated lobe imbalance values observed in Fig.~\ref {fig:lobe_discrepancy}.}}{44}{figure.29}\protected@file@percent }
\newlabel{fig:app_attractor_grid_fixedmean}{{29}{44}{(Appendix) Attractor geometry for the FixedMean regime across observation modes. FixedMean displays substantial geometric degradation at moderate and high noise levels. Analysis trajectories (blue) deviate from truth (gray), often collapsing toward fixed points or producing fragmented, non-chaotic shapes. These distortions are consistent with the elevated lobe imbalance values observed in Fig.~\ref {fig:lobe_discrepancy}}{figure.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces (Appendix) Attractor geometry for the Resample regime across observation modes. Resample shows excellent geometric fidelity: analysis trajectories (blue) closely follow truth (gray) across all noise levels. Balanced wing visitation and correct separatrix crossings confirm global topology preservation, even under the challenging $x^2$ mode and $\sigma =1.00$.}}{46}{figure.30}\protected@file@percent }
\newlabel{fig:app_attractor_grid_resample}{{30}{46}{(Appendix) Attractor geometry for the Resample regime across observation modes. Resample shows excellent geometric fidelity: analysis trajectories (blue) closely follow truth (gray) across all noise levels. Balanced wing visitation and correct separatrix crossings confirm global topology preservation, even under the challenging $x^2$ mode and $\sigma =1.00$}{figure.30}{}}
\@writefile{toc}{\contentsline {subsection}{A.4 Extended Resample Metrics}{47}{figure.30}\protected@file@percent }
\newlabel{sec:app_resample}{{6}{47}{}{figure.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces (Appendix) $\Delta $RMSE improvement across noise and observation modes in the Resample regime. Improvement patterns depend more strongly on observation mode than on architecture.}}{47}{figure.31}\protected@file@percent }
\newlabel{fig:app_delta_modes}{{31}{47}{(Appendix) $\Delta $RMSE improvement across noise and observation modes in the Resample regime. Improvement patterns depend more strongly on observation mode than on architecture}{figure.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces (Appendix) Correlation between Baseline RMSE and improvement in FixedMean regime. Weak negative correlation indicates that higher baseline errors lead to plateauing or declining assimilation gains.}}{48}{figure.32}\protected@file@percent }
\newlabel{fig:app_corr_baseline_fixed}{{32}{48}{(Appendix) Correlation between Baseline RMSE and improvement in FixedMean regime. Weak negative correlation indicates that higher baseline errors lead to plateauing or declining assimilation gains}{figure.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces (Appendix) Correlation between FixedMean and Resample improvement. Weak positive trend ($\approx 0.25$--0.35) indicates partial transfer consistency: architectures that adapt efficiently under controlled conditions tend to retain relative ranking with stochastic resampling.}}{49}{figure.33}\protected@file@percent }
\newlabel{fig:app_corr_fixed_resample}{{33}{49}{(Appendix) Correlation between FixedMean and Resample improvement. Weak positive trend ($\approx 0.25$--0.35) indicates partial transfer consistency: architectures that adapt efficiently under controlled conditions tend to retain relative ranking with stochastic resampling}{figure.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces (Appendix) Correlation between Baseline and Resample improvement. Near-flat slope reveals low cross-regime alignment---models showing stronger gains in deterministic conditions do not necessarily maintain them under random perturbations.}}{50}{figure.34}\protected@file@percent }
\newlabel{fig:app_corr_baseline_resample}{{34}{50}{(Appendix) Correlation between Baseline and Resample improvement. Near-flat slope reveals low cross-regime alignment---models showing stronger gains in deterministic conditions do not necessarily maintain them under random perturbations}{figure.34}{}}
\@writefile{toc}{\contentsline {subsection}{A.5 Observation-Mode Diagnostics}{51}{figure.34}\protected@file@percent }
\newlabel{sec:app_obs_mode}{{6}{51}{}{figure.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces (Appendix) Noise scaling behavior: degradation ratio RMSE$_a$/RMSE$_b$ on logarithmic axes. The $xy$ mode remains nearly flat ($\approx 0.9 \to 0.95$), confirming linear scaling and high resilience. The $x^2$ mode rises sharply from $\approx 0.93$ to $\approx 1.02$ between $\sigma = 0.05$ and $0.1$, then plateaus---evidence of near-quadratic sensitivity in the low-noise regime.}}{51}{figure.35}\protected@file@percent }
\newlabel{fig:app_noise_scaling}{{35}{51}{(Appendix) Noise scaling behavior: degradation ratio RMSE$_a$/RMSE$_b$ on logarithmic axes. The $xy$ mode remains nearly flat ($\approx 0.9 \to 0.95$), confirming linear scaling and high resilience. The $x^2$ mode rises sharply from $\approx 0.93$ to $\approx 1.02$ between $\sigma = 0.05$ and $0.1$, then plateaus---evidence of near-quadratic sensitivity in the low-noise regime}{figure.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces (Appendix) Cross-architecture dependence on observation mode. For $xy$, all three architectures converge to comparable performance ($\approx 4.0$), demonstrating that temporal coupling dominates. Under $x$, LSTM achieves lowest RMSE ($\approx 3.7$). For $x^2$, MLP shows modest advantage ($\approx 6.7$ vs. 7--8 for recurrent models).}}{52}{figure.36}\protected@file@percent }
\newlabel{fig:app_cross_arch_mode}{{36}{52}{(Appendix) Cross-architecture dependence on observation mode. For $xy$, all three architectures converge to comparable performance ($\approx 4.0$), demonstrating that temporal coupling dominates. Under $x$, LSTM achieves lowest RMSE ($\approx 3.7$). For $x^2$, MLP shows modest advantage ($\approx 6.7$ vs. 7--8 for recurrent models)}{figure.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces (Appendix) Standard deviation of $\Delta $RMSE across stochastic resampling for different observation modes. Variance is lowest for $xy$ ($< 0.1$) across all $\sigma $, signifying stable convergence. The $x^2$ mode exhibits pronounced variance (0.3--0.35), peaking near $\sigma = 0.5$.}}{52}{figure.37}\protected@file@percent }
\newlabel{fig:app_variance_modes}{{37}{52}{(Appendix) Standard deviation of $\Delta $RMSE across stochastic resampling for different observation modes. Variance is lowest for $xy$ ($< 0.1$) across all $\sigma $, signifying stable convergence. The $x^2$ mode exhibits pronounced variance (0.3--0.35), peaking near $\sigma = 0.5$}{figure.37}{}}
\@writefile{toc}{\contentsline {subsection}{A.6 Trajectory and Residual Reconstructions}{53}{figure.37}\protected@file@percent }
\newlabel{sec:app_trajectory}{{6}{53}{}{figure.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces (Appendix) Representative trajectory reconstructions under different observation modes showing fidelity to the true Lorenz-63 attractor. Resample regime maintains correct structure, while FixedMean exhibits drift and distortion.}}{53}{figure.38}\protected@file@percent }
\newlabel{fig:app_traj_fidelity}{{38}{53}{(Appendix) Representative trajectory reconstructions under different observation modes showing fidelity to the true Lorenz-63 attractor. Resample regime maintains correct structure, while FixedMean exhibits drift and distortion}{figure.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces (Appendix) Visualization of unseen trajectory reconstructions confirming generalization capacity of Resample vs. FixedMean regimes. Resample preserves chaotic dynamics on held-out test trajectories, while FixedMean frequently exhibits attractor escape.}}{54}{figure.39}\protected@file@percent }
\newlabel{fig:app_unseen_traj}{{39}{54}{(Appendix) Visualization of unseen trajectory reconstructions confirming generalization capacity of Resample vs. FixedMean regimes. Resample preserves chaotic dynamics on held-out test trajectories, while FixedMean frequently exhibits attractor escape}{figure.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces (Appendix) Component-wise residual patterns ($\Delta x = \hat  {x}^a - \bar  {x}$) for $X$, $Y$, and $Z$ components across architectures. Recurrent networks (GRU/LSTM) produce lower-variance $X$ and $Y$ residuals ($\sigma _{\text  {res}} \approx 0.1$--0.2) compared to MLP, demonstrating that temporal awareness smooths corrections.}}{55}{figure.40}\protected@file@percent }
\newlabel{fig:app_componentwise}{{40}{55}{(Appendix) Component-wise residual patterns ($\Delta x = \hat {x}^a - \bar {x}$) for $X$, $Y$, and $Z$ components across architectures. Recurrent networks (GRU/LSTM) produce lower-variance $X$ and $Y$ residuals ($\sigma _{\text {res}} \approx 0.1$--0.2) compared to MLP, demonstrating that temporal awareness smooths corrections}{figure.40}{}}
\@writefile{toc}{\contentsline {subsection}{A.7 Ablation Studies}{56}{figure.40}\protected@file@percent }
\newlabel{sec:app_ablations}{{6}{56}{}{figure.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces (Appendix) Effect of sequence length ($L$) on RMSE for GRU and LSTM architectures. Performance improves sharply up to $L = 10$ time steps (approximately one Lyapunov time), then plateaus or slightly increases. LSTM achieves lower minimum RMSE than GRU across tested lengths.}}{56}{figure.41}\protected@file@percent }
\newlabel{fig:app_seq_length}{{41}{56}{(Appendix) Effect of sequence length ($L$) on RMSE for GRU and LSTM architectures. Performance improves sharply up to $L = 10$ time steps (approximately one Lyapunov time), then plateaus or slightly increases. LSTM achieves lower minimum RMSE than GRU across tested lengths}{figure.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces (Appendix) Effect of background covariance ($B$) scaling factor ($\lambda $) on post-assimilation RMSE. Optimal performance is achieved when $\lambda = 1.0$ (true $B$), confirming that the network learns the underlying variational minimization problem. Deviation produces a characteristic U-shaped error curve.}}{56}{figure.42}\protected@file@percent }
\newlabel{fig:app_B_scaling}{{42}{56}{(Appendix) Effect of background covariance ($B$) scaling factor ($\lambda $) on post-assimilation RMSE. Optimal performance is achieved when $\lambda = 1.0$ (true $B$), confirming that the network learns the underlying variational minimization problem. Deviation produces a characteristic U-shaped error curve}{figure.42}{}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef\minted@oldcachelist{,
  default.pygstyle,
  36880528411FBE0BEF1AE6A5F43D1AACB8572DF2D5F430D748FAFA403DF594D3.pygtex,
  7C49B45040C6E691BC7C6729953D3D1EB8572DF2D5F430D748FAFA403DF594D3.pygtex}
\@writefile{lof}{\contentsline {figure}{\numberline {43}{\ignorespaces (Appendix) Regime-specific robustness to $B$ misestimation. Resample (solid) demonstrates substantially higher robustness with a shallower, lower basin around $\lambda = 1.0$. FixedMean (dashed) shows acute sensitivity with RMSE spiking dramatically as $B$ scaling deviates from true value.}}{57}{figure.43}\protected@file@percent }
\newlabel{fig:app_B_regime}{{43}{57}{(Appendix) Regime-specific robustness to $B$ misestimation. Resample (solid) demonstrates substantially higher robustness with a shallower, lower basin around $\lambda = 1.0$. FixedMean (dashed) shows acute sensitivity with RMSE spiking dramatically as $B$ scaling deviates from true value}{figure.43}{}}
\gdef \@abspage@last{58}
